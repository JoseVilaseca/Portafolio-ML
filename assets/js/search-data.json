{
  
    
  
    
  
    
        "post2": {
            "title": "Caso De Estudio 2   Parkinsons Disease Descripción Y Eda",
            "content": "Parkinsons Disease Data Set Case Study . 1) Case Study introduction . “Parkinson’s disease is a degenerative brain disorder which usually presents symptoms like stiffness, body tremors, and difficulty maintaining balance and coordination. The symptoms get worse with age and usually start developing around 60 years of age. . There is currently no way to diagnose Parkinson’s using blood or laboratory tests for non-genetic cases. Diagnoses are done using patients’ medical history and neurological exams. It is important to diagnose Parkinson’s early, due to very similar symptoms it has to other diseases which require different treatments.”[1] . 2) Brief description of the data set and a summary of its attributes . The dataset was created by Max Little (University of Oxford) in collaboration with the National Centre for Voice and Speech (Denver, Colorado), who recorded speech signals. . It contains biomedical voice measurements from 31 people, 23 with Parkinson’s disease. The attributes are particular voice measures, and the examples correspond with the 195 voice recordings. . Attributes: . name - ASCII subject name and recording number . MDVP:Fo(Hz) - Average vocal fundamental frequency . MDVP:Fhi(Hz) - Maximum vocal fundamental frequency . MDVP:Flo(Hz) - Minimum vocal fundamental frequency . MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several . measures of variation in fundamental frequency . MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude . NHR,HNR - Two measures of ratio of noise to tonal components in the voice . status - Health status of the subject (one) - Parkinson’s, (zero) - healthy . RPDE,D2 - Two nonlinear dynamical complexity measures . DFA - Signal fractal scaling exponent . spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation . (Attribute information from Parkinson.names file in the UCI Repository). . All attributes but name (pacient name) and status (pacient has the disease (1) or not (0)) are real values. The pacient name is not important as it shouldn’t have any descriptive insight to whether a person has Parkinson or not, so we will later remove that attribute. This makes all our predictors real values. . 3) Initial plan for data exploration . The initial plan is to explore attribute distributions, correlations and detect outliers within the distributions. . First, I have decided to plot (sns pairplot) one of each type of attribute so as to have a clearer visualization than if we had called sns pairplot on the entire dataset. By type of attribute im referring to choosing only one predictor that models a certain characteristic. For example, I only plot NHR instead of plotting NHR and HNR in order to reduce the number of graphs created. . . By checking at some pairs of attributes, we can notice some degree of separation between both classes. For example, we can see that when comparing NHR and spread1, patients who have parkinsons disease have higher values of spread 1 than patients who don’t have the disease. The outlier points in the majority of the plots tend to belong to the parkinsons class too. . The highest correlations between attributes and status are 0.56 (spread1) and 0.53 (PPE). . 4) Feature engineering and data cleaning . Regarding Data cleaning, not much is required for the selected dataset. We don’t have highly correlated columns and there are no null values. Thus we don’t need to think about which would be the best strategy (imputation, drop column, drop row, etc.). The only cleaning made was to drop the “pacient name” attribute. . As for feature engineering, I have noticed the dataset has some attributes whose distribution is heavily skewed. . . Due to this fact, I have decided to use a log transformation for attributes with a skew value greater than 1. This makes the distribution of the transformed predictors to resemble a gaussian distribution, which is a prerequisite for many ML algorithms. . Attributes’ distribution after transformation: . . The next step is to normalize the data. I chose to use z transformation, but Min-Max normalization is another option . . 5) Key findings and insights . By comparing the distributions of parkinson patients’ attributes with non- parkinson patients’ attributes, we can identify some insights which were previously mentioned. The range of values in almost all of the attributes is undeniably greater than the one In attributes of non- parkinson patients. This could be a key insight when trying to predict a parkinson patient. For example, if spread1 is greater than -4, we could infer that the patient has parkinson. Also, the higher spread1 is, the more likely a given patient has parkinson. Also, outlier points tend to correspond with patients who have the disease. . 6) Formulate hypothesis and . 7) Conduct a formal significance tests, discussing results . 1-Null hypothesis: The mean of spread1 for parkinsons patients is the same than the mean of spread1 for non-parkinsons patients. . Alternative hypothesis: The mean is different . . Thus, we reject the null hypothesis by a great margin. The means are different. . 2-Null hypothesis: The mean of spread1 for parkinsons patients is larger than the mean of spread1 for non-parkinsons patients by at least 1.5 standard deviations of the attribute’s distribution. . Alternative-hypothesis: The mean of spread1 for parkinsons patients is not larger than the mean of spread1 for non-parkinsons patients by at least 1.5 standard deviations of the attribute’s distribution. . . The null hypothesis is accepted, so we can confirm with 95%confidence that the null hypothesis is correct. . 3-Null hypothesis: The mean of DFA for parkinsons patients is larger than the mean of DFA for non-parkinsons patients by at least 1 standard deviation of the attributes distribution. . Alternative-hypothesis: The mean of DFA for parkinsons patients is not larger than the mean of DFA for non-parkinsons patients by at least 1 standard deviation of the attributes distribution. . . The null hypothsis is rejected, so we can’t confirm with a 95% confidence that the mean of DFA for parkinsons’ patients is larger than the mean of DFA for non-parkinsons’ patients by at least 1 standard deviation of the attributes distribution. . 8) Suggestions for next steps in analyzing this data. . I would suggest to analyze and compare the distributions of attributes within the same attribute types (attributes that measure the same thing). It is important to separate the distributions by status, so we can check the distribution for parkinsons patients and non-parkinsons patients. . Also, we could implement Principal component analysis to check which attributes have the greatest variance. . 9) Summary of quality of data . I believe the quality of the dataset is decent. There are no missing values and you can visually have an idea of which attributes are going to have a greater effect on predictions (such as spread1). . However, the dataset only has 195 rows, so we could be building this whole EDA on a biased small group of individuals. If more data could be generated with patients from different parts of the world, different age ranges and social status, I would be more confident that the model generated with this data would be a solid one, capable of making correct predictions from samples all round the world. . References: . [1]: https://www.beloit.edu/live/profiles/2530-employing-data-mining-techniques-on-biomedical . https://archive.ics.uci.edu/ml/datasets/parkinsons . Required Citations:  ’Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection’, Little MA, McSharry PE, Roberts SJ, Costello DAE, Moroz IM. BioMedical Engineering OnLine 2007, 6:23 (26 June 2007) .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/11/12/Caso-de-Estudio-2-Parkinsons-Disease-Descripci%C3%B3n-y-EDA.html",
            "relUrl": "/2021/11/12/Caso-de-Estudio-2-Parkinsons-Disease-Descripci%C3%B3n-y-EDA.html",
            "date": " • Nov 12, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Diferentes Estrategias Para Feature Selection   Dataset Radar",
            "content": "Intervalos . 1) 9-13 . 2) 19-25 . 3) 33-37 . 4) 42-50 . Ejercicio 1: Benchmarking . Se obtienen los mismos resultados con o sin la corrección de Laplace. . ## . ## . Ejercicio 2: Forward selection . . Atributos seleccionados: 12, 15, 17, 18 . El algoritmo encuentra un máximo local (atributo 12) en 4 iteraciones. . Ejercicio 3: Backward Elimination . . Atributos descartados: 3, 14, 20, 36, 47, 48, 52, 59 . Ejercicio 4 . . . Análisis personal de los resultados . La mejor exactitud fue obtenida por el modelo en el que se utilizó el enfoque genético para la selección de atributos. Igualmente, considero que se eligieron demasiados atributos, muchos de los cuales no hacen más que incluir ruido en el modelo. . Debido a esto, decido indagar más en los parámetros del bloque optimize selection y encuentro una opción con la que se puede limitar la cantidad de atributos seleccionados. Escojo el valor 10. Los resultados son los siguientes. . 10 atributos: . . Se obtuvieron mejores resultados y los atributos seleccionados tienen más sentido cuando se los compara con los intervalos en los que las 2 gráficas (mine y rock) están más separadas. .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/10/10/Diferentes-estrategias-para-Feature-Selection-Dataset-Radar.html",
            "relUrl": "/2021/10/10/Diferentes-estrategias-para-Feature-Selection-Dataset-Radar.html",
            "date": " • Oct 10, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Title",
            "content": "Se importa el dataset y se imprimen las 10 primeras filas . import os, types import pandas as pd from botocore.client import Config import ibm_boto3 def __iter__(self): return 0 # @hidden_cell # The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials. # You might want to remove those credentials before you share the notebook. if os.environ.get(&#39;RUNTIME_ENV_LOCATION_TYPE&#39;) == &#39;external&#39;: endpoint_1d798346e565465c95da182cbfe9fccb = &#39;https://s3.us.cloud-object-storage.appdomain.cloud&#39; else: endpoint_1d798346e565465c95da182cbfe9fccb = &#39;https://s3.private.us.cloud-object-storage.appdomain.cloud&#39; client_1d798346e565465c95da182cbfe9fccb = ibm_boto3.client(service_name=&#39;s3&#39;, ibm_api_key_id=&#39;MiApiKey&#39;, ibm_auth_endpoint=&quot;https://iam.cloud.ibm.com/oidc/token&quot;, config=Config(signature_version=&#39;oauth&#39;), endpoint_url=endpoint_1d798346e565465c95da182cbfe9fccb) body = client_1d798346e565465c95da182cbfe9fccb.get_object(Bucket=&#39;facultadia-donotdelete-pr-qcdq5gqyjj8z2s&#39;,Key=&#39;wine.csv&#39;)[&#39;Body&#39;] # add missing __iter__ method, so pandas accepts body as file-like object if not hasattr(body, &quot;__iter__&quot;): body.__iter__ = types.MethodType( __iter__, body ) df = pd.read_csv(body) df.head(10) . Class Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline . 0 1 | 14.23 | 1.71 | 2.43 | 15.6 | 127 | 2.80 | 3.06 | 0.28 | 2.29 | 5.64 | 1.04 | 3.92 | 1065 | . 1 1 | 13.20 | 1.78 | 2.14 | 11.2 | 100 | 2.65 | 2.76 | 0.26 | 1.28 | 4.38 | 1.05 | 3.40 | 1050 | . 2 1 | 13.16 | 2.36 | 2.67 | 18.6 | 101 | 2.80 | 3.24 | 0.30 | 2.81 | 5.68 | 1.03 | 3.17 | 1185 | . 3 1 | 14.37 | 1.95 | 2.50 | 16.8 | 113 | 3.85 | 3.49 | 0.24 | 2.18 | 7.80 | 0.86 | 3.45 | 1480 | . 4 1 | 13.24 | 2.59 | 2.87 | 21.0 | 118 | 2.80 | 2.69 | 0.39 | 1.82 | 4.32 | 1.04 | 2.93 | 735 | . 5 1 | 14.20 | 1.76 | 2.45 | 15.2 | 112 | 3.27 | 3.39 | 0.34 | 1.97 | 6.75 | 1.05 | 2.85 | 1450 | . 6 1 | 14.39 | 1.87 | 2.45 | 14.6 | 96 | 2.50 | 2.52 | 0.30 | 1.98 | 5.25 | 1.02 | 3.58 | 1290 | . 7 1 | 14.06 | 2.15 | 2.61 | 17.6 | 121 | 2.60 | 2.51 | 0.31 | 1.25 | 5.05 | 1.06 | 3.58 | 1295 | . 8 1 | 14.83 | 1.64 | 2.17 | 14.0 | 97 | 2.80 | 2.98 | 0.29 | 1.98 | 5.20 | 1.08 | 2.85 | 1045 | . 9 1 | 13.86 | 1.35 | 2.27 | 16.0 | 98 | 2.98 | 3.15 | 0.22 | 1.85 | 7.22 | 1.01 | 3.55 | 1045 | . #3, 4 5 y 6 con una única línea de código # Mediante el describe, podemos ver que todos los atributos son numéricos. De lo contrario, podríamos utilizar el código comentado anteriormente para convertir el tipo a numérico. df.describe(include=&#39;all&#39;) . Class Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline . count 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | . mean 1.938202 | 13.000618 | 2.336348 | 2.366517 | 19.494944 | 99.741573 | 2.295112 | 2.029270 | 0.361854 | 1.590899 | 5.058090 | 0.957449 | 2.611685 | 746.893258 | . std 0.775035 | 0.811827 | 1.117146 | 0.274344 | 3.339564 | 14.282484 | 0.625851 | 0.998859 | 0.124453 | 0.572359 | 2.318286 | 0.228572 | 0.709990 | 314.907474 | . min 1.000000 | 11.030000 | 0.740000 | 1.360000 | 10.600000 | 70.000000 | 0.980000 | 0.340000 | 0.130000 | 0.410000 | 1.280000 | 0.480000 | 1.270000 | 278.000000 | . 25% 1.000000 | 12.362500 | 1.602500 | 2.210000 | 17.200000 | 88.000000 | 1.742500 | 1.205000 | 0.270000 | 1.250000 | 3.220000 | 0.782500 | 1.937500 | 500.500000 | . 50% 2.000000 | 13.050000 | 1.865000 | 2.360000 | 19.500000 | 98.000000 | 2.355000 | 2.135000 | 0.340000 | 1.555000 | 4.690000 | 0.965000 | 2.780000 | 673.500000 | . 75% 3.000000 | 13.677500 | 3.082500 | 2.557500 | 21.500000 | 107.000000 | 2.800000 | 2.875000 | 0.437500 | 1.950000 | 6.200000 | 1.120000 | 3.170000 | 985.000000 | . max 3.000000 | 14.830000 | 5.800000 | 3.230000 | 30.000000 | 162.000000 | 3.880000 | 5.080000 | 0.660000 | 3.580000 | 13.000000 | 1.710000 | 4.000000 | 1680.000000 | . Normalizar MinMax y estandarizar . df_class = df[&quot;Class&quot;] df = df.drop(columns=[&quot;Class&quot;]) df_min_max = df.copy() df_normalized = df.copy() df_normalized.head() . Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline . 0 14.23 | 1.71 | 2.43 | 15.6 | 127 | 2.80 | 3.06 | 0.28 | 2.29 | 5.64 | 1.04 | 3.92 | 1065 | . 1 13.20 | 1.78 | 2.14 | 11.2 | 100 | 2.65 | 2.76 | 0.26 | 1.28 | 4.38 | 1.05 | 3.40 | 1050 | . 2 13.16 | 2.36 | 2.67 | 18.6 | 101 | 2.80 | 3.24 | 0.30 | 2.81 | 5.68 | 1.03 | 3.17 | 1185 | . 3 14.37 | 1.95 | 2.50 | 16.8 | 113 | 3.85 | 3.49 | 0.24 | 2.18 | 7.80 | 0.86 | 3.45 | 1480 | . 4 13.24 | 2.59 | 2.87 | 21.0 | 118 | 2.80 | 2.69 | 0.39 | 1.82 | 4.32 | 1.04 | 2.93 | 735 | . df_normalized =(df_normalized-df_normalized.mean())/df_normalized.std() df_normalized[&quot;Class&quot;] = df_class df_normalized.head() . Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline Class . 0 1.514341 | -0.560668 | 0.231400 | -1.166303 | 1.908522 | 0.806722 | 1.031908 | -0.657708 | 1.221438 | 0.251009 | 0.361158 | 1.842721 | 1.010159 | 1 | . 1 0.245597 | -0.498009 | -0.825667 | -2.483841 | 0.018094 | 0.567048 | 0.731565 | -0.818411 | -0.543189 | -0.292496 | 0.404908 | 1.110317 | 0.962526 | 1 | . 2 0.196325 | 0.021172 | 1.106214 | -0.267982 | 0.088110 | 0.806722 | 1.212114 | -0.497005 | 2.129959 | 0.268263 | 0.317409 | 0.786369 | 1.391224 | 1 | . 3 1.686791 | -0.345835 | 0.486554 | -0.806975 | 0.928300 | 2.484437 | 1.462399 | -0.979113 | 1.029251 | 1.182732 | -0.426341 | 1.180741 | 2.328007 | 1 | . 4 0.294868 | 0.227053 | 1.835226 | 0.450674 | 1.278379 | 0.806722 | 0.661485 | 0.226158 | 0.400275 | -0.318377 | 0.361158 | 0.448336 | -0.037767 | 1 | . df_min_max =(df_min_max-df_min_max.min())/(df_min_max.max()-df_min_max.min()) df_min_max[&quot;Class&quot;] = df_class df_min_max.head() . Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline Class . 0 0.842105 | 0.191700 | 0.572193 | 0.257732 | 0.619565 | 0.627586 | 0.573840 | 0.283019 | 0.593060 | 0.372014 | 0.455285 | 0.970696 | 0.561341 | 1 | . 1 0.571053 | 0.205534 | 0.417112 | 0.030928 | 0.326087 | 0.575862 | 0.510549 | 0.245283 | 0.274448 | 0.264505 | 0.463415 | 0.780220 | 0.550642 | 1 | . 2 0.560526 | 0.320158 | 0.700535 | 0.412371 | 0.336957 | 0.627586 | 0.611814 | 0.320755 | 0.757098 | 0.375427 | 0.447154 | 0.695971 | 0.646933 | 1 | . 3 0.878947 | 0.239130 | 0.609626 | 0.319588 | 0.467391 | 0.989655 | 0.664557 | 0.207547 | 0.558360 | 0.556314 | 0.308943 | 0.798535 | 0.857347 | 1 | . 4 0.581579 | 0.365613 | 0.807487 | 0.536082 | 0.521739 | 0.627586 | 0.495781 | 0.490566 | 0.444795 | 0.259386 | 0.455285 | 0.608059 | 0.325963 | 1 | . from sklearn.model_selection import train_test_split # 9 Creación de train y test df_normalized_y = df_normalized[&quot;Class&quot;] df_normalized_x = df_normalized.drop(columns=[&quot;Class&quot;]) #Realizo la division con el dataset normalizado con transformación z, pero se podría hacer análogamente para el minmax. Se aplica un shuffle al dataset antes de realizar el split #con la seed 42 x_train, x_test, y_train, y_test = train_test_split( df_normalized_x, df_normalized_y, test_size=0.10, random_state=42 ) . train = x_train.join(y_train) train.head() . Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline Class . 9 1.058578 | -0.882918 | -0.351810 | -1.046527 | -0.121938 | 1.094330 | 1.122011 | -1.139816 | 0.452690 | 0.932547 | 0.229909 | 1.321588 | 0.946649 | 1 | . 114 -1.134008 | -0.847112 | 0.486554 | 0.899835 | -1.102159 | 0.423244 | 0.261028 | 0.547563 | -0.962506 | -0.930899 | -0.120091 | 0.814539 | -1.149205 | 2 | . 18 1.465069 | -0.668085 | 0.413653 | -0.896807 | 0.578221 | 1.605634 | 1.902902 | -0.336302 | 0.470162 | 1.570950 | 1.192408 | 0.293405 | 2.963114 | 1 | . 66 0.134736 | -1.187265 | -2.429493 | -1.345967 | -1.522254 | 1.094330 | 1.152045 | -0.818411 | 1.203967 | 0.104349 | 0.711158 | 0.800454 | -0.777667 | 2 | . 60 -0.826061 | -1.106702 | -0.315359 | -1.046527 | 0.088110 | -0.391646 | -0.940343 | 2.154591 | -2.063214 | -0.771298 | 1.279908 | -1.326335 | -0.212422 | 2 | . test = x_test.join(y_test) test.head() . Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline Class . 19 0.787585 | 0.683574 | 0.705257 | -1.286079 | 1.138347 | 0.646939 | 1.001874 | -1.541573 | 0.120730 | 0.018078 | 0.011159 | 1.053978 | 0.311541 | 1 | . 45 1.489705 | 1.525003 | 0.267850 | -0.178150 | 0.788268 | 0.886613 | 0.621440 | -0.497005 | -0.595603 | 0.078468 | -0.382591 | 1.011724 | 1.057792 | 1 | . 140 -0.086987 | 0.423984 | 1.215566 | 0.450674 | -0.261969 | -1.206537 | -1.531017 | 1.351077 | -1.469181 | -0.197599 | -0.820091 | -0.424915 | -0.466465 | 3 | . 30 0.898446 | -0.748647 | 1.215566 | 0.899835 | 0.088110 | 1.126287 | 1.222125 | -0.577356 | 1.378682 | 0.276890 | 1.017408 | 0.138473 | 1.708777 | 1 | . 67 -0.776789 | -1.044043 | -1.627580 | 0.031458 | -1.522254 | -0.295777 | -0.029303 | -0.738059 | -0.962506 | -0.163090 | 0.711158 | 1.222995 | -0.752263 | 2 | .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/10/10/DataPrepWine.html",
            "relUrl": "/2021/10/10/DataPrepWine.html",
            "date": " • Oct 10, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Clustering",
            "content": "Clustering . En este post me gustaría hablar sobre diferentes estrategias de clustering de datos para problemas supervisados y no supervisados. . Previo a un análisis de casos especiales, me parece pertinente hacer una breve introducción al concepto de clustering. Supongamos que se tiene un conjunto de datos estructurado o tabular como por ejemplo un archivo .csv. En él habrá una gran cantidad de filas o ejemplos y otra cantidad de columnas o atributos. ¿Cómo podríamos hacer para extraer información relevante de este conjunto de datos? Si nuestro dataset contiene información de clientes de un supermercado y de los artículos que este compra, algo que podríamos querer hacer es agrupar a diferentes tipos de clientes para poder realizar ofertas direccionadas. El proceso de agrupar o armar clusters es lo que se conoce como clustering y tiene el propósito de extraer información mediante diferentes formas de agrupación. . Hierarchical clustering . El clustering jerárquico es un método de análisis de clusters que busca construir una jerarquía de clusters. Este tipo de clustering se basa en la idea que explica que un objeto está más relacionado con un ejemplo cercano que con uno que está a una distancia mayor. Por lo tanto, el clustering jerárquico conecta objetos o ejemplos basándose en la distancia entre ambos. A distancias diferentes, se formarán distintos clusters, que se pueden representar utilizando un dendograma. . Existen 2 estrategias para clustering jerárquico: . Agglomerative: Cada ejemplo comienza en su cluster propio y a medida que se aumenta la distancia (a medida que nos movemos hacia arriba en la jerarquía), se irán formando las agrupaciones. . Divisive: Los ejemplos comienzan agrupadops en un único cluster y se van separando recursivamente a medida que bajamos en la jerarquía. . Existen diferentes modos de medir las distancias entre clusters. Cada una tiene sus casos de aplicación particular, pero en el siguiente ejercicio, analizaremos el propósito de 2 de estos modos (complete link y single link). . K-Means . K-means es un tipo de clustering en el que el número de clusters a generar es un parámetro del algoritmo (k). Este algoritmo iniciará escogiendo k puntos aleatorios del dataset (centroides) y asignando un cluster a cada punto dependiendo de qué centroide esté más cerca de él. Luego, de forma iterativa, se vuelven a calcular los centroides tal que estos centroides (no necesariamente pertenecientes al dataset) minimicen la suma de las distancias a todos los puntos del cluster. Luego, se reclusterizan los puntos dependiendo de la proximidad de los centroides recalculados. La heurística termina cuando los centroides se hayan estabilizado o luego de un número máximo de iteraciones. Este algoritmo no siempre devuelve los mismos agrupamientos debido a la inicialización randómica de los centroides, por lo que es una buena práctica correr el algoritmo múltiples veces. . Ejercicio . Se desea agrupar los ejemplos del dataset ripley provisto por rapidminer en clusters. . Nota: Es de crucial importancia normalizar los atributos previo a realizar clustering, pues las formas de clustering discutidas utilizan la distancia euclídea para medir la similaridad entre ejemplos o clusters. . . Agglomerative clustering . Para el agglomerative clustering, se decide hacer una comparación entre la utilización entre complete link y single link. . Complete link y distancia euclídea . . Single link y distancia euclídea . . Como se puede observar en las imágenes anteriores, la utilización de comlpete link obtiene clusters más parejos (uniformes) y distancias intraclusters medias menores que el single link. Aquí ya podemos entender el propósito de la utilización de cada uno. . Si se desea obtener clusters uniformes y parejos, la mejor forma de medir distancias entre clusters es complete link. . Si, por el contrario se desea localizar datos extremos o outliers, single link es la forma de distancia a elegir. . Resultados para agglomerative clustering con complete link: . Avg. within cluster distance: -31.972 . Avg. within cluster distance for cluster 0: -37.551 . Avg. within cluster distance for cluster 1: -13.095 . Avg. within cluster distance for cluster 2: -34.942 . Top-Down Clustering . En cuanto a top-down clustering, se decide utilizar k-means con Max depth=5 y . Max leaf size=1. . . Resultados: . kmeans k=2 . Avg. within cluster distance: -36.971 . Avg. within cluster distance for cluster 0: -55.705 . Avg. within cluster distance for cluster 1: -23.788 . Avg. within cluster distance for cluster 2: -13.534 . Conclusiones . Se puede observar que la estrategia de aglomerative clustering obtuvo una menor distancia promedio intercluster que la estrategia utilizando top-down clustering con kmeans. Debido a esto, podemos argumentar que para este dataset, la mejor estrategia es la de agglomerative clustering . Referencias: . https://docs.rapidminer.com/latest/studio/operators/modeling/segmentation/agglomerative_clustering.html .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/10/09/Clustering.html",
            "relUrl": "/2021/10/09/Clustering.html",
            "date": " • Oct 9, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Naive Bayes   Dataset Golf",
            "content": "Naive bayes . Comparación entre utilización de atributos numéricos sin aplicar transformaciones y su conversión a atributos categóricas. . Se utiliza el dataset golf provisto por Rapidminer para este ejercicio. . Parte 1 . Conversión de atributos numéricos a categóricos (multinominales). Esto se aplica tanto para el dataset de entrenamiento como para el dataset de test. . Se genera el modelo con Naive Bayes y los resultados fueron los siguientes: . . La exactitud obtenida es de 10/14 . Parte 2 . Los atributos numéricos se tratan como numéricos. Se genera el modelo con Naive bayes y los resultados fueron los siguientes: . . La exactitud obtenida es de 9/14 . Comparación de resultados . En este caso podríamos concluir que el modelo generado con las variables categóricas tiene mejores resultados que el de variables numéricas.  . Las distribuciones no se asemejan a una distribución gaussiana. De aquí pueden derivar los peores resultados al tratar a los atributos como numéricos. . .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/18/Naive-Bayes-Dataset-Golf.html",
            "relUrl": "/2021/09/18/Naive-Bayes-Dataset-Golf.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Knn   Dataset Iris",
            "content": "KNN Iris . Se crea un nuevo proceso con el dataset Iris de UCI. Luego, se grafican petal length y petal width en un gráfico bidimensional, diferenciando las diferentes especies por color. . . Claramente la especie iris-setosa es muy distinta a las otras 2 y por lo tanto un modelo construido con knn debería reconocer perfectamente a todos los ejemplos de dicha clase. Para el caso de iris versicolor e iris virginica, la separación es más difusa, y es muy probable que nuestro modelo de knn cometa errores al clasificar algunos ejemplos de ellas. . En cuanto a las tareas de acondicionamiento necesarias para este dataset y este algoritmo, se deberán normalizar todos los atributos dado que KNN trabaja con distancias. Una diferencia en escala entre los atributos podría perjudicar substancialmente la performance del modelo. . Operador Knn de rapidminer . K: Refiere a la cantidad de vecinos más cercanos que utilizará el algoritmo para hacer una predicción sobre un ejemplo no visto. . Weighted vote: La aplicación de un voto ponderado refiere a que; dentro de los k vecinos escogidos, el más cercano a un nuevo ejemplo será el que tendrá el mayor peso al aplicar la predicción y el más lejano será el que tendrá el menor peso. . Measure types: El tipo de medida que se utilizará para encontrar los vecinos más cercanos. Aquí se podrá mencionar si se tendrán atributos categóricos, numéricos ambos, etc. . Función de medida: Se podrá seleccionar un tipo de distancia tal que sea acorde con el tipo de medida que se seleccionó anteriormente. Por ejemplo para datos numéricos, se puede utilizar la distancia euclídea, mientras que para datos nominales se puede utilizar la Jaccard similarity. . Entrenamiento de modelo y resultados . K=5 . K=3 . K=1 . K=1 Distancia= Chebychev Distance . Análisis de resultados . Contrario a lo que hubiera pensado, los mejores resultados fueron obtenidos con k=1 para este Split de este dataset. Las distancias de manhattan y euclídea presentaron los mismos resultados para k = 1, mientras que la de chebychev presentó una exactitud levemente menor. . Mi hipótesis inicial para la clase iris-setosa resultó ser cierta para la mayoría de los modelos creados (menos para el modelo en el que se utiliza la distancia de chebychev). Esta clase obtuvo una precisión y un recall de 100%. .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/16/KNN-Dataset-Iris.html",
            "relUrl": "/2021/09/16/KNN-Dataset-Iris.html",
            "date": " • Sep 16, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Árboles De Decisión   Dataset Iris Rapidminer",
            "content": "Utilización de árboles de decisión para clasificar los ejemplos del dataset Iris utilizando Rapidminer. . . . .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/15/%C3%81rboles-de-decisi%C3%B3n-Dataset-Iris-Rapidminer.html",
            "relUrl": "/2021/09/15/%C3%81rboles-de-decisi%C3%B3n-Dataset-Iris-Rapidminer.html",
            "date": " • Sep 15, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Title",
            "content": "import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline . df = pd.read_csv(&quot;Iris.csv&quot;) df.head() . Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species . 0 1 | 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa | . 1 2 | 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa | . 2 3 | 4.7 | 3.2 | 1.3 | 0.2 | Iris-setosa | . 3 4 | 4.6 | 3.1 | 1.5 | 0.2 | Iris-setosa | . 4 5 | 5.0 | 3.6 | 1.4 | 0.2 | Iris-setosa | . df.drop(columns=[&quot;Id&quot;], inplace=True) df.describe(include=&quot;all&quot;) . SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species . count 150.000000 | 150.000000 | 150.000000 | 150.000000 | 150 | . unique NaN | NaN | NaN | NaN | 3 | . top NaN | NaN | NaN | NaN | Iris-setosa | . freq NaN | NaN | NaN | NaN | 50 | . mean 5.843333 | 3.054000 | 3.758667 | 1.198667 | NaN | . std 0.828066 | 0.433594 | 1.764420 | 0.763161 | NaN | . min 4.300000 | 2.000000 | 1.000000 | 0.100000 | NaN | . 25% 5.100000 | 2.800000 | 1.600000 | 0.300000 | NaN | . 50% 5.800000 | 3.000000 | 4.350000 | 1.300000 | NaN | . 75% 6.400000 | 3.300000 | 5.100000 | 1.800000 | NaN | . max 7.900000 | 4.400000 | 6.900000 | 2.500000 | NaN | . Visualizaci&#243;n de Datos . ax = plt.axes() ax.scatter(df[&quot;SepalLengthCm&quot;], df[&quot;SepalWidthCm&quot;]) ax.set(xlabel=&#39;Sepal Length&#39;, ylabel=&#39;Sepal Width&#39;, title=&#39;Sepal Length vs Sepal Width&#39;) . [Text(0, 0.5, &#39;Sepal Width&#39;), Text(0.5, 0, &#39;Sepal Length&#39;), Text(0.5, 1.0, &#39;Sepal Length vs Sepal Width&#39;)] . sns.set_context(&#39;notebook&#39;) ax = df.plot.hist(bins=25, alpha=0.3) ax.set_xlabel(&#39;Size (cm)&#39;) . Text(0.5, 0, &#39;Size (cm)&#39;) . sns.set_context(&#39;talk&#39;) sns.pairplot(df, hue=&#39;Species&#39;) . &lt;seaborn.axisgrid.PairGrid at 0x1cc600ec9a0&gt; . from sklearn import tree from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder clf = tree.DecisionTreeClassifier() y = df[&quot;Species&quot;] le = LabelEncoder() y_encoded = le.fit_transform(y) X = df.drop(columns=[&quot;Species&quot;]).values . train_X, test_X, train_y, test_y = train_test_split(X, y_encoded, test_size=0.3, random_state=42, shuffle=True) . clf = clf.fit(train_X, train_y) . plt.figure(dpi=50, figsize=(50,50)) tree.plot_tree(clf) . [Text(968.75, 1730.2083333333333, &#39;X[3] &lt;= 0.8 ngini = 0.664 nsamples = 105 nvalue = [31, 37, 37]&#39;), Text(819.7115384615386, 1415.625, &#39;gini = 0.0 nsamples = 31 nvalue = [31, 0, 0]&#39;), Text(1117.7884615384617, 1415.625, &#39;X[3] &lt;= 1.75 ngini = 0.5 nsamples = 74 nvalue = [0, 37, 37]&#39;), Text(596.1538461538462, 1101.0416666666667, &#39;X[2] &lt;= 4.95 ngini = 0.214 nsamples = 41 nvalue = [0, 36, 5]&#39;), Text(298.0769230769231, 786.4583333333335, &#39;X[3] &lt;= 1.6 ngini = 0.056 nsamples = 35 nvalue = [0, 34, 1]&#39;), Text(149.03846153846155, 471.875, &#39;gini = 0.0 nsamples = 34 nvalue = [0, 34, 0]&#39;), Text(447.11538461538464, 471.875, &#39;gini = 0.0 nsamples = 1 nvalue = [0, 0, 1]&#39;), Text(894.2307692307693, 786.4583333333335, &#39;X[3] &lt;= 1.55 ngini = 0.444 nsamples = 6 nvalue = [0, 2, 4]&#39;), Text(745.1923076923077, 471.875, &#39;gini = 0.0 nsamples = 3 nvalue = [0, 0, 3]&#39;), Text(1043.269230769231, 471.875, &#39;X[2] &lt;= 5.45 ngini = 0.444 nsamples = 3 nvalue = [0, 2, 1]&#39;), Text(894.2307692307693, 157.29166666666674, &#39;gini = 0.0 nsamples = 2 nvalue = [0, 2, 0]&#39;), Text(1192.3076923076924, 157.29166666666674, &#39;gini = 0.0 nsamples = 1 nvalue = [0, 0, 1]&#39;), Text(1639.4230769230771, 1101.0416666666667, &#39;X[2] &lt;= 4.85 ngini = 0.059 nsamples = 33 nvalue = [0, 1, 32]&#39;), Text(1490.3846153846155, 786.4583333333335, &#39;X[1] &lt;= 3.1 ngini = 0.444 nsamples = 3 nvalue = [0, 1, 2]&#39;), Text(1341.3461538461538, 471.875, &#39;gini = 0.0 nsamples = 2 nvalue = [0, 0, 2]&#39;), Text(1639.4230769230771, 471.875, &#39;gini = 0.0 nsamples = 1 nvalue = [0, 1, 0]&#39;), Text(1788.4615384615386, 786.4583333333335, &#39;gini = 0.0 nsamples = 30 nvalue = [0, 0, 30]&#39;)] . y_pred = clf.predict(test_X) print(&quot;Predicted vs Expected&quot;) print(y_pred) print(test_y) . Predicted vs Expected [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0 0 0 2 1 1 0 0] [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0 0 0 2 1 1 0 0] . from sklearn.metrics import confusion_matrix test_y_inv = le.inverse_transform(test_y) y_pred_inv = le.inverse_transform(y_pred) categories = set(df[&quot;Species&quot;]) cf_matrix = confusion_matrix(test_y_inv, y_pred_inv) sns.heatmap(cf_matrix, annot=True,cmap=&#39;Blues&#39;, xticklabels=categories, yticklabels=categories) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1cc6168b100&gt; .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/15/%C3%81rboles-de-decisi%C3%B3n-Dataset-Iris-Notebook.html",
            "relUrl": "/2021/09/15/%C3%81rboles-de-decisi%C3%B3n-Dataset-Iris-Notebook.html",
            "date": " • Sep 15, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Svm",
            "content": "Máquinas de soporte vectorial . Se generó un dataset de ejemplo para mostrar un caso en el que las máquinas de soporte vectorial demuestran su potencial. El dataset utilizado es SVM-Anillos.csv. . Al visualizar los datos, las clases parecerían estar separadas mediante un anillo. Por esta razón podríamos plantearnos las siguientes hipótesis: . 1) Utilizar un kernel lineal es una mala opción, pues no existe una recta que haga una buena separación de los 2 conjuntos. . 2) Probablemente un kernel radial o uno polinómico de grado par obtenga buenos resultados . . Flujo de Rapidminer . . Cross validation . . Resultados: . Kernel lineal . . Kernel radial . . Kernel Polinómico Par e Impar . . Observamos el mismo comportamiento que con el kernel radial para un kernel polinómico de grado 2. Modificando el grado del kernel a valores pares, se puede ver una accuracy de 100%, mientras que utilizando un grado impar, los resultados se acercan más al 70-75% utilizando 10-fold-CV. . Cambio de la constante C . No se observan cambios notorios al modificar la constante c para este dataset, dado que los datos están perfectamente separados. . Conclusiones . Como conclusión, se probaron ambas hipótesis planteadas mediante el entrenamiento de diferentes máquinas de soporte vectorial con la utilización de distintos kernels. .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/15/SVM.html",
            "relUrl": "/2021/09/15/SVM.html",
            "date": " • Sep 15, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Title",
            "content": "import matplotlib import matplotlib.pyplot as plt import pandas as pd from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.metrics import confusion_matrix, classification_report from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder import seaborn as sns . input_file = &quot;sports_Training.csv&quot; df = pd.read_csv(input_file, header=0) df.head() . Edad Fuerza Velocidad Lesiones Vision Resistencia Agilidad CapacidadDecision DeportePrimario . 0 15.1 | 3 | 2 | 1 | 2 | 3 | 29 | 4 | Futbol | . 1 15.4 | 3 | 2 | 0 | 3 | 5 | 18 | 8 | Rugby | . 2 13.6 | 5 | 5 | 0 | 2 | 5 | 27 | 28 | Voleibol | . 3 18.8 | 5 | 1 | 1 | 1 | 3 | 48 | 36 | Voleibol | . 4 16.1 | 3 | 1 | 0 | 3 | 3 | 38 | 29 | Futbol | . data = df[(df[&#39;CapacidadDecision&#39;] &gt;= 3) &amp; (df[&#39;CapacidadDecision&#39;] &lt;= 100)] data.head() . Edad Fuerza Velocidad Lesiones Vision Resistencia Agilidad CapacidadDecision DeportePrimario . 0 15.1 | 3 | 2 | 1 | 2 | 3 | 29 | 4 | Futbol | . 1 15.4 | 3 | 2 | 0 | 3 | 5 | 18 | 8 | Rugby | . 2 13.6 | 5 | 5 | 0 | 2 | 5 | 27 | 28 | Voleibol | . 3 18.8 | 5 | 1 | 1 | 1 | 3 | 48 | 36 | Voleibol | . 4 16.1 | 3 | 1 | 0 | 3 | 3 | 38 | 29 | Futbol | . data.describe() . Edad Fuerza Velocidad Lesiones Vision Resistencia Agilidad CapacidadDecision . count 482.000000 | 482.000000 | 482.000000 | 482.000000 | 482.000000 | 482.000000 | 482.000000 | 482.000000 | . mean 15.954564 | 3.500000 | 1.983402 | 0.639004 | 1.692946 | 3.856846 | 33.680498 | 29.157676 | . std 1.817320 | 1.460854 | 1.505269 | 0.480788 | 1.134010 | 1.331782 | 12.523973 | 19.477265 | . min 13.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 13.000000 | 3.000000 | . 25% 14.300000 | 3.000000 | 1.000000 | 0.000000 | 1.000000 | 3.000000 | 23.000000 | 11.000000 | . 50% 15.900000 | 4.000000 | 1.000000 | 1.000000 | 2.000000 | 5.000000 | 31.500000 | 29.000000 | . 75% 17.600000 | 4.000000 | 2.000000 | 1.000000 | 3.000000 | 5.000000 | 42.750000 | 40.000000 | . max 19.000000 | 7.000000 | 6.000000 | 1.000000 | 3.000000 | 6.000000 | 80.000000 | 100.000000 | . data.dtypes . Edad float64 Fuerza int64 Velocidad int64 Lesiones int64 Vision int64 Resistencia int64 Agilidad int64 CapacidadDecision int64 DeportePrimario object dtype: object . y = data[&quot;DeportePrimario&quot;] le = LabelEncoder() y_encoded = le.fit_transform(y) . X = data.drop(columns=[&#39;DeportePrimario&#39;]).values type(X) . numpy.ndarray . type(y_encoded) . numpy.ndarray . train_X, test_X, train_y, test_y = train_test_split(X, y_encoded, test_size=0.25, random_state=41, shuffle=True) . lda = LinearDiscriminantAnalysis() lda = lda.fit(train_X, train_y) . type(test_X) . numpy.ndarray . y_pred = lda.predict(test_X) print(&quot;Predicted vs Expected&quot;) print(y_pred) print(test_y) . Predicted vs Expected [3 1 2 1 1 0 0 0 1 2 2 2 1 3 3 0 2 3 2 3 3 2 1 1 2 3 0 0 1 3 3 1 1 2 2 2 2 1 1 1 1 2 3 3 1 1 3 1 1 1 1 2 1 1 1 1 2 3 3 1 2 1 0 1 2 1 1 2 1 1 3 2 2 2 2 1 0 1 2 1 1 3 1 1 1 1 1 2 1 2 1 2 0 3 1 1 1 2 2 3 2 0 2 1 1 1 1 3 1 0 1 1 3 1 2 3 1 1 3 1 3] [1 2 2 1 3 3 2 2 1 1 2 2 1 1 3 2 2 0 2 3 1 2 2 0 2 0 1 0 1 3 0 0 2 1 0 1 2 1 3 3 3 2 0 3 1 1 1 3 3 1 1 3 3 1 3 1 0 2 3 2 1 1 0 0 3 1 1 3 2 0 0 1 1 2 0 3 1 1 0 0 2 0 1 3 3 1 2 0 3 2 2 2 0 0 2 3 1 1 2 2 2 0 1 3 2 1 1 1 1 3 2 1 3 0 2 1 1 2 3 1 3] . print(classification_report(test_y, y_pred, digits=3)) . precision recall f1-score support 0 0.364 0.182 0.242 22 1 0.429 0.600 0.500 40 2 0.484 0.469 0.476 32 3 0.348 0.296 0.320 27 accuracy 0.421 121 macro avg 0.406 0.387 0.385 121 weighted avg 0.413 0.421 0.407 121 . print(confusion_matrix(test_y, y_pred)) . [[ 4 6 5 7] [ 2 24 8 6] [ 3 12 15 2] [ 2 14 3 8]] . test_y_inv = le.inverse_transform(test_y) y_pred_inv = le.inverse_transform(y_pred) categories = [&#39;Futbol&#39;, &#39;Rugby&#39;, &quot;Basketball&quot;,&#39;Voleibol&#39;] cf_matrix = confusion_matrix(test_y_inv, y_pred_inv) sns.heatmap(cf_matrix, annot=True,cmap=&#39;Blues&#39;, xticklabels=categories, yticklabels=categories) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2022c31dd60&gt; . Predicciones en set no visto: Validaci&#243;n . validation_df = pd.read_csv(&quot;sports_Scoring.csv&quot;) validation_df.head() . Edad Fuerza Velocidad Lesiones Vision Resistencia Agilidad CapacidadDecision . 0 18.5 | 5 | 1 | 1 | 0 | 5 | 33 | 61 | . 1 13.3 | 1 | 2 | 1 | 3 | 5 | 18 | 59 | . 2 13.4 | 2 | 1 | 0 | 2 | 5 | 40 | 11 | . 3 13.6 | 4 | 1 | 0 | 0 | 5 | 28 | 0 | . 4 16.3 | 3 | 1 | 0 | 2 | 5 | 32 | 35 | . validation_df = validation_df[(validation_df[&#39;CapacidadDecision&#39;] &gt;= 3) &amp; (validation_df[&#39;CapacidadDecision&#39;] &lt;= 100)] validation_df.describe() . Edad Fuerza Velocidad Lesiones Vision Resistencia Agilidad CapacidadDecision . count 1767.000000 | 1767.000000 | 1767.000000 | 1767.000000 | 1767.000000 | 1767.000000 | 1767.000000 | 1767.000000 | . mean 15.982513 | 3.567063 | 1.987550 | 0.664969 | 1.599887 | 3.736276 | 34.013016 | 29.946237 | . std 1.729906 | 1.479915 | 1.551992 | 0.472135 | 1.188402 | 1.366378 | 12.299599 | 21.116807 | . min 13.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 13.000000 | 3.000000 | . 25% 14.500000 | 3.000000 | 1.000000 | 0.000000 | 0.000000 | 3.000000 | 24.000000 | 11.000000 | . 50% 16.000000 | 4.000000 | 1.000000 | 1.000000 | 2.000000 | 3.000000 | 32.000000 | 29.000000 | . 75% 17.400000 | 4.000000 | 2.000000 | 1.000000 | 3.000000 | 5.000000 | 43.000000 | 44.000000 | . max 19.000000 | 7.000000 | 6.000000 | 1.000000 | 3.000000 | 6.000000 | 80.000000 | 100.000000 | . validation_df.columns . Index([&#39;Edad&#39;, &#39;Fuerza&#39;, &#39;Velocidad&#39;, &#39;Lesiones&#39;, &#39;Vision&#39;, &#39;Resistencia&#39;, &#39;Agilidad&#39;, &#39;CapacidadDecision&#39;], dtype=&#39;object&#39;) . y_pred = lda.predict(validation_df.to_numpy()) . print(y_pred) . [0 2 1 ... 1 0 1] .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/12/Algoritmos-Lineales-para-clasificaci%C3%B3n-Dataset-Sports.html",
            "relUrl": "/2021/09/12/Algoritmos-Lineales-para-clasificaci%C3%B3n-Dataset-Sports.html",
            "date": " • Sep 12, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Preparación De Datos Y Modelado Con El Dataset Titanic",
            "content": "Preparación de datos y modelado para el dataset Titanic . 1) Importar librerías y paquetes . Además de hablar de las librerías y paquetes, me pareció interesante destacar ciertos conceptos que no fueron presentados en la clase y quizás alguna posible mejora para alguna parte del código en el caso de manejar datasets más pesados y más cómputo-dependientes. . Inicialmente, se importan las librerías necesarias como numpy y pandas para cálculos matemáticos y estructuras de datos además de matplotlib y seaborn para la parte de visualización de datos y gráficas. . Luego, se importan varias funcionalidades/modelos de la librería scikit learn. Los modelos importados y una breve explicación de ellos es la siguiente. . SVC, Linear SVC son algoritmos del tipo Support vector machines, los cuales buscan definir una recta, plano o hiperplano (hablando de un SVM lineal) tal que la distancia entre esta recta y el punto más cercano de cada clase sea máxima. . Dentro de los algoritmos de árboles o ensembles, se utilizan un decisionTreeClassifier, un randomForestClassifier y XGBoost. El decisionTreeClassifier construye un único árbol y lo utiliza para hacer las predicciones. El random forest construye múltiples árboles armados utilizando bagging y luego al realizar una predicción se le da un peso al resultado de cada árbol del bosque y se decide qué predice el bosque (juntando las predicciones de cada uno y sus pesos). Por último, XGBoost utiliza una forma diferente de armar los árboles dado que son dependientes uno de otro, intentando mejorar las métricas con cada árbol nuevo. . También se utiliza la técnica de Gridsearch para tunear los hiperparámetros de los distintos algoritmos. Una mejor alternativa para un caso computacionalmente complejo sería utilizar randomsearch para ubicar los puntos de interés donde es muy probable encontrar óptimos en la cercanía y aplicar gridsearch en espacios pequeños cercanos a dichos puntos de interés. . 2) &gt; Cargar el dataset y mostrarlo . Para realizar estas dos tareas, se utilizan las funcionalidades de pandas llamadas read_csv para leer un archivo csv y cargarlo utilizando la estructura de datos llamada dataframe y head como forma de imprimir las primeras x filas (5 por defecto) del dataset de una forma prolija. . 3) &gt; Gestionar los valores faltantes . Se utiliza la función pd.isnull para identificar los valores faltantes de cada columna. Se dropean las columnas Cabin y ticket por la gran cantidad de valores faltantes. . Se observa una distribución relativamente parecida a una normal para el atributo age, pero con un leve sesgo hacia la derecha, lo que determinará la estrategia de imputación utilizada. Debido a este sesgo, elegir la mediana es una mejor alternativa que elegir la media. Si no tuviéramos este sesgo, la estrategia de imputación elegida sería la media. . Para el atributo fare, la distribución está fuertemente sesgada hacia la derecha, por lo que se elige la mediana como valor a imputar. Alternativamente, se podría elegir aplicar una transformación a los datos para que su distribución se asemeje más a una normal. 2 estrategias para esto podrían ser aplicar el logaritmo natural a los valores del atributo o alguna de las variantes de la transformación “Box and Cox” (1964) presentada en applied-predictivemodeling, presente en la bibliografía del curso. . 4) &gt; Graficar los datos . Se grafican varios atributos en relación al atributo a predecir para formar unas hipótesis iniciales sobre si algún atributo está correlacionado con la variable a predecir. Las librerías para . hacer esto son seaborn y matplotlib.pyplot. Aquí se pueden observar ratios como la proporción de mujeres que sobrevivieron o indicios como qué clase es la que presentó más proporción de personas que sobrevivieron. . 5) &gt; Feature Engineering . En la parte de feature engineering, lo primero que se hace es encodear los atributos categóricos a valores enteros. Se podría realizar mediante one hot encoding pero en este notebok, se decide no hacerlo. . Una hipótesis muy interesante provista por Samson Qian (creador del notebook en kaggle) es que el titulo de la persona puede influir en si esta sobrevivió o no. Desde mi punto de vista personal, quizás esto podría estar agregando un atributo fuertemente correlacionado con Sex. Luego de calcular la matriz de correlación, se ve que la correlación es muy baja, lo que no me parece intuitivo, dado que la gran mayoría de los ejemplos de títulos (más del 90%) son Mr, Mrs y Miss lols cuales se podrían relacionar directamente con el género. . (Imagen de correlación provista debajo) . . Luego, se normalizan los datos con el standard scaler (estandarización Z). . Pasos Opcionales . Como no se requiere, no voy a comentar todos los aspectos sobre la parte de modelado, predicción y evaluación de rendimiento, pero sí los incluí en el código y me interesaría remarcar algunos puntos. . Se puede ver que los mejores resultados a primera vista, pues la única métrica que estamos mirando es accuracy (no es suficiente para tomar una decisión sobre si el modelo es bueno o no), son los de SVC y random forest, alrededor de 0.82 – 0.83, pero varía cada vez, dado que el random forest tiene un grado de aleatoriedad en su construcción. . Cabe destacar que, aunque los resultados del random forest sean los mejores para este set de testeo, el modelo tardó 60 segundos en entrenar, mientras que SVC tardó 1.2 segundos y la regresión logística 3.4s. Para conjuntos de datos más grandes o muy cambiantes para los que se necesita reentrenar el modelo constantemente, habría que ver si los pocos puntos de mejora en la accuracy justifican el costo inmensamente mayor de entrenamiento del modelo de random forest. Probablemente los tiempos de inferencia también sean mucho mayores en el random forest. . .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/11/Preparaci%C3%B3n-de-datos-y-modelado-con-el-dataset-Titanic.html",
            "relUrl": "/2021/09/11/Preparaci%C3%B3n-de-datos-y-modelado-con-el-dataset-Titanic.html",
            "date": " • Sep 11, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Normalización Y Detección De Outliers",
            "content": "Rapid Miner outlier detection and normalization tutorial. . Se aplica normalización para luego aplicar el cálculo de las distancias para detectar a los outliers. Siempre se debe normalizar previo al cálculo de los outliers por el hecho de que los diferentes atributos suelen no tener la misma unidad o escala. Para la normalización se utiliza la transformación z. . El operador detect outliers detectará los 10 ejemplos que se alejen más del resto. En el flujo, luego colocamos un filter para remover los 10 outliers antes mencionados. . Ejercicio 2 . El dataset posee 13 atributos sin incluir a la variable objetivo. Estos describen distintos elementos de la composición de vinos para luego poder clasificarlos. . La variable objetivo posee tres valores posibles. Estos valores representan a 3 diferentes variantes de viñedos (las plantas de las que se genera el vino son diferentes genéticamente entre sí). . La variable objetivo está relativamente balanceada, no hay una clase que presente un numero muy bajo de ejemplos, por lo que argumentaría que no habría que aplicar ningún tipo de sampling. . Para algunos atributos, la distribución parece ser normal (ash, alcalinity of ash, proanthocyanins), mientras que algunas otras parecen tener un sesgo hacia la derecha (por ejemplo malic acid y proline), pero no es muy grave tampoco. . Creación del modelo con naive bayes. . Al visualizar objetivamente la matriz de confusión, los resultados no variaron al aplicar la normalización. Con ambos tipos de normalización (z score y min-max), los resultados fueron los mismos. . Flujo de rapidminer: . El para los Split data, se utilizó la misma seed y la misma proporción (70/30) para que los conjuntos de entrenamiento y testeo sean los mismos para ambas pruebas, con la diferencia que uno estará normalizado y otro no. . . Matriz sin normalizado . . Matriz con normalizado . . Conclusiones . El normalizado no afecta los resultados para este caso en particular. Esto se debe al algoritmo que utilizamos principalmente. .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/10/Normalizaci%C3%B3n-y-detecci%C3%B3n-de-outliers.html",
            "relUrl": "/2021/09/10/Normalizaci%C3%B3n-y-detecci%C3%B3n-de-outliers.html",
            "date": " • Sep 10, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://josevilaseca.github.io/Portafolio-ML/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Actualmente soy un estudiante de Ingeniería en Informática y trabajo como científico de datos y desarrollador de aplicaciones desde el año 2020. El propósito de este portafolio es exponer algunos proyectos realizados en relación a Machine Learning. .",
          "url": "https://josevilaseca.github.io/Portafolio-ML/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://josevilaseca.github.io/Portafolio-ML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}