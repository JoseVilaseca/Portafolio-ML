{
  
    
  
    
  
    
        "post2": {
            "title": "Exploratory Data Analysis",
            "content": "Parkinsons Disease Data Set Case Study . 1) Brief description of the data set and a summary of its attributes . The dataset was created by Max Little (University of Oxford) in collaboration with the National Centre for Voice and Speech (Denver, Colorado), who recorded speech signals. . It contains biomedical voice measurements from 31 people, 23 with Parkinson’s disease. The attributes are particular voice measures, and the examples correspond with the 195 voice recordings. . Attributes: . name - ASCII subject name and recording number . MDVP:Fo(Hz) - Average vocal fundamental frequency . MDVP:Fhi(Hz) - Maximum vocal fundamental frequency . MDVP:Flo(Hz) - Minimum vocal fundamental frequency . MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several . measures of variation in fundamental frequency . MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude . NHR,HNR - Two measures of ratio of noise to tonal components in the voice . status - Health status of the subject (one) - Parkinson’s, (zero) - healthy . RPDE,D2 - Two nonlinear dynamical complexity measures . DFA - Signal fractal scaling exponent . spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation . (Attribute information from Parkinson.names file in the UCI Repository). . All attributes but name (pacient name) and status (pacient has the disease (1) or not (0)) are real values. The pacient name is not important as it shouldn’t have any descriptive insight to whether a person has Parkinson or not, so we will later remove that attribute. This makes all our predictors real values. . 2) Initial plan for data exploration . The initial plan is to explore attribute distributions, correlations and detect outliers within the distributions. . First, I have decided to plot (sns pairplot) one of each type of attribute so as to have a clearer visualization than if we had called sns pairplot on the entire dataset. By type of attribute im referring to choosing only one predictor that models a certain characteristic. For example, I only plot NHR instead of plotting NHR and HNR in order to reduce the number of graphs created. . . By checking at some pairs of attributes, we can notice some degree of separation between both classes. For example, we can see that when comparing NHR and spread1, patients who have parkinsons disease have higher values of spread 1 than patients who don’t have the disease. The outlier points in the majority of the plots tend to belong to the parkinsons class too. . The highest correlations between attributes and status are 0.56 (spread1) and 0.53 (PPE). . 3) Feature engineering and data cleaning . Regarding Data cleaning, not much is required for the selected dataset. We don’t have highly correlated columns and there are no null values. Thus we don’t need to think about which would be the best strategy (imputation, drop column, drop row, etc.). The only cleaning made was to drop the “pacient name” attribute. . As for feature engineering, I have noticed the dataset has some attributes whose distribution is heavily skewed. . . Due to this fact, I have decided to use a log transformation for attributes with a skew value greater than 1. This makes the distribution of the transformed predictors to resemble a gaussian distribution, which is a prerequisite for many ML algorithms. . Attributes’ distribution after transformation: . . The next step is to normalize the data. I chose to use z transformation, but Min-Max normalization is another option . . 4) Key findings and insights . By comparing the distributions of parkinson patients’ attributes with non- parkinson patients’ attributes, we can identify some insights which were previously mentioned. The range of values in almost all of the attributes is undeniably greater than the one In attributes of non- parkinson patients. This could be a key insight when trying to predict a parkinson patient. For example, if spread1 is greater than -4, we could infer that the patient has parkinson. Also, the higher spread1 is, the more likely a given patient has parkinson. Also, outlier points tend to correspond with patients who have the disease. . 5) Formulate hypothesis and . 6) Conduct a formal significance tests, discussing results . 1-Null hypothesis: The mean of spread1 for parkinsons patients is the same than the mean of spread1 for non-parkinsons patients. . Alternative hypothesis: The mean is different . . Thus, we reject the null hypothesis by a great margin. The means are different. . 2-Null hypothesis: The mean of spread1 for parkinsons patients is larger than the mean of spread1 for non-parkinsons patients by at least 1.5 standard deviations of the attribute’s distribution. . Alternative-hypothesis: The mean of spread1 for parkinsons patients is not larger than the mean of spread1 for non-parkinsons patients by at least 1.5 standard deviations of the attribute’s distribution. . . The null hypothesis is accepted, so we can confirm with 95%confidence that the null hypothesis is correct. . 3-Null hypothesis: The mean of DFA for parkinsons patients is larger than the mean of DFA for non-parkinsons patients by at least 1 standard deviation of the attributes distribution. . Alternative-hypothesis: The mean of DFA for parkinsons patients is not larger than the mean of DFA for non-parkinsons patients by at least 1 standard deviation of the attributes distribution. . . The null hypothsis is rejected, so we can’t confirm with a 95% confidence that the mean of DFA for parkinsons’ patients is larger than the mean of DFA for non-parkinsons’ patients by at least 1 standard deviation of the attributes distribution. . 7) Suggestions for next steps in analyzing this data. . I would suggest to analyze and compare the distributions of attributes within the same attribute types (attributes that measure the same thing). It is important to separate the distributions by status, so we can check the distribution for parkinsons patients and non-parkinsons patients. . Also, we could implement Principal component analysis to check which attributes have the greatest variance. . 8) Summary of quality of data . I believe the quality of the dataset is decent. There are no missing values and you can visually have an idea of which attributes are going to have a greater effect on predictions (such as spread1). . However, the dataset only has 195 rows, so we could be building this whole EDA on a biased small group of individuals. If more data could be generated with patients from different parts of the world, different age ranges and social status, I would be more confident that the model generated with this data would be a good one. . References: . https://archive.ics.uci.edu/ml/datasets/parkinsons . Required Citations:  ’Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection’, Little MA, McSharry PE, Roberts SJ, Costello DAE, Moroz IM. BioMedical Engineering OnLine 2007, 6:23 (26 June 2007) .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/11/12/Exploratory-data-analysis.html",
            "relUrl": "/2021/11/12/Exploratory-data-analysis.html",
            "date": " • Nov 12, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Title",
            "content": "import os, types import pandas as pd # def __iter__(self): return 0 # # @hidden_cell # # The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials. # # You might want to remove those credentials before you share the notebook. # if os.environ.get(&#39;RUNTIME_ENV_LOCATION_TYPE&#39;) == &#39;external&#39;: # endpoint_edb04af4bfbd4b478c6eb955554ec728 = &#39;https://s3.us.cloud-object-storage.appdomain.cloud&#39; # else: # endpoint_edb04af4bfbd4b478c6eb955554ec728 = &#39;https://s3.private.us.cloud-object-storage.appdomain.cloud&#39; # client_edb04af4bfbd4b478c6eb955554ec728 = ibm_boto3.client(service_name=&#39;s3&#39;, # ibm_api_key_id=&#39;DdD1_G6mjg6CJl0hlrqBBsb5SccoiGMT0tlWEdsmX4tA&#39;, # ibm_auth_endpoint=&quot;https://iam.cloud.ibm.com/oidc/token&quot;, # config=Config(signature_version=&#39;oauth&#39;), # endpoint_url=endpoint_edb04af4bfbd4b478c6eb955554ec728) # body = client_edb04af4bfbd4b478c6eb955554ec728.get_object(Bucket=&#39;exploratorydataanalysisformachine-donotdelete-pr-a9r235otzochhr&#39;,Key=&#39;parkinsons.csv&#39;)[&#39;Body&#39;] # # add missing __iter__ method, so pandas accepts body as file-like object # if not hasattr(body, &quot;__iter__&quot;): body.__iter__ = types.MethodType( __iter__, body ) df_data_1 = pd.read_csv(&quot;./data/parkinsons.csv&quot;) df_data_1.head() . name MDVP:Fo(Hz) MDVP:Fhi(Hz) MDVP:Flo(Hz) MDVP:Jitter(%) MDVP:Jitter(Abs) MDVP:RAP MDVP:PPQ Jitter:DDP MDVP:Shimmer ... Shimmer:DDA NHR HNR status RPDE DFA spread1 spread2 D2 PPE . 0 phon_R01_S01_1 | 119.992 | 157.302 | 74.997 | 0.00784 | 0.00007 | 0.00370 | 0.00554 | 0.01109 | 0.04374 | ... | 0.06545 | 0.02211 | 21.033 | 1 | 0.414783 | 0.815285 | -4.813031 | 0.266482 | 2.301442 | 0.284654 | . 1 phon_R01_S01_2 | 122.400 | 148.650 | 113.819 | 0.00968 | 0.00008 | 0.00465 | 0.00696 | 0.01394 | 0.06134 | ... | 0.09403 | 0.01929 | 19.085 | 1 | 0.458359 | 0.819521 | -4.075192 | 0.335590 | 2.486855 | 0.368674 | . 2 phon_R01_S01_3 | 116.682 | 131.111 | 111.555 | 0.01050 | 0.00009 | 0.00544 | 0.00781 | 0.01633 | 0.05233 | ... | 0.08270 | 0.01309 | 20.651 | 1 | 0.429895 | 0.825288 | -4.443179 | 0.311173 | 2.342259 | 0.332634 | . 3 phon_R01_S01_4 | 116.676 | 137.871 | 111.366 | 0.00997 | 0.00009 | 0.00502 | 0.00698 | 0.01505 | 0.05492 | ... | 0.08771 | 0.01353 | 20.644 | 1 | 0.434969 | 0.819235 | -4.117501 | 0.334147 | 2.405554 | 0.368975 | . 4 phon_R01_S01_5 | 116.014 | 141.781 | 110.655 | 0.01284 | 0.00011 | 0.00655 | 0.00908 | 0.01966 | 0.06425 | ... | 0.10470 | 0.01767 | 19.649 | 1 | 0.417356 | 0.823484 | -3.747787 | 0.234513 | 2.332180 | 0.410335 | . 5 rows × 24 columns . df = df_data_1.copy() . df.shape . (195, 24) . print(df.columns) . Index([&#39;name&#39;, &#39;MDVP:Fo(Hz)&#39;, &#39;MDVP:Fhi(Hz)&#39;, &#39;MDVP:Flo(Hz)&#39;, &#39;MDVP:Jitter(%)&#39;, &#39;MDVP:Jitter(Abs)&#39;, &#39;MDVP:RAP&#39;, &#39;MDVP:PPQ&#39;, &#39;Jitter:DDP&#39;, &#39;MDVP:Shimmer&#39;, &#39;MDVP:Shimmer(dB)&#39;, &#39;Shimmer:APQ3&#39;, &#39;Shimmer:APQ5&#39;, &#39;MDVP:APQ&#39;, &#39;Shimmer:DDA&#39;, &#39;NHR&#39;, &#39;HNR&#39;, &#39;status&#39;, &#39;RPDE&#39;, &#39;DFA&#39;, &#39;spread1&#39;, &#39;spread2&#39;, &#39;D2&#39;, &#39;PPE&#39;], dtype=&#39;object&#39;) . df.dtypes . name object MDVP:Fo(Hz) float64 MDVP:Fhi(Hz) float64 MDVP:Flo(Hz) float64 MDVP:Jitter(%) float64 MDVP:Jitter(Abs) float64 MDVP:RAP float64 MDVP:PPQ float64 Jitter:DDP float64 MDVP:Shimmer float64 MDVP:Shimmer(dB) float64 Shimmer:APQ3 float64 Shimmer:APQ5 float64 MDVP:APQ float64 Shimmer:DDA float64 NHR float64 HNR float64 status int64 RPDE float64 DFA float64 spread1 float64 spread2 float64 D2 float64 PPE float64 dtype: object . import seaborn as sns import matplotlib.pyplot as plt import statistics . df.describe() . MDVP:Fo(Hz) MDVP:Fhi(Hz) MDVP:Flo(Hz) MDVP:Jitter(%) MDVP:Jitter(Abs) MDVP:RAP MDVP:PPQ Jitter:DDP MDVP:Shimmer MDVP:Shimmer(dB) ... Shimmer:DDA NHR HNR status RPDE DFA spread1 spread2 D2 PPE . count 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | ... | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | . mean 154.228641 | 197.104918 | 116.324631 | 0.006220 | 0.000044 | 0.003306 | 0.003446 | 0.009920 | 0.029709 | 0.282251 | ... | 0.046993 | 0.024847 | 21.885974 | 0.753846 | 0.498536 | 0.718099 | -5.684397 | 0.226510 | 2.381826 | 0.206552 | . std 41.390065 | 91.491548 | 43.521413 | 0.004848 | 0.000035 | 0.002968 | 0.002759 | 0.008903 | 0.018857 | 0.194877 | ... | 0.030459 | 0.040418 | 4.425764 | 0.431878 | 0.103942 | 0.055336 | 1.090208 | 0.083406 | 0.382799 | 0.090119 | . min 88.333000 | 102.145000 | 65.476000 | 0.001680 | 0.000007 | 0.000680 | 0.000920 | 0.002040 | 0.009540 | 0.085000 | ... | 0.013640 | 0.000650 | 8.441000 | 0.000000 | 0.256570 | 0.574282 | -7.964984 | 0.006274 | 1.423287 | 0.044539 | . 25% 117.572000 | 134.862500 | 84.291000 | 0.003460 | 0.000020 | 0.001660 | 0.001860 | 0.004985 | 0.016505 | 0.148500 | ... | 0.024735 | 0.005925 | 19.198000 | 1.000000 | 0.421306 | 0.674758 | -6.450096 | 0.174351 | 2.099125 | 0.137451 | . 50% 148.790000 | 175.829000 | 104.315000 | 0.004940 | 0.000030 | 0.002500 | 0.002690 | 0.007490 | 0.022970 | 0.221000 | ... | 0.038360 | 0.011660 | 22.085000 | 1.000000 | 0.495954 | 0.722254 | -5.720868 | 0.218885 | 2.361532 | 0.194052 | . 75% 182.769000 | 224.205500 | 140.018500 | 0.007365 | 0.000060 | 0.003835 | 0.003955 | 0.011505 | 0.037885 | 0.350000 | ... | 0.060795 | 0.025640 | 25.075500 | 1.000000 | 0.587562 | 0.761881 | -5.046192 | 0.279234 | 2.636456 | 0.252980 | . max 260.105000 | 592.030000 | 239.170000 | 0.033160 | 0.000260 | 0.021440 | 0.019580 | 0.064330 | 0.119080 | 1.302000 | ... | 0.169420 | 0.314820 | 33.047000 | 1.000000 | 0.685151 | 0.825288 | -2.434031 | 0.450493 | 3.671155 | 0.527367 | . 8 rows × 23 columns . df.isnull().sum() . name 0 MDVP:Fo(Hz) 0 MDVP:Fhi(Hz) 0 MDVP:Flo(Hz) 0 MDVP:Jitter(%) 0 MDVP:Jitter(Abs) 0 MDVP:RAP 0 MDVP:PPQ 0 Jitter:DDP 0 MDVP:Shimmer 0 MDVP:Shimmer(dB) 0 Shimmer:APQ3 0 Shimmer:APQ5 0 MDVP:APQ 0 Shimmer:DDA 0 NHR 0 HNR 0 status 0 RPDE 0 DFA 0 spread1 0 spread2 0 D2 0 PPE 0 dtype: int64 . Data Visualization . print(df.columns) . Index([&#39;name&#39;, &#39;MDVP:Fo(Hz)&#39;, &#39;MDVP:Fhi(Hz)&#39;, &#39;MDVP:Flo(Hz)&#39;, &#39;MDVP:Jitter(%)&#39;, &#39;MDVP:Jitter(Abs)&#39;, &#39;MDVP:RAP&#39;, &#39;MDVP:PPQ&#39;, &#39;Jitter:DDP&#39;, &#39;MDVP:Shimmer&#39;, &#39;MDVP:Shimmer(dB)&#39;, &#39;Shimmer:APQ3&#39;, &#39;Shimmer:APQ5&#39;, &#39;MDVP:APQ&#39;, &#39;Shimmer:DDA&#39;, &#39;NHR&#39;, &#39;HNR&#39;, &#39;status&#39;, &#39;RPDE&#39;, &#39;DFA&#39;, &#39;spread1&#39;, &#39;spread2&#39;, &#39;D2&#39;, &#39;PPE&#39;], dtype=&#39;object&#39;) . df.drop(columns=[&quot;name&quot;], inplace=True) . g= sns.pairplot(df, hue =&#39;status&#39;, vars=[&#39;MDVP:Fo(Hz)&#39;, &#39;Jitter:DDP&#39;, &#39;MDVP:Shimmer(dB)&#39;,&#39;NHR&#39;, &#39;RPDE&#39;, &#39;DFA&#39;, &#39;spread1&#39;]) . df.corr()[&quot;status&quot;] . MDVP:Fo(Hz) -0.383535 MDVP:Fhi(Hz) -0.166136 MDVP:Flo(Hz) -0.380200 MDVP:Jitter(%) 0.278220 MDVP:Jitter(Abs) 0.338653 MDVP:RAP 0.266668 MDVP:PPQ 0.288698 Jitter:DDP 0.266646 MDVP:Shimmer 0.367430 MDVP:Shimmer(dB) 0.350697 Shimmer:APQ3 0.347617 Shimmer:APQ5 0.351148 MDVP:APQ 0.364316 Shimmer:DDA 0.347608 NHR 0.189429 HNR -0.361515 status 1.000000 RPDE 0.308567 DFA 0.231739 spread1 0.564838 spread2 0.454842 D2 0.340232 PPE 0.531039 Name: status, dtype: float64 . from itertools import combinations # number_of_variables = 6 # for columns_names in df.columns.to_list(): # arr = [&quot;status&quot;] # arr.append(columns_names) # sns.pairplot(df, vars=arr ) # plt.show() . sns.pairplot(df, vars=[&quot;MDVP:Fo(Hz)&quot;, &quot;status&quot;]) plt.show() . df.skew() . MDVP:Fo(Hz) 0.591737 MDVP:Fhi(Hz) 2.542146 MDVP:Flo(Hz) 1.217350 MDVP:Jitter(%) 3.084946 MDVP:Jitter(Abs) 2.649071 MDVP:RAP 3.360708 MDVP:PPQ 3.073892 Jitter:DDP 3.362058 MDVP:Shimmer 1.666480 MDVP:Shimmer(dB) 1.999389 Shimmer:APQ3 1.580576 Shimmer:APQ5 1.798697 MDVP:APQ 2.618047 Shimmer:DDA 1.580618 NHR 4.220709 HNR -0.514317 status -1.187727 RPDE -0.143402 DFA -0.033214 spread1 0.432139 spread2 0.144430 D2 0.430384 PPE 0.797491 dtype: float64 . status_df = df[[&quot;status&quot;]] predictor_df = df.drop(columns=[&quot;status&quot;]) . predictor_df.head() . MDVP:Fo(Hz) MDVP:Fhi(Hz) MDVP:Flo(Hz) MDVP:Jitter(%) MDVP:Jitter(Abs) MDVP:RAP MDVP:PPQ Jitter:DDP MDVP:Shimmer MDVP:Shimmer(dB) ... MDVP:APQ Shimmer:DDA NHR HNR RPDE DFA spread1 spread2 D2 PPE . 0 119.992 | 157.302 | 74.997 | 0.00784 | 0.00007 | 0.00370 | 0.00554 | 0.01109 | 0.04374 | 0.426 | ... | 0.02971 | 0.06545 | 0.02211 | 21.033 | 0.414783 | 0.815285 | -4.813031 | 0.266482 | 2.301442 | 0.284654 | . 1 122.400 | 148.650 | 113.819 | 0.00968 | 0.00008 | 0.00465 | 0.00696 | 0.01394 | 0.06134 | 0.626 | ... | 0.04368 | 0.09403 | 0.01929 | 19.085 | 0.458359 | 0.819521 | -4.075192 | 0.335590 | 2.486855 | 0.368674 | . 2 116.682 | 131.111 | 111.555 | 0.01050 | 0.00009 | 0.00544 | 0.00781 | 0.01633 | 0.05233 | 0.482 | ... | 0.03590 | 0.08270 | 0.01309 | 20.651 | 0.429895 | 0.825288 | -4.443179 | 0.311173 | 2.342259 | 0.332634 | . 3 116.676 | 137.871 | 111.366 | 0.00997 | 0.00009 | 0.00502 | 0.00698 | 0.01505 | 0.05492 | 0.517 | ... | 0.03772 | 0.08771 | 0.01353 | 20.644 | 0.434969 | 0.819235 | -4.117501 | 0.334147 | 2.405554 | 0.368975 | . 4 116.014 | 141.781 | 110.655 | 0.01284 | 0.00011 | 0.00655 | 0.00908 | 0.01966 | 0.06425 | 0.584 | ... | 0.04465 | 0.10470 | 0.01767 | 19.649 | 0.417356 | 0.823484 | -3.747787 | 0.234513 | 2.332180 | 0.410335 | . 5 rows × 22 columns . skew_df =pd.DataFrame(predictor_df.skew().to_list(),index=predictor_df.skew().index.to_list(), columns=[&quot;skew&quot;]) #predictor_df.skew().index.to_list() #skew_df.head() skewedCols = skew_df[skew_df[&quot;skew&quot;]&gt;1] skewedCols.head(10) skewedCols = skewedCols.index.to_list() print(skewedCols) . [&#39;MDVP:Fhi(Hz)&#39;, &#39;MDVP:Flo(Hz)&#39;, &#39;MDVP:Jitter(%)&#39;, &#39;MDVP:Jitter(Abs)&#39;, &#39;MDVP:RAP&#39;, &#39;MDVP:PPQ&#39;, &#39;Jitter:DDP&#39;, &#39;MDVP:Shimmer&#39;, &#39;MDVP:Shimmer(dB)&#39;, &#39;Shimmer:APQ3&#39;, &#39;Shimmer:APQ5&#39;, &#39;MDVP:APQ&#39;, &#39;Shimmer:DDA&#39;, &#39;NHR&#39;] . skew_df = df[skewedCols] skew_df.shape . (195, 14) . skew_df.head() . MDVP:Fhi(Hz) MDVP:Flo(Hz) MDVP:Jitter(%) MDVP:Jitter(Abs) MDVP:RAP MDVP:PPQ Jitter:DDP MDVP:Shimmer MDVP:Shimmer(dB) Shimmer:APQ3 Shimmer:APQ5 MDVP:APQ Shimmer:DDA NHR . 0 157.302 | 74.997 | 0.00784 | 0.00007 | 0.00370 | 0.00554 | 0.01109 | 0.04374 | 0.426 | 0.02182 | 0.03130 | 0.02971 | 0.06545 | 0.02211 | . 1 148.650 | 113.819 | 0.00968 | 0.00008 | 0.00465 | 0.00696 | 0.01394 | 0.06134 | 0.626 | 0.03134 | 0.04518 | 0.04368 | 0.09403 | 0.01929 | . 2 131.111 | 111.555 | 0.01050 | 0.00009 | 0.00544 | 0.00781 | 0.01633 | 0.05233 | 0.482 | 0.02757 | 0.03858 | 0.03590 | 0.08270 | 0.01309 | . 3 137.871 | 111.366 | 0.00997 | 0.00009 | 0.00502 | 0.00698 | 0.01505 | 0.05492 | 0.517 | 0.02924 | 0.04005 | 0.03772 | 0.08771 | 0.01353 | . 4 141.781 | 110.655 | 0.01284 | 0.00011 | 0.00655 | 0.00908 | 0.01966 | 0.06425 | 0.584 | 0.03490 | 0.04825 | 0.04465 | 0.10470 | 0.01767 | . skew_df.describe() . MDVP:Fhi(Hz) MDVP:Flo(Hz) MDVP:Jitter(%) MDVP:Jitter(Abs) MDVP:RAP MDVP:PPQ Jitter:DDP MDVP:Shimmer MDVP:Shimmer(dB) Shimmer:APQ3 Shimmer:APQ5 MDVP:APQ Shimmer:DDA NHR . count 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | 195.000000 | . mean 197.104918 | 116.324631 | 0.006220 | 0.000044 | 0.003306 | 0.003446 | 0.009920 | 0.029709 | 0.282251 | 0.015664 | 0.017878 | 0.024081 | 0.046993 | 0.024847 | . std 91.491548 | 43.521413 | 0.004848 | 0.000035 | 0.002968 | 0.002759 | 0.008903 | 0.018857 | 0.194877 | 0.010153 | 0.012024 | 0.016947 | 0.030459 | 0.040418 | . min 102.145000 | 65.476000 | 0.001680 | 0.000007 | 0.000680 | 0.000920 | 0.002040 | 0.009540 | 0.085000 | 0.004550 | 0.005700 | 0.007190 | 0.013640 | 0.000650 | . 25% 134.862500 | 84.291000 | 0.003460 | 0.000020 | 0.001660 | 0.001860 | 0.004985 | 0.016505 | 0.148500 | 0.008245 | 0.009580 | 0.013080 | 0.024735 | 0.005925 | . 50% 175.829000 | 104.315000 | 0.004940 | 0.000030 | 0.002500 | 0.002690 | 0.007490 | 0.022970 | 0.221000 | 0.012790 | 0.013470 | 0.018260 | 0.038360 | 0.011660 | . 75% 224.205500 | 140.018500 | 0.007365 | 0.000060 | 0.003835 | 0.003955 | 0.011505 | 0.037885 | 0.350000 | 0.020265 | 0.022380 | 0.029400 | 0.060795 | 0.025640 | . max 592.030000 | 239.170000 | 0.033160 | 0.000260 | 0.021440 | 0.019580 | 0.064330 | 0.119080 | 1.302000 | 0.056470 | 0.079400 | 0.137780 | 0.169420 | 0.314820 | . fig, axs = plt.subplots(7,2 , figsize=(10, 50)) col = [0, 1] row = [x for x in range(0,7)] import itertools cartesian_product = itertools.product(row, col) axis_coords = list(cartesian_product) i=0 for column in skew_df.columns.to_list(): sns.histplot(data=skew_df, x=str(column), kde=True, color=&quot;skyblue&quot;, ax=axs[axis_coords[i][0], axis_coords[i][1]]) i+=1 . import numpy as np skew_df = np.log(skew_df) . fig, axs = plt.subplots(7,2 , figsize=(10, 50)) col = [0, 1] row = [x for x in range(0,7)] import itertools cartesian_product = itertools.product(row, col) axis_coords = list(cartesian_product) i=0 for column in skew_df.columns.to_list(): sns.histplot(data=skew_df, x=str(column), kde=True, color=&quot;skyblue&quot;, ax=axs[axis_coords[i][0], axis_coords[i][1]]) i+=1 . predictor_df[skew_df.columns.to_list()] = skew_df[skew_df.columns.to_list()] predictor_df.head() . MDVP:Fo(Hz) MDVP:Fhi(Hz) MDVP:Flo(Hz) MDVP:Jitter(%) MDVP:Jitter(Abs) MDVP:RAP MDVP:PPQ Jitter:DDP MDVP:Shimmer MDVP:Shimmer(dB) ... MDVP:APQ Shimmer:DDA NHR HNR RPDE DFA spread1 spread2 D2 PPE . 0 119.992 | 5.058168 | 4.317448 | -4.848516 | -9.567015 | -5.599422 | -5.195761 | -4.501711 | -3.129492 | -0.853316 | ... | -3.516272 | -2.726469 | -3.811725 | 21.033 | 0.414783 | 0.815285 | -4.813031 | 0.266482 | 2.301442 | 0.284654 | . 1 122.400 | 5.001595 | 4.734609 | -4.637693 | -9.433484 | -5.370888 | -4.967576 | -4.272993 | -2.791323 | -0.468405 | ... | -3.130865 | -2.364141 | -3.948168 | 19.085 | 0.458359 | 0.819521 | -4.075192 | 0.335590 | 2.486855 | 0.368674 | . 2 116.682 | 4.876044 | 4.714518 | -4.556380 | -9.315701 | -5.213976 | -4.852350 | -4.114751 | -2.950185 | -0.729811 | ... | -3.327018 | -2.492536 | -4.335907 | 20.651 | 0.429895 | 0.825288 | -4.443179 | 0.311173 | 2.342259 | 0.332634 | . 3 116.676 | 4.926318 | 4.712822 | -4.608175 | -9.315701 | -5.294325 | -4.964706 | -4.196377 | -2.901878 | -0.659712 | ... | -3.277565 | -2.433719 | -4.302846 | 20.644 | 0.434969 | 0.819235 | -4.117501 | 0.334147 | 2.405554 | 0.368975 | . 4 116.014 | 4.954284 | 4.706417 | -4.355190 | -9.115030 | -5.028290 | -4.701681 | -3.929169 | -2.744974 | -0.537854 | ... | -3.108901 | -2.256656 | -4.035887 | 19.649 | 0.417356 | 0.823484 | -3.747787 | 0.234513 | 2.332180 | 0.410335 | . 5 rows × 22 columns . normalized_df=(predictor_df-predictor_df.mean())/predictor_df.std() normalized_df[&quot;status&quot;] = status_df[&quot;status&quot;] normalized_df.head() . MDVP:Fo(Hz) MDVP:Fhi(Hz) MDVP:Flo(Hz) MDVP:Jitter(%) MDVP:Jitter(Abs) MDVP:RAP MDVP:PPQ Jitter:DDP MDVP:Shimmer MDVP:Shimmer(dB) ... Shimmer:DDA NHR HNR RPDE DFA spread1 spread2 D2 PPE status . 0 -0.827171 | -0.415271 | -1.127228 | 0.730419 | 1.001937 | 0.545302 | 1.150192 | 0.543770 | 0.988053 | 1.019244 | ... | 0.869064 | 0.496891 | -0.192729 | -0.805764 | 1.756293 | 0.799266 | 0.479243 | -0.209990 | 0.866655 | 1 | . 1 -0.768992 | -0.571305 | 0.113642 | 1.097305 | 1.189745 | 0.905477 | 1.541298 | 0.904351 | 1.594278 | 1.679079 | ... | 1.488615 | 0.372162 | -0.632879 | -0.386529 | 1.832844 | 1.476053 | 1.307819 | 0.274371 | 1.798974 | 1 | . 2 -0.907141 | -0.917587 | 0.053878 | 1.238811 | 1.355404 | 1.152774 | 1.738793 | 1.153822 | 1.309491 | 1.230963 | ... | 1.269071 | 0.017714 | -0.279042 | -0.660375 | 1.937062 | 1.138515 | 1.015070 | -0.103363 | 1.399060 | 1 | . 3 -0.907286 | -0.778925 | 0.048834 | 1.148675 | 1.355404 | 1.026142 | 1.546216 | 1.025137 | 1.396090 | 1.351129 | ... | 1.369642 | 0.047937 | -0.280624 | -0.611559 | 1.827676 | 1.437245 | 1.290518 | 0.061985 | 1.802314 | 1 | . 4 -0.923281 | -0.701794 | 0.029783 | 1.588933 | 1.637642 | 1.445419 | 1.997038 | 1.446398 | 1.677367 | 1.560025 | ... | 1.672406 | 0.291975 | -0.505444 | -0.781010 | 1.904461 | 1.776368 | 0.095948 | -0.129692 | 2.261262 | 1 | . 5 rows × 23 columns . fig, axs = plt.subplots(12,2 , figsize=(10, 50)) col = [0, 1] row = [x for x in range(0,12)] cartesian_product = itertools.product(row, col) axis_coords = list(cartesian_product) i=0 for column in normalized_df.columns.to_list(): sns.histplot(data=normalized_df, x=str(column), kde=True, color=&quot;skyblue&quot;, ax=axs[axis_coords[i][0], axis_coords[i][1]]) i+=1 . Modelado y feature engineering . Benchmarking con Naive Bayes . from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import train_test_split benchmark_df = df.copy() y = benchmark_df[&quot;status&quot;] X = benchmark_df.drop(columns=[&quot;status&quot;]) . X_train, X_test, y_train, y_test = train_test_split( X.values, y.values, test_size=0.30, random_state=42) . from sklearn.metrics import * gnb = GaussianNB() y_pred = gnb.fit(X_train, y_train).predict(X_test) print(classification_report(y_test, y_pred, digits=2)) . precision recall f1-score support 0 0.50 0.80 0.62 15 1 0.91 0.73 0.81 44 accuracy 0.75 59 macro avg 0.71 0.76 0.71 59 weighted avg 0.81 0.75 0.76 59 . cf_matrix = confusion_matrix(y_test, y_pred) categories = [&quot;Parkinsons&quot;, &quot;No Parkinsons&quot;] sns.heatmap(cf_matrix, annot=True,cmap=&#39;Blues&#39;, xticklabels=categories, yticklabels=categories) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a8fba0f6a0&gt; . Algoritmo genético para elegir parámetros Regresión logística Random Forest Gradientboosted tree . y = normalized_df[&quot;status&quot;] X = normalized_df.drop(columns=[&quot;status&quot;]) . !pip install sklearn-genetic . Requirement already satisfied: sklearn-genetic in c: python38 lib site-packages (0.4.1) Requirement already satisfied: deap&gt;=1.0.2 in c: python38 lib site-packages (from sklearn-genetic) (1.3.1) Requirement already satisfied: scikit-learn&gt;=0.20.3 in c: python38 lib site-packages (from sklearn-genetic) (0.24.2) Requirement already satisfied: numpy in c: python38 lib site-packages (from deap&gt;=1.0.2-&gt;sklearn-genetic) (1.18.2) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in c: python38 lib site-packages (from scikit-learn&gt;=0.20.3-&gt;sklearn-genetic) (2.2.0) Requirement already satisfied: scipy&gt;=0.19.1 in c: python38 lib site-packages (from scikit-learn&gt;=0.20.3-&gt;sklearn-genetic) (1.4.1) Requirement already satisfied: joblib&gt;=0.11 in c: python38 lib site-packages (from scikit-learn&gt;=0.20.3-&gt;sklearn-genetic) (1.0.1) . WARNING: You are using pip version 20.0.2; however, version 21.3.1 is available. You should consider upgrading via the &#39;c: python38 python.exe -m pip install --upgrade pip&#39; command. . from genetic_selection import * from sklearn.metrics import * from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.model_selection import * from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import GradientBoostingClassifier mcc = make_scorer(matthews_corrcoef) # estimator = RandomForestClassifier(max_depth=5, random_state=42, n_estimators=250) # #estimator = GradientBoostingClassifier(max_depth=3, random_state=27, n_estimators=100) # #estimator = LinearDiscriminantAnalysis() # report = pd.DataFrame() # nofeats = [] # chosen_feats = [] # cvscore = [] # rkf = RepeatedStratifiedKFold(n_repeats = 1, n_splits = 10) # for i in range(4,10): # selector = GeneticSelectionCV(estimator, # cv = rkf, # verbose = 0, # scoring = mcc, # max_features = i, # #n_population = 50, # n_population = 50, # crossover_proba = 0.5, # mutation_proba = 0.2, # #n_generations = 10, # n_generations = 10, # crossover_independent_proba=0.5, # mutation_independent_proba=0.05, # #tournament_size = 3, # n_gen_no_change=10, # caching=True, # n_jobs=-1) # # selector = selector.fit(D[allfeats], y) # # genfeats = D[allfeats].columns[selector.support_] # selector = selector.fit(X, y) # genfeats = X.columns[selector.support_] # #print(&quot;El Selector support: &quot;+str(selector.support_)) # genfeats = list(genfeats) # print(&quot;Chosen Feats: &quot;, genfeats) # cv_score = selector.generation_scores_[-1] # nofeats.append(len(genfeats)) # chosen_feats.append(genfeats) # cvscore.append(cv_score) # report[&quot;No of Feats&quot;] = nofeats # report[&quot;Chosen Feats&quot;] = chosen_feats # report[&quot;Scores&quot;] = cvscore # report.head(10) . &quot;&quot;&quot; 10 splits, n 100, max height 5 Chosen Feats: [&#39;PPE&#39;] Chosen Feats: [&#39;MDVP:Fhi(Hz)&#39;, &#39;spread2&#39;] Chosen Feats: [&#39;MDVP:Fhi(Hz)&#39;, &#39;Jitter:DDP&#39;, &#39;spread1&#39;] Chosen Feats: [&#39;MDVP:Fo(Hz)&#39;, &#39;Shimmer:APQ3&#39;, &#39;RPDE&#39;, &#39;PPE&#39;] Chosen Feats: [&#39;MDVP:Fo(Hz)&#39;, &#39;MDVP:Flo(Hz)&#39;, &#39;Shimmer:DDA&#39;, &#39;spread1&#39;] Chosen Feats: [&#39;MDVP:Fo(Hz)&#39;, &#39;MDVP:Fhi(Hz)&#39;, &#39;MDVP:Jitter(%)&#39;, &#39;Jitter:DDP&#39;, &#39;spread2&#39;, &#39;PPE&#39;] Chosen Feats: [&#39;MDVP:Fo(Hz)&#39;, &#39;MDVP:Fhi(Hz)&#39;, &#39;Jitter:DDP&#39;, &#39;NHR&#39;, &#39;RPDE&#39;, &#39;DFA&#39;, &#39;spread2&#39;] Chosen Feats: [&#39;MDVP:Fo(Hz)&#39;, &#39;Shimmer:APQ3&#39;, &#39;HNR&#39;, &#39;PPE&#39;] Chosen Feats: [&#39;MDVP:Fo(Hz)&#39;, &#39;MDVP:Flo(Hz)&#39;, &#39;MDVP:RAP&#39;, &#39;Shimmer:APQ3&#39;, &#39;Shimmer:DDA&#39;, &#39;RPDE&#39;, &#39;spread1&#39;] 0 1 [PPE] 0.581319 1 2 [MDVP:Fhi(Hz), spread2] 0.759835 2 3 [MDVP:Fhi(Hz), Jitter:DDP, spread1] 0.746843 3 4 [MDVP:Fo(Hz), Shimmer:APQ3, RPDE, PPE] 0.824767 4 4 [MDVP:Fo(Hz), MDVP:Flo(Hz), Shimmer:DDA, spread1] 0.863302 5 6 [MDVP:Fo(Hz), MDVP:Fhi(Hz), MDVP:Jitter(%), Ji... 0.827838 6 7 [MDVP:Fo(Hz), MDVP:Fhi(Hz), Jitter:DDP, NHR, R... 0.858792 7 4 [MDVP:Fo(Hz), Shimmer:APQ3, HNR, PPE] 0.863075 8 7 [MDVP:Fo(Hz), MDVP:Flo(Hz), MDVP:RAP, Shimmer:... 0.837370 &quot;&quot;&quot; #Luego de multiples corridas con diferentes parámetros seleccionados, la mejor fue con [MDVP:Fo(Hz), MDVP:Flo(Hz), Shimmer:DDA, spread1] . &#34; n10 splits, n 100, max height 5 n nChosen Feats: [&#39;PPE&#39;] nChosen Feats: [&#39;MDVP:Fhi(Hz)&#39;, &#39;spread2&#39;] nChosen Feats: [&#39;MDVP:Fhi(Hz)&#39;, &#39;Jitter:DDP&#39;, &#39;spread1&#39;] nChosen Feats: [&#39;MDVP:Fo(Hz)&#39;, &#39;Shimmer:APQ3&#39;, &#39;RPDE&#39;, &#39;PPE&#39;] nChosen Feats: [&#39;MDVP:Fo(Hz)&#39;, &#39;MDVP:Flo(Hz)&#39;, &#39;Shimmer:DDA&#39;, &#39;spread1&#39;] nChosen Feats: [&#39;MDVP:Fo(Hz)&#39;, &#39;MDVP:Fhi(Hz)&#39;, &#39;MDVP:Jitter(%)&#39;, &#39;Jitter:DDP&#39;, &#39;spread2&#39;, &#39;PPE&#39;] nChosen Feats: [&#39;MDVP:Fo(Hz)&#39;, &#39;MDVP:Fhi(Hz)&#39;, &#39;Jitter:DDP&#39;, &#39;NHR&#39;, &#39;RPDE&#39;, &#39;DFA&#39;, &#39;spread2&#39;] nChosen Feats: [&#39;MDVP:Fo(Hz)&#39;, &#39;Shimmer:APQ3&#39;, &#39;HNR&#39;, &#39;PPE&#39;] nChosen Feats: [&#39;MDVP:Fo(Hz)&#39;, &#39;MDVP:Flo(Hz)&#39;, &#39;MDVP:RAP&#39;, &#39;Shimmer:APQ3&#39;, &#39;Shimmer:DDA&#39;, &#39;RPDE&#39;, &#39;spread1&#39;] n n n0 t1 t[PPE] t0.581319 n1 t2 t[MDVP:Fhi(Hz), spread2] t0.759835 n2 t3 t[MDVP:Fhi(Hz), Jitter:DDP, spread1] t0.746843 n3 t4 t[MDVP:Fo(Hz), Shimmer:APQ3, RPDE, PPE] t0.824767 n4 t4 t[MDVP:Fo(Hz), MDVP:Flo(Hz), Shimmer:DDA, spread1] t0.863302 n5 t6 t[MDVP:Fo(Hz), MDVP:Fhi(Hz), MDVP:Jitter(%), Ji... t0.827838 n6 t7 t[MDVP:Fo(Hz), MDVP:Fhi(Hz), Jitter:DDP, NHR, R... t0.858792 n7 t4 t[MDVP:Fo(Hz), Shimmer:APQ3, HNR, PPE] t0.863075 n8 t7 t[MDVP:Fo(Hz), MDVP:Flo(Hz), MDVP:RAP, Shimmer:... t0.837370 n n&#34; . # X= X[[&quot;MDVP:Fo(Hz)&quot;, &quot;MDVP:Flo(Hz)&quot;, &quot;Shimmer:DDA&quot;, &quot;spread1&quot;]] # X.head() . MDVP:Fo(Hz) MDVP:Flo(Hz) Shimmer:DDA spread1 . 0 -0.827171 | -1.127228 | 0.869064 | 0.799266 | . 1 -0.768992 | 0.113642 | 1.488615 | 1.476053 | . 2 -0.907141 | 0.053878 | 1.269071 | 1.138515 | . 3 -0.907286 | 0.048834 | 1.369642 | 1.437245 | . 4 -0.923281 | 0.029783 | 1.672406 | 1.776368 | . Optimizaci&#243;n de Hiperpar&#225;metros para distintos algoritmos. . XGBoost . !pip install xgboost . Requirement already satisfied: xgboost in c: python38 lib site-packages (1.5.0) Requirement already satisfied: numpy in c: python38 lib site-packages (from xgboost) (1.18.2) Requirement already satisfied: scipy in c: python38 lib site-packages (from xgboost) (1.4.1) . WARNING: You are using pip version 20.0.2; however, version 21.3.1 is available. You should consider upgrading via the &#39;c: python38 python.exe -m pip install --upgrade pip&#39; command. . from xgboost import XGBClassifier from sklearn.model_selection import KFold params = { &#39;max_depth&#39;: [5, 8, 10], &#39;learning_rate&#39;: [0.025, 0.04, 0.05,0.75, 0.1, 0.125], &#39;n_estimators&#39; : [80, 100, 125, 350, 500] } xgb = XGBClassifier(seed = 42) classifier = GridSearchCV(estimator=xgb, param_grid=params, cv=StratifiedKFold(n_splits=5), verbose=1, n_jobs = -1) classifier.fit(X,y) print(f&quot;best params: {classifier.best_params_}&quot;) print(f&quot;best acc: {classifier.best_score_}&quot;) # 83 con todos los params . Fitting 5 folds for each of 90 candidates, totalling 450 fits [23:32:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. best params: {&#39;learning_rate&#39;: 0.75, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 80} best acc: 0.8205128205128205 . C: Python38 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . n_features . 22 . . MDVP:Fo(Hz) MDVP:Fhi(Hz) MDVP:Flo(Hz) MDVP:Jitter(%) MDVP:Jitter(Abs) MDVP:RAP MDVP:PPQ Jitter:DDP MDVP:Shimmer MDVP:Shimmer(dB) ... MDVP:APQ Shimmer:DDA NHR HNR RPDE DFA spread1 spread2 D2 PPE . 0 -0.827171 | -0.415271 | -1.127228 | 0.730419 | 1.001937 | 0.545302 | 1.150192 | 0.543770 | 0.988053 | 1.019244 | ... | 0.681709 | 0.869064 | 0.496891 | -0.192729 | -0.805764 | 1.756293 | 0.799266 | 0.479243 | -0.209990 | 0.866655 | . 1 -0.768992 | -0.571305 | 0.113642 | 1.097305 | 1.189745 | 0.905477 | 1.541298 | 0.904351 | 1.594278 | 1.679079 | ... | 1.361677 | 1.488615 | 0.372162 | -0.632879 | -0.386529 | 1.832844 | 1.476053 | 1.307819 | 0.274371 | 1.798974 | . 2 -0.907141 | -0.917587 | 0.053878 | 1.238811 | 1.355404 | 1.152774 | 1.738793 | 1.153822 | 1.309491 | 1.230963 | ... | 1.015607 | 1.269071 | 0.017714 | -0.279042 | -0.660375 | 1.937062 | 1.138515 | 1.015070 | -0.103363 | 1.399060 | . 3 -0.907286 | -0.778925 | 0.048834 | 1.148675 | 1.355404 | 1.026142 | 1.546216 | 1.025137 | 1.396090 | 1.351129 | ... | 1.102856 | 1.369642 | 0.047937 | -0.280624 | -0.611559 | 1.827676 | 1.437245 | 1.290518 | 0.061985 | 1.802314 | . 4 -0.923281 | -0.701794 | 0.029783 | 1.588933 | 1.637642 | 1.445419 | 1.997038 | 1.446398 | 1.677367 | 1.560025 | ... | 1.400427 | 1.672406 | 0.291975 | -0.505444 | -0.781010 | 1.904461 | 1.776368 | 0.095948 | -0.129692 | 2.261262 | . 5 rows × 22 columns . n_features = X.shape[1] params = { &quot;max_features&quot;:[n_features, (n_features//3), (n_features//5), (n_features*2//5),(n_features*3//5), (n_features*4//5)], &#39;n_estimators&#39; : [ 1000, 1250, 2000, 3000] } randomForest = RandomForestClassifier(random_state= 42) classifier = GridSearchCV(estimator=randomForest, param_grid=params, cv=StratifiedKFold(n_splits=5), verbose=1, n_jobs = -1) classifier.fit(X,y) print(f&quot;best params: {classifier.best_params_}&quot;) print(f&quot;best acc: {classifier.best_score_}&quot;) #81 con todos los params . Fitting 5 folds for each of 24 candidates, totalling 120 fits best params: {&#39;max_features&#39;: 7, &#39;n_estimators&#39;: 1250} best acc: 0.8 . from sklearn.decomposition import PCA def generatePCA(df_predictors, df_target, n_components): print(n_components) pca = PCA(n_components= n_components) principalComponents = pca.fit_transform(df_predictors) column_names = [f&quot;pc_{i}&quot; for i in range(n_components)] print(column_names) principalDf = pd.DataFrame(data = principalComponents , columns = column_names) df_result = pd.concat([df_target, principalDf], axis=1) return df_result . for i in range(5,15): pca = generatePCA(X,y,i) pca_y = pca[&quot;status&quot;] pca_X = pca.drop(columns=[&quot;status&quot;]) n_features = pca_X.shape[1] params = { &quot;max_features&quot;:[n_features, (n_features//3), (n_features//5), (n_features*2//5),(n_features*3//5), (n_features*4//5)], &#39;n_estimators&#39; : [ 1000, 1250, 2000, 3000] } randomForest = RandomForestClassifier(random_state= 42) classifier = GridSearchCV(estimator=randomForest, param_grid=params, cv=StratifiedKFold(n_splits=5), verbose=1, n_jobs = -1) classifier.fit(pca_X,pca_y) print(f&quot;{i} best params: {classifier.best_params_}&quot;) print(f&quot;{i} best acc: {classifier.best_score_}&quot;) . 5 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;] 5 6 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;] 6 7 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;] 7 8 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;, &#39;pc_7&#39;] 8 9 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;, &#39;pc_7&#39;, &#39;pc_8&#39;] 9 10 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;, &#39;pc_7&#39;, &#39;pc_8&#39;, &#39;pc_9&#39;] 10 11 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;, &#39;pc_7&#39;, &#39;pc_8&#39;, &#39;pc_9&#39;, &#39;pc_10&#39;] 11 12 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;, &#39;pc_7&#39;, &#39;pc_8&#39;, &#39;pc_9&#39;, &#39;pc_10&#39;, &#39;pc_11&#39;] 12 13 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;, &#39;pc_7&#39;, &#39;pc_8&#39;, &#39;pc_9&#39;, &#39;pc_10&#39;, &#39;pc_11&#39;, &#39;pc_12&#39;] 13 14 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;, &#39;pc_7&#39;, &#39;pc_8&#39;, &#39;pc_9&#39;, &#39;pc_10&#39;, &#39;pc_11&#39;, &#39;pc_12&#39;, &#39;pc_13&#39;] 14 . for i in range(5,15): pca = generatePCA(X,y,i) pca_y = pca[&quot;status&quot;] pca_X = pca.drop(columns=[&quot;status&quot;]) n_features = pca_X.shape[1] params = { &#39;max_depth&#39;: [3,4,5], &#39;learning_rate&#39;: [0.075, 0.1, 0.125], &#39;n_estimators&#39; : [ 1000, 3000], &quot;early_stopping_rounds&quot;:[50] } xgb = XGBClassifier(seed = 42) classifier = GridSearchCV(estimator=xgb, param_grid=params, cv=StratifiedKFold(n_splits=5), verbose=1, n_jobs = -1) classifier.fit(pca_X,pca_y) print(f&quot;{i} best params: {classifier.best_params_}&quot;) print(f&quot;{i} best acc: {classifier.best_score_}&quot;) . 5 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;] Fitting 5 folds for each of 18 candidates, totalling 90 fits . C: Python38 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [00:52:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: Parameters: { &#34;early_stopping_rounds&#34; } might not be used. This could be a false alarm, with some parameters getting used by language bindings but then being mistakenly passed down to XGBoost core, or some parameter actually being used but getting flagged wrongly here. Please open an issue if you find any such cases. [00:52:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. 5 best params: {&#39;early_stopping_rounds&#39;: 50, &#39;learning_rate&#39;: 0.125, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 3000} 5 best acc: 0.7794871794871795 6 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;] Fitting 5 folds for each of 18 candidates, totalling 90 fits . C: Python38 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [00:52:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: Parameters: { &#34;early_stopping_rounds&#34; } might not be used. This could be a false alarm, with some parameters getting used by language bindings but then being mistakenly passed down to XGBoost core, or some parameter actually being used but getting flagged wrongly here. Please open an issue if you find any such cases. [00:52:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. 6 best params: {&#39;early_stopping_rounds&#39;: 50, &#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 3000} 6 best acc: 0.7641025641025643 7 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;] Fitting 5 folds for each of 18 candidates, totalling 90 fits . C: Python38 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [00:53:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: Parameters: { &#34;early_stopping_rounds&#34; } might not be used. This could be a false alarm, with some parameters getting used by language bindings but then being mistakenly passed down to XGBoost core, or some parameter actually being used but getting flagged wrongly here. Please open an issue if you find any such cases. [00:53:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. 7 best params: {&#39;early_stopping_rounds&#39;: 50, &#39;learning_rate&#39;: 0.125, &#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 1000} 7 best acc: 0.7589743589743589 8 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;, &#39;pc_7&#39;] Fitting 5 folds for each of 18 candidates, totalling 90 fits . C: Python38 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [00:53:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: Parameters: { &#34;early_stopping_rounds&#34; } might not be used. This could be a false alarm, with some parameters getting used by language bindings but then being mistakenly passed down to XGBoost core, or some parameter actually being used but getting flagged wrongly here. Please open an issue if you find any such cases. [00:53:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. 8 best params: {&#39;early_stopping_rounds&#39;: 50, &#39;learning_rate&#39;: 0.125, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 3000} 8 best acc: 0.7794871794871796 9 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;, &#39;pc_7&#39;, &#39;pc_8&#39;] Fitting 5 folds for each of 18 candidates, totalling 90 fits . C: Python38 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [00:54:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: Parameters: { &#34;early_stopping_rounds&#34; } might not be used. This could be a false alarm, with some parameters getting used by language bindings but then being mistakenly passed down to XGBoost core, or some parameter actually being used but getting flagged wrongly here. Please open an issue if you find any such cases. [00:54:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. 9 best params: {&#39;early_stopping_rounds&#39;: 50, &#39;learning_rate&#39;: 0.125, &#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 3000} 9 best acc: 0.7692307692307694 10 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;, &#39;pc_7&#39;, &#39;pc_8&#39;, &#39;pc_9&#39;] Fitting 5 folds for each of 18 candidates, totalling 90 fits . C: Python38 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [00:54:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: Parameters: { &#34;early_stopping_rounds&#34; } might not be used. This could be a false alarm, with some parameters getting used by language bindings but then being mistakenly passed down to XGBoost core, or some parameter actually being used but getting flagged wrongly here. Please open an issue if you find any such cases. [00:54:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. 10 best params: {&#39;early_stopping_rounds&#39;: 50, &#39;learning_rate&#39;: 0.125, &#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 3000} 10 best acc: 0.7794871794871796 11 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;, &#39;pc_7&#39;, &#39;pc_8&#39;, &#39;pc_9&#39;, &#39;pc_10&#39;] Fitting 5 folds for each of 18 candidates, totalling 90 fits . C: Python38 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [00:55:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: Parameters: { &#34;early_stopping_rounds&#34; } might not be used. This could be a false alarm, with some parameters getting used by language bindings but then being mistakenly passed down to XGBoost core, or some parameter actually being used but getting flagged wrongly here. Please open an issue if you find any such cases. [00:55:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. 11 best params: {&#39;early_stopping_rounds&#39;: 50, &#39;learning_rate&#39;: 0.075, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 3000} 11 best acc: 0.7692307692307694 12 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;, &#39;pc_7&#39;, &#39;pc_8&#39;, &#39;pc_9&#39;, &#39;pc_10&#39;, &#39;pc_11&#39;] Fitting 5 folds for each of 18 candidates, totalling 90 fits . C: Python38 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [00:55:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: Parameters: { &#34;early_stopping_rounds&#34; } might not be used. This could be a false alarm, with some parameters getting used by language bindings but then being mistakenly passed down to XGBoost core, or some parameter actually being used but getting flagged wrongly here. Please open an issue if you find any such cases. [00:55:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. 12 best params: {&#39;early_stopping_rounds&#39;: 50, &#39;learning_rate&#39;: 0.075, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 1000} 12 best acc: 0.7589743589743592 13 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;, &#39;pc_7&#39;, &#39;pc_8&#39;, &#39;pc_9&#39;, &#39;pc_10&#39;, &#39;pc_11&#39;, &#39;pc_12&#39;] Fitting 5 folds for each of 18 candidates, totalling 90 fits . C: Python38 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [00:56:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: Parameters: { &#34;early_stopping_rounds&#34; } might not be used. This could be a false alarm, with some parameters getting used by language bindings but then being mistakenly passed down to XGBoost core, or some parameter actually being used but getting flagged wrongly here. Please open an issue if you find any such cases. [00:56:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. 13 best params: {&#39;early_stopping_rounds&#39;: 50, &#39;learning_rate&#39;: 0.075, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 1000} 13 best acc: 0.7846153846153847 14 [&#39;pc_0&#39;, &#39;pc_1&#39;, &#39;pc_2&#39;, &#39;pc_3&#39;, &#39;pc_4&#39;, &#39;pc_5&#39;, &#39;pc_6&#39;, &#39;pc_7&#39;, &#39;pc_8&#39;, &#39;pc_9&#39;, &#39;pc_10&#39;, &#39;pc_11&#39;, &#39;pc_12&#39;, &#39;pc_13&#39;] Fitting 5 folds for each of 18 candidates, totalling 90 fits . C: Python38 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [00:56:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: Parameters: { &#34;early_stopping_rounds&#34; } might not be used. This could be a false alarm, with some parameters getting used by language bindings but then being mistakenly passed down to XGBoost core, or some parameter actually being used but getting flagged wrongly here. Please open an issue if you find any such cases. [00:56:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. 14 best params: {&#39;early_stopping_rounds&#39;: 50, &#39;learning_rate&#39;: 0.075, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 1000} 14 best acc: 0.7794871794871796 . restringir la altura de los árboles? No, pero van a ser bajos .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/11/12/Caso-de-Estudio-2.html",
            "relUrl": "/2021/11/12/Caso-de-Estudio-2.html",
            "date": " • Nov 12, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Diferentes Estrategias Para Feature Selection   Dataset Radar",
            "content": "Intervalos . 1) 9-13 . 2) 19-25 . 3) 33-37 . 4) 42-50 . Ejercicio 1: Benchmarking . Se obtienen los mismos resultados con o sin la corrección de Laplace. . ## . ## . Ejercicio 2: Forward selection . . Atributos seleccionados: 12, 15, 17, 18 . El algoritmo encuentra un máximo local (atributo 12) en 4 iteraciones. . Ejercicio 3: Backward Elimination . . Atributos descartados: 3, 14, 20, 36, 47, 48, 52, 59 . Ejercicio 4 . . . Análisis personal de los resultados . La mejor exactitud fue obtenida por el modelo en el que se utilizó el enfoque genético para la selección de atributos. Igualmente, considero que se eligieron demasiados atributos, muchos de los cuales no hacen más que incluir ruido en el modelo. . Debido a esto, decido indagar más en los parámetros del bloque optimize selection y encuentro una opción con la que se puede limitar la cantidad de atributos seleccionados. Escojo el valor 10. Los resultados son los siguientes. . 10 atributos: . . Se obtuvieron mejores resultados y los atributos seleccionados tienen más sentido cuando se los compara con los intervalos en los que las 2 gráficas (mine y rock) están más separadas. .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/10/10/Diferentes-estrategias-para-Feature-Selection-Dataset-Radar.html",
            "relUrl": "/2021/10/10/Diferentes-estrategias-para-Feature-Selection-Dataset-Radar.html",
            "date": " • Oct 10, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Title",
            "content": "Se importa el dataset y se imprimen las 10 primeras filas . import os, types import pandas as pd from botocore.client import Config import ibm_boto3 def __iter__(self): return 0 # @hidden_cell # The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials. # You might want to remove those credentials before you share the notebook. if os.environ.get(&#39;RUNTIME_ENV_LOCATION_TYPE&#39;) == &#39;external&#39;: endpoint_1d798346e565465c95da182cbfe9fccb = &#39;https://s3.us.cloud-object-storage.appdomain.cloud&#39; else: endpoint_1d798346e565465c95da182cbfe9fccb = &#39;https://s3.private.us.cloud-object-storage.appdomain.cloud&#39; client_1d798346e565465c95da182cbfe9fccb = ibm_boto3.client(service_name=&#39;s3&#39;, ibm_api_key_id=&#39;MiApiKey&#39;, ibm_auth_endpoint=&quot;https://iam.cloud.ibm.com/oidc/token&quot;, config=Config(signature_version=&#39;oauth&#39;), endpoint_url=endpoint_1d798346e565465c95da182cbfe9fccb) body = client_1d798346e565465c95da182cbfe9fccb.get_object(Bucket=&#39;facultadia-donotdelete-pr-qcdq5gqyjj8z2s&#39;,Key=&#39;wine.csv&#39;)[&#39;Body&#39;] # add missing __iter__ method, so pandas accepts body as file-like object if not hasattr(body, &quot;__iter__&quot;): body.__iter__ = types.MethodType( __iter__, body ) df = pd.read_csv(body) df.head(10) . Class Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline . 0 1 | 14.23 | 1.71 | 2.43 | 15.6 | 127 | 2.80 | 3.06 | 0.28 | 2.29 | 5.64 | 1.04 | 3.92 | 1065 | . 1 1 | 13.20 | 1.78 | 2.14 | 11.2 | 100 | 2.65 | 2.76 | 0.26 | 1.28 | 4.38 | 1.05 | 3.40 | 1050 | . 2 1 | 13.16 | 2.36 | 2.67 | 18.6 | 101 | 2.80 | 3.24 | 0.30 | 2.81 | 5.68 | 1.03 | 3.17 | 1185 | . 3 1 | 14.37 | 1.95 | 2.50 | 16.8 | 113 | 3.85 | 3.49 | 0.24 | 2.18 | 7.80 | 0.86 | 3.45 | 1480 | . 4 1 | 13.24 | 2.59 | 2.87 | 21.0 | 118 | 2.80 | 2.69 | 0.39 | 1.82 | 4.32 | 1.04 | 2.93 | 735 | . 5 1 | 14.20 | 1.76 | 2.45 | 15.2 | 112 | 3.27 | 3.39 | 0.34 | 1.97 | 6.75 | 1.05 | 2.85 | 1450 | . 6 1 | 14.39 | 1.87 | 2.45 | 14.6 | 96 | 2.50 | 2.52 | 0.30 | 1.98 | 5.25 | 1.02 | 3.58 | 1290 | . 7 1 | 14.06 | 2.15 | 2.61 | 17.6 | 121 | 2.60 | 2.51 | 0.31 | 1.25 | 5.05 | 1.06 | 3.58 | 1295 | . 8 1 | 14.83 | 1.64 | 2.17 | 14.0 | 97 | 2.80 | 2.98 | 0.29 | 1.98 | 5.20 | 1.08 | 2.85 | 1045 | . 9 1 | 13.86 | 1.35 | 2.27 | 16.0 | 98 | 2.98 | 3.15 | 0.22 | 1.85 | 7.22 | 1.01 | 3.55 | 1045 | . #3, 4 5 y 6 con una única línea de código # Mediante el describe, podemos ver que todos los atributos son numéricos. De lo contrario, podríamos utilizar el código comentado anteriormente para convertir el tipo a numérico. df.describe(include=&#39;all&#39;) . Class Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline . count 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | 178.000000 | . mean 1.938202 | 13.000618 | 2.336348 | 2.366517 | 19.494944 | 99.741573 | 2.295112 | 2.029270 | 0.361854 | 1.590899 | 5.058090 | 0.957449 | 2.611685 | 746.893258 | . std 0.775035 | 0.811827 | 1.117146 | 0.274344 | 3.339564 | 14.282484 | 0.625851 | 0.998859 | 0.124453 | 0.572359 | 2.318286 | 0.228572 | 0.709990 | 314.907474 | . min 1.000000 | 11.030000 | 0.740000 | 1.360000 | 10.600000 | 70.000000 | 0.980000 | 0.340000 | 0.130000 | 0.410000 | 1.280000 | 0.480000 | 1.270000 | 278.000000 | . 25% 1.000000 | 12.362500 | 1.602500 | 2.210000 | 17.200000 | 88.000000 | 1.742500 | 1.205000 | 0.270000 | 1.250000 | 3.220000 | 0.782500 | 1.937500 | 500.500000 | . 50% 2.000000 | 13.050000 | 1.865000 | 2.360000 | 19.500000 | 98.000000 | 2.355000 | 2.135000 | 0.340000 | 1.555000 | 4.690000 | 0.965000 | 2.780000 | 673.500000 | . 75% 3.000000 | 13.677500 | 3.082500 | 2.557500 | 21.500000 | 107.000000 | 2.800000 | 2.875000 | 0.437500 | 1.950000 | 6.200000 | 1.120000 | 3.170000 | 985.000000 | . max 3.000000 | 14.830000 | 5.800000 | 3.230000 | 30.000000 | 162.000000 | 3.880000 | 5.080000 | 0.660000 | 3.580000 | 13.000000 | 1.710000 | 4.000000 | 1680.000000 | . Normalizar MinMax y estandarizar . df_class = df[&quot;Class&quot;] df = df.drop(columns=[&quot;Class&quot;]) df_min_max = df.copy() df_normalized = df.copy() df_normalized.head() . Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline . 0 14.23 | 1.71 | 2.43 | 15.6 | 127 | 2.80 | 3.06 | 0.28 | 2.29 | 5.64 | 1.04 | 3.92 | 1065 | . 1 13.20 | 1.78 | 2.14 | 11.2 | 100 | 2.65 | 2.76 | 0.26 | 1.28 | 4.38 | 1.05 | 3.40 | 1050 | . 2 13.16 | 2.36 | 2.67 | 18.6 | 101 | 2.80 | 3.24 | 0.30 | 2.81 | 5.68 | 1.03 | 3.17 | 1185 | . 3 14.37 | 1.95 | 2.50 | 16.8 | 113 | 3.85 | 3.49 | 0.24 | 2.18 | 7.80 | 0.86 | 3.45 | 1480 | . 4 13.24 | 2.59 | 2.87 | 21.0 | 118 | 2.80 | 2.69 | 0.39 | 1.82 | 4.32 | 1.04 | 2.93 | 735 | . df_normalized =(df_normalized-df_normalized.mean())/df_normalized.std() df_normalized[&quot;Class&quot;] = df_class df_normalized.head() . Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline Class . 0 1.514341 | -0.560668 | 0.231400 | -1.166303 | 1.908522 | 0.806722 | 1.031908 | -0.657708 | 1.221438 | 0.251009 | 0.361158 | 1.842721 | 1.010159 | 1 | . 1 0.245597 | -0.498009 | -0.825667 | -2.483841 | 0.018094 | 0.567048 | 0.731565 | -0.818411 | -0.543189 | -0.292496 | 0.404908 | 1.110317 | 0.962526 | 1 | . 2 0.196325 | 0.021172 | 1.106214 | -0.267982 | 0.088110 | 0.806722 | 1.212114 | -0.497005 | 2.129959 | 0.268263 | 0.317409 | 0.786369 | 1.391224 | 1 | . 3 1.686791 | -0.345835 | 0.486554 | -0.806975 | 0.928300 | 2.484437 | 1.462399 | -0.979113 | 1.029251 | 1.182732 | -0.426341 | 1.180741 | 2.328007 | 1 | . 4 0.294868 | 0.227053 | 1.835226 | 0.450674 | 1.278379 | 0.806722 | 0.661485 | 0.226158 | 0.400275 | -0.318377 | 0.361158 | 0.448336 | -0.037767 | 1 | . df_min_max =(df_min_max-df_min_max.min())/(df_min_max.max()-df_min_max.min()) df_min_max[&quot;Class&quot;] = df_class df_min_max.head() . Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline Class . 0 0.842105 | 0.191700 | 0.572193 | 0.257732 | 0.619565 | 0.627586 | 0.573840 | 0.283019 | 0.593060 | 0.372014 | 0.455285 | 0.970696 | 0.561341 | 1 | . 1 0.571053 | 0.205534 | 0.417112 | 0.030928 | 0.326087 | 0.575862 | 0.510549 | 0.245283 | 0.274448 | 0.264505 | 0.463415 | 0.780220 | 0.550642 | 1 | . 2 0.560526 | 0.320158 | 0.700535 | 0.412371 | 0.336957 | 0.627586 | 0.611814 | 0.320755 | 0.757098 | 0.375427 | 0.447154 | 0.695971 | 0.646933 | 1 | . 3 0.878947 | 0.239130 | 0.609626 | 0.319588 | 0.467391 | 0.989655 | 0.664557 | 0.207547 | 0.558360 | 0.556314 | 0.308943 | 0.798535 | 0.857347 | 1 | . 4 0.581579 | 0.365613 | 0.807487 | 0.536082 | 0.521739 | 0.627586 | 0.495781 | 0.490566 | 0.444795 | 0.259386 | 0.455285 | 0.608059 | 0.325963 | 1 | . from sklearn.model_selection import train_test_split # 9 Creación de train y test df_normalized_y = df_normalized[&quot;Class&quot;] df_normalized_x = df_normalized.drop(columns=[&quot;Class&quot;]) #Realizo la division con el dataset normalizado con transformación z, pero se podría hacer análogamente para el minmax. Se aplica un shuffle al dataset antes de realizar el split #con la seed 42 x_train, x_test, y_train, y_test = train_test_split( df_normalized_x, df_normalized_y, test_size=0.10, random_state=42 ) . train = x_train.join(y_train) train.head() . Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline Class . 9 1.058578 | -0.882918 | -0.351810 | -1.046527 | -0.121938 | 1.094330 | 1.122011 | -1.139816 | 0.452690 | 0.932547 | 0.229909 | 1.321588 | 0.946649 | 1 | . 114 -1.134008 | -0.847112 | 0.486554 | 0.899835 | -1.102159 | 0.423244 | 0.261028 | 0.547563 | -0.962506 | -0.930899 | -0.120091 | 0.814539 | -1.149205 | 2 | . 18 1.465069 | -0.668085 | 0.413653 | -0.896807 | 0.578221 | 1.605634 | 1.902902 | -0.336302 | 0.470162 | 1.570950 | 1.192408 | 0.293405 | 2.963114 | 1 | . 66 0.134736 | -1.187265 | -2.429493 | -1.345967 | -1.522254 | 1.094330 | 1.152045 | -0.818411 | 1.203967 | 0.104349 | 0.711158 | 0.800454 | -0.777667 | 2 | . 60 -0.826061 | -1.106702 | -0.315359 | -1.046527 | 0.088110 | -0.391646 | -0.940343 | 2.154591 | -2.063214 | -0.771298 | 1.279908 | -1.326335 | -0.212422 | 2 | . test = x_test.join(y_test) test.head() . Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline Class . 19 0.787585 | 0.683574 | 0.705257 | -1.286079 | 1.138347 | 0.646939 | 1.001874 | -1.541573 | 0.120730 | 0.018078 | 0.011159 | 1.053978 | 0.311541 | 1 | . 45 1.489705 | 1.525003 | 0.267850 | -0.178150 | 0.788268 | 0.886613 | 0.621440 | -0.497005 | -0.595603 | 0.078468 | -0.382591 | 1.011724 | 1.057792 | 1 | . 140 -0.086987 | 0.423984 | 1.215566 | 0.450674 | -0.261969 | -1.206537 | -1.531017 | 1.351077 | -1.469181 | -0.197599 | -0.820091 | -0.424915 | -0.466465 | 3 | . 30 0.898446 | -0.748647 | 1.215566 | 0.899835 | 0.088110 | 1.126287 | 1.222125 | -0.577356 | 1.378682 | 0.276890 | 1.017408 | 0.138473 | 1.708777 | 1 | . 67 -0.776789 | -1.044043 | -1.627580 | 0.031458 | -1.522254 | -0.295777 | -0.029303 | -0.738059 | -0.962506 | -0.163090 | 0.711158 | 1.222995 | -0.752263 | 2 | .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/10/10/DataPrepWine.html",
            "relUrl": "/2021/10/10/DataPrepWine.html",
            "date": " • Oct 10, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Naive Bayes   Dataset Golf",
            "content": "Naive bayes . Comparación entre utilización de atributos numéricos sin aplicar transformaciones y su conversión a atributos categóricas. . Se utiliza el dataset golf provisto por Rapidminer para este ejercicio. . Parte 1 . Conversión de atributos numéricos a categóricos (multinominales). Esto se aplica tanto para el dataset de entrenamiento como para el dataset de test. . Se genera el modelo con Naive Bayes y los resultados fueron los siguientes: . . La exactitud obtenida es de 10/14 . Parte 2 . Los atributos numéricos se tratan como numéricos. Se genera el modelo con Naive bayes y los resultados fueron los siguientes: . . La exactitud obtenida es de 9/14 . Comparación de resultados . En este caso podríamos concluir que el modelo generado con las variables categóricas tiene mejores resultados que el de variables numéricas.  . Las distribuciones no se asemejan a una distribución gaussiana. De aquí pueden derivar los peores resultados al tratar a los atributos como numéricos. . .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/18/Naive-Bayes-Dataset-Golf.html",
            "relUrl": "/2021/09/18/Naive-Bayes-Dataset-Golf.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Knn   Dataset Iris",
            "content": "KNN Iris . Se crea un nuevo proceso con el dataset Iris de UCI. Luego, se grafican petal length y petal width en un gráfico bidimensional, diferenciando las diferentes especies por color. . . Claramente la especie iris-setosa es muy distinta a las otras 2 y por lo tanto un modelo construido con knn debería reconocer perfectamente a todos los ejemplos de dicha clase. Para el caso de iris versicolor e iris virginica, la separación es más difusa, y es muy probable que nuestro modelo de knn cometa errores al clasificar algunos ejemplos de ellas. . En cuanto a las tareas de acondicionamiento necesarias para este dataset y este algoritmo, se deberán normalizar todos los atributos dado que KNN trabaja con distancias. Una diferencia en escala entre los atributos podría perjudicar substancialmente la performance del modelo. . Operador Knn de rapidminer . K: Refiere a la cantidad de vecinos más cercanos que utilizará el algoritmo para hacer una predicción sobre un ejemplo no visto. . Weighted vote: La aplicación de un voto ponderado refiere a que; dentro de los k vecinos escogidos, el más cercano a un nuevo ejemplo será el que tendrá el mayor peso al aplicar la predicción y el más lejano será el que tendrá el menor peso. . Measure types: El tipo de medida que se utilizará para encontrar los vecinos más cercanos. Aquí se podrá mencionar si se tendrán atributos categóricos, numéricos ambos, etc. . Función de medida: Se podrá seleccionar un tipo de distancia tal que sea acorde con el tipo de medida que se seleccionó anteriormente. Por ejemplo para datos numéricos, se puede utilizar la distancia euclídea, mientras que para datos nominales se puede utilizar la Jaccard similarity. . Entrenamiento de modelo y resultados . K=5 . K=3 . K=1 . K=1 Distancia= Chebychev Distance . Análisis de resultados . Contrario a lo que hubiera pensado, los mejores resultados fueron obtenidos con k=1 para este Split de este dataset. Las distancias de manhattan y euclídea presentaron los mismos resultados para k = 1, mientras que la de chebychev presentó una exactitud levemente menor. . Mi hipótesis inicial para la clase iris-setosa resultó ser cierta para la mayoría de los modelos creados (menos para el modelo en el que se utiliza la distancia de chebychev). Esta clase obtuvo una precisión y un recall de 100%. .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/16/KNN-Dataset-Iris.html",
            "relUrl": "/2021/09/16/KNN-Dataset-Iris.html",
            "date": " • Sep 16, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Árboles De Decisión   Dataset Iris Rapidminer",
            "content": "Utilización de árboles de decisión para clasificar los ejemplos del dataset Iris utilizando Rapidminer. . . . .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/15/%C3%81rboles-de-decisi%C3%B3n-Dataset-Iris-Rapidminer.html",
            "relUrl": "/2021/09/15/%C3%81rboles-de-decisi%C3%B3n-Dataset-Iris-Rapidminer.html",
            "date": " • Sep 15, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Title",
            "content": "import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline . df = pd.read_csv(&quot;Iris.csv&quot;) df.head() . Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species . 0 1 | 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa | . 1 2 | 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa | . 2 3 | 4.7 | 3.2 | 1.3 | 0.2 | Iris-setosa | . 3 4 | 4.6 | 3.1 | 1.5 | 0.2 | Iris-setosa | . 4 5 | 5.0 | 3.6 | 1.4 | 0.2 | Iris-setosa | . df.drop(columns=[&quot;Id&quot;], inplace=True) df.describe(include=&quot;all&quot;) . SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species . count 150.000000 | 150.000000 | 150.000000 | 150.000000 | 150 | . unique NaN | NaN | NaN | NaN | 3 | . top NaN | NaN | NaN | NaN | Iris-setosa | . freq NaN | NaN | NaN | NaN | 50 | . mean 5.843333 | 3.054000 | 3.758667 | 1.198667 | NaN | . std 0.828066 | 0.433594 | 1.764420 | 0.763161 | NaN | . min 4.300000 | 2.000000 | 1.000000 | 0.100000 | NaN | . 25% 5.100000 | 2.800000 | 1.600000 | 0.300000 | NaN | . 50% 5.800000 | 3.000000 | 4.350000 | 1.300000 | NaN | . 75% 6.400000 | 3.300000 | 5.100000 | 1.800000 | NaN | . max 7.900000 | 4.400000 | 6.900000 | 2.500000 | NaN | . Visualizaci&#243;n de Datos . ax = plt.axes() ax.scatter(df[&quot;SepalLengthCm&quot;], df[&quot;SepalWidthCm&quot;]) ax.set(xlabel=&#39;Sepal Length&#39;, ylabel=&#39;Sepal Width&#39;, title=&#39;Sepal Length vs Sepal Width&#39;) . [Text(0, 0.5, &#39;Sepal Width&#39;), Text(0.5, 0, &#39;Sepal Length&#39;), Text(0.5, 1.0, &#39;Sepal Length vs Sepal Width&#39;)] . sns.set_context(&#39;notebook&#39;) ax = df.plot.hist(bins=25, alpha=0.3) ax.set_xlabel(&#39;Size (cm)&#39;) . Text(0.5, 0, &#39;Size (cm)&#39;) . sns.set_context(&#39;talk&#39;) sns.pairplot(df, hue=&#39;Species&#39;) . &lt;seaborn.axisgrid.PairGrid at 0x1cc600ec9a0&gt; . from sklearn import tree from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder clf = tree.DecisionTreeClassifier() y = df[&quot;Species&quot;] le = LabelEncoder() y_encoded = le.fit_transform(y) X = df.drop(columns=[&quot;Species&quot;]).values . train_X, test_X, train_y, test_y = train_test_split(X, y_encoded, test_size=0.3, random_state=42, shuffle=True) . clf = clf.fit(train_X, train_y) . plt.figure(dpi=50, figsize=(50,50)) tree.plot_tree(clf) . [Text(968.75, 1730.2083333333333, &#39;X[3] &lt;= 0.8 ngini = 0.664 nsamples = 105 nvalue = [31, 37, 37]&#39;), Text(819.7115384615386, 1415.625, &#39;gini = 0.0 nsamples = 31 nvalue = [31, 0, 0]&#39;), Text(1117.7884615384617, 1415.625, &#39;X[3] &lt;= 1.75 ngini = 0.5 nsamples = 74 nvalue = [0, 37, 37]&#39;), Text(596.1538461538462, 1101.0416666666667, &#39;X[2] &lt;= 4.95 ngini = 0.214 nsamples = 41 nvalue = [0, 36, 5]&#39;), Text(298.0769230769231, 786.4583333333335, &#39;X[3] &lt;= 1.6 ngini = 0.056 nsamples = 35 nvalue = [0, 34, 1]&#39;), Text(149.03846153846155, 471.875, &#39;gini = 0.0 nsamples = 34 nvalue = [0, 34, 0]&#39;), Text(447.11538461538464, 471.875, &#39;gini = 0.0 nsamples = 1 nvalue = [0, 0, 1]&#39;), Text(894.2307692307693, 786.4583333333335, &#39;X[3] &lt;= 1.55 ngini = 0.444 nsamples = 6 nvalue = [0, 2, 4]&#39;), Text(745.1923076923077, 471.875, &#39;gini = 0.0 nsamples = 3 nvalue = [0, 0, 3]&#39;), Text(1043.269230769231, 471.875, &#39;X[2] &lt;= 5.45 ngini = 0.444 nsamples = 3 nvalue = [0, 2, 1]&#39;), Text(894.2307692307693, 157.29166666666674, &#39;gini = 0.0 nsamples = 2 nvalue = [0, 2, 0]&#39;), Text(1192.3076923076924, 157.29166666666674, &#39;gini = 0.0 nsamples = 1 nvalue = [0, 0, 1]&#39;), Text(1639.4230769230771, 1101.0416666666667, &#39;X[2] &lt;= 4.85 ngini = 0.059 nsamples = 33 nvalue = [0, 1, 32]&#39;), Text(1490.3846153846155, 786.4583333333335, &#39;X[1] &lt;= 3.1 ngini = 0.444 nsamples = 3 nvalue = [0, 1, 2]&#39;), Text(1341.3461538461538, 471.875, &#39;gini = 0.0 nsamples = 2 nvalue = [0, 0, 2]&#39;), Text(1639.4230769230771, 471.875, &#39;gini = 0.0 nsamples = 1 nvalue = [0, 1, 0]&#39;), Text(1788.4615384615386, 786.4583333333335, &#39;gini = 0.0 nsamples = 30 nvalue = [0, 0, 30]&#39;)] . y_pred = clf.predict(test_X) print(&quot;Predicted vs Expected&quot;) print(y_pred) print(test_y) . Predicted vs Expected [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0 0 0 2 1 1 0 0] [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0 0 0 2 1 1 0 0] . from sklearn.metrics import confusion_matrix test_y_inv = le.inverse_transform(test_y) y_pred_inv = le.inverse_transform(y_pred) categories = set(df[&quot;Species&quot;]) cf_matrix = confusion_matrix(test_y_inv, y_pred_inv) sns.heatmap(cf_matrix, annot=True,cmap=&#39;Blues&#39;, xticklabels=categories, yticklabels=categories) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1cc6168b100&gt; .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/15/%C3%81rboles-de-decisi%C3%B3n-Dataset-Iris-Notebook.html",
            "relUrl": "/2021/09/15/%C3%81rboles-de-decisi%C3%B3n-Dataset-Iris-Notebook.html",
            "date": " • Sep 15, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Svm",
            "content": "Máquinas de soporte vectorial . Se generó un dataset de ejemplo para mostrar un caso en el que las máquinas de soporte vectorial demuestran su potencial. El dataset utilizado es SVM-Anillos.csv. . Al visualizar los datos, las clases parecerían estar separadas mediante un anillo. Por esta razón podríamos plantearnos las siguientes hipótesis: . 1) Utilizar un kernel lineal es una mala opción, pues no existe una recta que haga una buena separación de los 2 conjuntos. . 2) Probablemente un kernel radial o uno polinómico de grado par obtenga buenos resultados . . Flujo de Rapidminer . . Cross validation . . Resultados: . Kernel lineal . . Kernel radial . . Kernel Polinómico Par e Impar . . Observamos el mismo comportamiento que con el kernel radial para un kernel polinómico de grado 2. Modificando el grado del kernel a valores pares, se puede ver una accuracy de 100%, mientras que utilizando un grado impar, los resultados se acercan más al 70-75% utilizando 10-fold-CV. . Cambio de la constante C . No se observan cambios notorios al modificar la constante c para este dataset, dado que los datos están perfectamente separados. . Conclusiones . Como conclusión, se probaron ambas hipótesis planteadas mediante el entrenamiento de diferentes máquinas de soporte vectorial con la utilización de distintos kernels. .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/15/SVM.html",
            "relUrl": "/2021/09/15/SVM.html",
            "date": " • Sep 15, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Title",
            "content": "import matplotlib import matplotlib.pyplot as plt import pandas as pd from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.metrics import confusion_matrix, classification_report from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder import seaborn as sns . input_file = &quot;sports_Training.csv&quot; df = pd.read_csv(input_file, header=0) df.head() . Edad Fuerza Velocidad Lesiones Vision Resistencia Agilidad CapacidadDecision DeportePrimario . 0 15.1 | 3 | 2 | 1 | 2 | 3 | 29 | 4 | Futbol | . 1 15.4 | 3 | 2 | 0 | 3 | 5 | 18 | 8 | Rugby | . 2 13.6 | 5 | 5 | 0 | 2 | 5 | 27 | 28 | Voleibol | . 3 18.8 | 5 | 1 | 1 | 1 | 3 | 48 | 36 | Voleibol | . 4 16.1 | 3 | 1 | 0 | 3 | 3 | 38 | 29 | Futbol | . data = df[(df[&#39;CapacidadDecision&#39;] &gt;= 3) &amp; (df[&#39;CapacidadDecision&#39;] &lt;= 100)] data.head() . Edad Fuerza Velocidad Lesiones Vision Resistencia Agilidad CapacidadDecision DeportePrimario . 0 15.1 | 3 | 2 | 1 | 2 | 3 | 29 | 4 | Futbol | . 1 15.4 | 3 | 2 | 0 | 3 | 5 | 18 | 8 | Rugby | . 2 13.6 | 5 | 5 | 0 | 2 | 5 | 27 | 28 | Voleibol | . 3 18.8 | 5 | 1 | 1 | 1 | 3 | 48 | 36 | Voleibol | . 4 16.1 | 3 | 1 | 0 | 3 | 3 | 38 | 29 | Futbol | . data.describe() . Edad Fuerza Velocidad Lesiones Vision Resistencia Agilidad CapacidadDecision . count 482.000000 | 482.000000 | 482.000000 | 482.000000 | 482.000000 | 482.000000 | 482.000000 | 482.000000 | . mean 15.954564 | 3.500000 | 1.983402 | 0.639004 | 1.692946 | 3.856846 | 33.680498 | 29.157676 | . std 1.817320 | 1.460854 | 1.505269 | 0.480788 | 1.134010 | 1.331782 | 12.523973 | 19.477265 | . min 13.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 13.000000 | 3.000000 | . 25% 14.300000 | 3.000000 | 1.000000 | 0.000000 | 1.000000 | 3.000000 | 23.000000 | 11.000000 | . 50% 15.900000 | 4.000000 | 1.000000 | 1.000000 | 2.000000 | 5.000000 | 31.500000 | 29.000000 | . 75% 17.600000 | 4.000000 | 2.000000 | 1.000000 | 3.000000 | 5.000000 | 42.750000 | 40.000000 | . max 19.000000 | 7.000000 | 6.000000 | 1.000000 | 3.000000 | 6.000000 | 80.000000 | 100.000000 | . data.dtypes . Edad float64 Fuerza int64 Velocidad int64 Lesiones int64 Vision int64 Resistencia int64 Agilidad int64 CapacidadDecision int64 DeportePrimario object dtype: object . y = data[&quot;DeportePrimario&quot;] le = LabelEncoder() y_encoded = le.fit_transform(y) . X = data.drop(columns=[&#39;DeportePrimario&#39;]).values type(X) . numpy.ndarray . type(y_encoded) . numpy.ndarray . train_X, test_X, train_y, test_y = train_test_split(X, y_encoded, test_size=0.25, random_state=41, shuffle=True) . lda = LinearDiscriminantAnalysis() lda = lda.fit(train_X, train_y) . type(test_X) . numpy.ndarray . y_pred = lda.predict(test_X) print(&quot;Predicted vs Expected&quot;) print(y_pred) print(test_y) . Predicted vs Expected [3 1 2 1 1 0 0 0 1 2 2 2 1 3 3 0 2 3 2 3 3 2 1 1 2 3 0 0 1 3 3 1 1 2 2 2 2 1 1 1 1 2 3 3 1 1 3 1 1 1 1 2 1 1 1 1 2 3 3 1 2 1 0 1 2 1 1 2 1 1 3 2 2 2 2 1 0 1 2 1 1 3 1 1 1 1 1 2 1 2 1 2 0 3 1 1 1 2 2 3 2 0 2 1 1 1 1 3 1 0 1 1 3 1 2 3 1 1 3 1 3] [1 2 2 1 3 3 2 2 1 1 2 2 1 1 3 2 2 0 2 3 1 2 2 0 2 0 1 0 1 3 0 0 2 1 0 1 2 1 3 3 3 2 0 3 1 1 1 3 3 1 1 3 3 1 3 1 0 2 3 2 1 1 0 0 3 1 1 3 2 0 0 1 1 2 0 3 1 1 0 0 2 0 1 3 3 1 2 0 3 2 2 2 0 0 2 3 1 1 2 2 2 0 1 3 2 1 1 1 1 3 2 1 3 0 2 1 1 2 3 1 3] . print(classification_report(test_y, y_pred, digits=3)) . precision recall f1-score support 0 0.364 0.182 0.242 22 1 0.429 0.600 0.500 40 2 0.484 0.469 0.476 32 3 0.348 0.296 0.320 27 accuracy 0.421 121 macro avg 0.406 0.387 0.385 121 weighted avg 0.413 0.421 0.407 121 . print(confusion_matrix(test_y, y_pred)) . [[ 4 6 5 7] [ 2 24 8 6] [ 3 12 15 2] [ 2 14 3 8]] . test_y_inv = le.inverse_transform(test_y) y_pred_inv = le.inverse_transform(y_pred) categories = [&#39;Futbol&#39;, &#39;Rugby&#39;, &quot;Basketball&quot;,&#39;Voleibol&#39;] cf_matrix = confusion_matrix(test_y_inv, y_pred_inv) sns.heatmap(cf_matrix, annot=True,cmap=&#39;Blues&#39;, xticklabels=categories, yticklabels=categories) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2022c31dd60&gt; . Predicciones en set no visto: Validaci&#243;n . validation_df = pd.read_csv(&quot;sports_Scoring.csv&quot;) validation_df.head() . Edad Fuerza Velocidad Lesiones Vision Resistencia Agilidad CapacidadDecision . 0 18.5 | 5 | 1 | 1 | 0 | 5 | 33 | 61 | . 1 13.3 | 1 | 2 | 1 | 3 | 5 | 18 | 59 | . 2 13.4 | 2 | 1 | 0 | 2 | 5 | 40 | 11 | . 3 13.6 | 4 | 1 | 0 | 0 | 5 | 28 | 0 | . 4 16.3 | 3 | 1 | 0 | 2 | 5 | 32 | 35 | . validation_df = validation_df[(validation_df[&#39;CapacidadDecision&#39;] &gt;= 3) &amp; (validation_df[&#39;CapacidadDecision&#39;] &lt;= 100)] validation_df.describe() . Edad Fuerza Velocidad Lesiones Vision Resistencia Agilidad CapacidadDecision . count 1767.000000 | 1767.000000 | 1767.000000 | 1767.000000 | 1767.000000 | 1767.000000 | 1767.000000 | 1767.000000 | . mean 15.982513 | 3.567063 | 1.987550 | 0.664969 | 1.599887 | 3.736276 | 34.013016 | 29.946237 | . std 1.729906 | 1.479915 | 1.551992 | 0.472135 | 1.188402 | 1.366378 | 12.299599 | 21.116807 | . min 13.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 13.000000 | 3.000000 | . 25% 14.500000 | 3.000000 | 1.000000 | 0.000000 | 0.000000 | 3.000000 | 24.000000 | 11.000000 | . 50% 16.000000 | 4.000000 | 1.000000 | 1.000000 | 2.000000 | 3.000000 | 32.000000 | 29.000000 | . 75% 17.400000 | 4.000000 | 2.000000 | 1.000000 | 3.000000 | 5.000000 | 43.000000 | 44.000000 | . max 19.000000 | 7.000000 | 6.000000 | 1.000000 | 3.000000 | 6.000000 | 80.000000 | 100.000000 | . validation_df.columns . Index([&#39;Edad&#39;, &#39;Fuerza&#39;, &#39;Velocidad&#39;, &#39;Lesiones&#39;, &#39;Vision&#39;, &#39;Resistencia&#39;, &#39;Agilidad&#39;, &#39;CapacidadDecision&#39;], dtype=&#39;object&#39;) . y_pred = lda.predict(validation_df.to_numpy()) . print(y_pred) . [0 2 1 ... 1 0 1] .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/12/Algoritmos-Lineales-para-clasificaci%C3%B3n-Sports.html",
            "relUrl": "/2021/09/12/Algoritmos-Lineales-para-clasificaci%C3%B3n-Sports.html",
            "date": " • Sep 12, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Preparación De Datos Y Modelado Con El Dataset Titanic",
            "content": "Preparación de datos y modelado para el dataset Titanic . 1) Importar librerías y paquetes . Además de hablar de las librerías y paquetes, me pareció interesante destacar ciertos conceptos que no fueron presentados en la clase y quizás alguna posible mejora para alguna parte del código en el caso de manejar datasets más pesados y más cómputo-dependientes. . Inicialmente, se importan las librerías necesarias como numpy y pandas para cálculos matemáticos y estructuras de datos además de matplotlib y seaborn para la parte de visualización de datos y gráficas. . Luego, se importan varias funcionalidades/modelos de la librería scikit learn. Los modelos importados y una breve explicación de ellos es la siguiente. . SVC, Linear SVC son algoritmos del tipo Support vector machines, los cuales buscan definir una recta, plano o hiperplano (hablando de un SVM lineal) tal que la distancia entre esta recta y el punto más cercano de cada clase sea máxima. . Dentro de los algoritmos de árboles o ensembles, se utilizan un decisionTreeClassifier, un randomForestClassifier y XGBoost. El decisionTreeClassifier construye un único árbol y lo utiliza para hacer las predicciones. El random forest construye múltiples árboles armados utilizando bagging y luego al realizar una predicción se le da un peso al resultado de cada árbol del bosque y se decide qué predice el bosque (juntando las predicciones de cada uno y sus pesos). Por último, XGBoost utiliza una forma diferente de armar los árboles dado que son dependientes uno de otro, intentando mejorar las métricas con cada árbol nuevo. . También se utiliza la técnica de Gridsearch para tunear los hiperparámetros de los distintos algoritmos. Una mejor alternativa para un caso computacionalmente complejo sería utilizar randomsearch para ubicar los puntos de interés donde es muy probable encontrar óptimos en la cercanía y aplicar gridsearch en espacios pequeños cercanos a dichos puntos de interés. . 2) &gt; Cargar el dataset y mostrarlo . Para realizar estas dos tareas, se utilizan las funcionalidades de pandas llamadas read_csv para leer un archivo csv y cargarlo utilizando la estructura de datos llamada dataframe y head como forma de imprimir las primeras x filas (5 por defecto) del dataset de una forma prolija. . 3) &gt; Gestionar los valores faltantes . Se utiliza la función pd.isnull para identificar los valores faltantes de cada columna. Se dropean las columnas Cabin y ticket por la gran cantidad de valores faltantes. . Se observa una distribución relativamente parecida a una normal para el atributo age, pero con un leve sesgo hacia la derecha, lo que determinará la estrategia de imputación utilizada. Debido a este sesgo, elegir la mediana es una mejor alternativa que elegir la media. Si no tuviéramos este sesgo, la estrategia de imputación elegida sería la media. . Para el atributo fare, la distribución está fuertemente sesgada hacia la derecha, por lo que se elige la mediana como valor a imputar. Alternativamente, se podría elegir aplicar una transformación a los datos para que su distribución se asemeje más a una normal. 2 estrategias para esto podrían ser aplicar el logaritmo natural a los valores del atributo o alguna de las variantes de la transformación “Box and Cox” (1964) presentada en applied-predictivemodeling, presente en la bibliografía del curso. . 4) &gt; Graficar los datos . Se grafican varios atributos en relación al atributo a predecir para formar unas hipótesis iniciales sobre si algún atributo está correlacionado con la variable a predecir. Las librerías para . hacer esto son seaborn y matplotlib.pyplot. Aquí se pueden observar ratios como la proporción de mujeres que sobrevivieron o indicios como qué clase es la que presentó más proporción de personas que sobrevivieron. . 5) &gt; Feature Engineering . En la parte de feature engineering, lo primero que se hace es encodear los atributos categóricos a valores enteros. Se podría realizar mediante one hot encoding pero en este notebok, se decide no hacerlo. . Una hipótesis muy interesante provista por Samson Qian (creador del notebook en kaggle) es que el titulo de la persona puede influir en si esta sobrevivió o no. Desde mi punto de vista personal, quizás esto podría estar agregando un atributo fuertemente correlacionado con Sex. Luego de calcular la matriz de correlación, se ve que la correlación es muy baja, lo que no me parece intuitivo, dado que la gran mayoría de los ejemplos de títulos (más del 90%) son Mr, Mrs y Miss lols cuales se podrían relacionar directamente con el género. . (Imagen de correlación provista debajo) . . Luego, se normalizan los datos con el standard scaler (estandarización Z). . Pasos Opcionales . Como no se requiere, no voy a comentar todos los aspectos sobre la parte de modelado, predicción y evaluación de rendimiento, pero sí los incluí en el código y me interesaría remarcar algunos puntos. . Se puede ver que los mejores resultados a primera vista, pues la única métrica que estamos mirando es accuracy (no es suficiente para tomar una decisión sobre si el modelo es bueno o no), son los de SVC y random forest, alrededor de 0.82 – 0.83, pero varía cada vez, dado que el random forest tiene un grado de aleatoriedad en su construcción. . Cabe destacar que, aunque los resultados del random forest sean los mejores para este set de testeo, el modelo tardó 60 segundos en entrenar, mientras que SVC tardó 1.2 segundos y la regresión logística 3.4s. Para conjuntos de datos más grandes o muy cambiantes para los que se necesita reentrenar el modelo constantemente, habría que ver si los pocos puntos de mejora en la accuracy justifican el costo inmensamente mayor de entrenamiento del modelo de random forest. Probablemente los tiempos de inferencia también sean mucho mayores en el random forest. . .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/11/Preparaci%C3%B3n-de-datos-y-modelado-con-el-dataset-Titanic.html",
            "relUrl": "/2021/09/11/Preparaci%C3%B3n-de-datos-y-modelado-con-el-dataset-Titanic.html",
            "date": " • Sep 11, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Normalización Y Detección De Outliers",
            "content": "Rapid Miner outlier detection and normalization tutorial. . Se aplica normalización para luego aplicar el cálculo de las distancias para detectar a los outliers. Siempre se debe normalizar previo al cálculo de los outliers por el hecho de que los diferentes atributos suelen no tener la misma unidad o escala. Para la normalización se utiliza la transformación z. . El operador detect outliers detectará los 10 ejemplos que se alejen más del resto. En el flujo, luego colocamos un filter para remover los 10 outliers antes mencionados. . Ejercicio 2 . El dataset posee 13 atributos sin incluir a la variable objetivo. Estos describen distintos elementos de la composición de vinos para luego poder clasificarlos. . La variable objetivo posee tres valores posibles. Estos valores representan a 3 diferentes variantes de viñedos (las plantas de las que se genera el vino son diferentes genéticamente entre sí). . La variable objetivo está relativamente balanceada, no hay una clase que presente un numero muy bajo de ejemplos, por lo que argumentaría que no habría que aplicar ningún tipo de sampling. . Para algunos atributos, la distribución parece ser normal (ash, alcalinity of ash, proanthocyanins), mientras que algunas otras parecen tener un sesgo hacia la derecha (por ejemplo malic acid y proline), pero no es muy grave tampoco. . Creación del modelo con naive bayes. . Al visualizar objetivamente la matriz de confusión, los resultados no variaron al aplicar la normalización. Con ambos tipos de normalización (z score y min-max), los resultados fueron los mismos. . Flujo de rapidminer: . El para los Split data, se utilizó la misma seed y la misma proporción (70/30) para que los conjuntos de entrenamiento y testeo sean los mismos para ambas pruebas, con la diferencia que uno estará normalizado y otro no. . . Matriz sin normalizado . . Matriz con normalizado . . Conclusiones . El normalizado no afecta los resultados para este caso en particular. Esto se debe al algoritmo que utilizamos principalmente. .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/2021/09/10/Normalizaci%C3%B3n-y-detecci%C3%B3n-de-outliers.html",
            "relUrl": "/2021/09/10/Normalizaci%C3%B3n-y-detecci%C3%B3n-de-outliers.html",
            "date": " • Sep 10, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://josevilaseca.github.io/Portafolio-ML/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://josevilaseca.github.io/Portafolio-ML/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://josevilaseca.github.io/Portafolio-ML/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://josevilaseca.github.io/Portafolio-ML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}