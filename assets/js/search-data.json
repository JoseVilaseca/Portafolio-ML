{
  
    
        "post0": {
            "title": "Introducción al modelo CrispDm",
            "content": "LinkedIn GitHub . Introducción . El desarrollo de soluciones que utilicen inteligencia artificial debe ser holistico en el sentido que todas las partes/períodos del desarrollo son sumamente importantes y están muy interrelacionadas entre sí. Por esto, existe un estándar llamado CRISP DM, en el cual se basará este portafolio para explicar qué se debe hacer en cada paso del desarrollo de soluciones, con los puntos clave en cada uno de ellos. . Crisp DM . El proceso CRISP-DM (Cross-Industry Standard Process for Data Mining) describe las fases principales de un proyecto de minería/ciencia de datos, incluyendo la relación que existe entre cada etapa. . . A continuación se explicará qué se debe considerar en cada etapa del proceso CRISP-DM. . Entendimiento del Negocio . El entendimiento del negocio y de la industria para la cual se realizará un proyecto que involucre aprendizaje automático es sumamente importante. Hay ciertas industrias que priorizan el entendimiento de los modelos antes que la performance per se que estos tengan. Por ejemplo, para industrias en las que el resultado de una predicción puede afectar positiva o negativamente la vida de una persona, es muy importante que se pueda explicar por qué se tomó esa decisión, por lo que algunos algoritmos complejos (cajas negras, ej. redes neuronales profundas) quedan descartados para el modelado. . También, la naturaleza de la solución pondrá restricciones sobre las distintas alternativas de una solución. Por ejemplo, un sistema de clasificación de imágenes/detección de objetos en tiempo real, deberá poder generar varias predicciones por segundo, lo que limita tanto el lugar en donde estará desplegado el modelo (probáblemente en un sistema embebido o en el edge) y deberá tener una potencia computacional alta para poder cumplir con las restricciones. Las librerías también deberán estar específicamente optimizadas para poder correr en sistemas que no posean grandes cantidades de memoria y ser sumamente eficientes en las funcionalidades que proveen (TF Lite es un buen ejemplo de esto). . Por otro lado, si estamos hablando de soluciones en las que el modelo se entrena una vez por día y se tiene un conjunto muy grande de datos, se deberán priorizar los algoritmos que puedan proveer una performance decente, pero principalmente que posean un tiempo de entrenamiento bajo, para cumplir con las restricciones temporales. . Entendimiento de los Datos . Luego de entender el negocio y las posibles aristas por las que se podrá proseguir en un proyecto, el siguiente paso es entender los datos. El entendimiento de los datos o análisis exploratorio de los datos, es nuestro primer acercamiento a ellos. En él, se intentarán descubrir los patrones invisibles a primera vista que existen en los datos. . En este paso, deberemos estudiar los tipos de los atributos, ver qué tipo de solución requiere el problema a resolver, ¿es de clasificación, regresión o clustering? También, se deben analizar las distribuciones de todos los atributos. Por último, se deben generar hipótesis sobre relaciones en los datos que luego deberán ser probadas en el paso del modelado. . . . Preparación de los Datos . Nuestros modelos son tan buenos como los son nuestros datos. Por esto es muy importante hacer un preprocesamiento a los mismos acorde al algoritmo de aprendizaje que vayamos a utilizar para modelar el problema. Se deberá contar con una cantidad de datos suficiente y representativa para que los modelos generados puedan abstraer los conceptos subyacentes que existen en el conjunto de datos. . Debido a lo antes mencionado, hay varias aristas a recorrer en la etapa de la preparación de los datos. . Normalizado y Estandarización . Existe una gran cantidad de algoritmos que obtienen mejores resultados cuando rangos de los predictores estén normalizados o estandarizados. Existen muchas formas de normalizar los datos pero a lo largo del portafolio, utilizaré las 2 más comunes mencionadas a continuación . Normalización Min-Max . Este tipo de normalización utiliza los valores mínimo y máximo del conjunto y utiliza la siguiente transformación para obtener los nuevos valores de nuestro predictor. Los posibles valores pertenecen al rango [0,1]. . . #### Estandarización o Transformación Z Dicha técnica de normalizado refiere a la utilización de la media y de la desviación estándar de un atributo para realizar la siguiente transformación. La distribución resultante tendrá media 0 y desviación estandar 1. . . Manejo de valores faltantes o nulos . En un mundo ideal, los datasets no poseerían valores faltantes pero la realidad está bastante alejada de eso. Tanto en datasets utilizados mundialmente (por ejemplo Titanic) como también en datasets de empresas que quieren incorporar ciencia de datos a sus procesos, la presencia de valores nulos es algo común. . En primera instancia, se debe analizar qué significan estos valores nulos, porque pueden tener distintos significados dependiendo del contexto. Uno de los posibles significados de un valor nulo es que no se cargó el dato o no se conoce el valor exacto, pero este dato probáblemente se encuentra en el rango de valores de la distribución del atributo al que pertenece (MAR). . Otro significado diferente es que un dato no tenga sentido que exista y debido a esto, se coloca el valor nulo en el ejemplo. Por ejemplo, en una encuesta en la que se pregunta por salario a personas, no tiene sentido que una persona que no posee un empleo actualmente cargue un valor numérico en el atributo salario. . Luego de conocer el significado de los valores nulos, hay que elegir una estrategia para tratarlos. . Elección de algoritmos que permitan valores nulos . Se pueden utilizar algoritmos como Naive Bayes o KNN. . Imputación de valores con media, mediana y moda . Para atributos numéricos, cuando la distribución de este es normal, se suele imputar con la media de la distribución. Cuando la distribución está sesgada, imputar con la mediana es una mejor elección. . Para atributos categóricos, imputar con la moda es una buena forma de deshacernos de los valores nulos. . Utilización de un modelo para predecir valores nulos. . Una estrategia más compleja, pero generalmente más aproximada es utilizar un modelo para predecir qué valor debería ocupar el lugar del valor nulo dependiendo de los otros atributos del ejemplo. . Outliers . Los outliers son valores que se alejan significativamente del resto de la muestra. Dependiendo de nuestra aplicación en particular, estos casos deberán ser removidos o tratados con especial cuidado porque pueden llegar a ser claves para detectar algún fenómeno que estemos tratando de predecir (por ejemplo detección de fraude). . . Muestreo para dasets desbalanceados . Cuando se posee un dataset significativamente desbalanceado, los ejemplos de una de las clases tienen una cardinalidad significativamente menor que la cardinalidad de los ejemplos de la otra clase (para problemas de clasificación binaria). La relación entre ellas puede ser 1:50, 1:100 o incluso más diferencia. . Este sesgo puede influir en el aprendizaje de muchos algoritmos de machine learning, lo que suele resultar en un sobreajuste que predice siempre la clase más numerosa, o tiene una performance baja para predecir ejemplos de la clase minoritaria, que suelen ser los casos más importantes y los que nos interesa reconocer. . Dos estrategias posibles para atacar estos casos es aplicar submuestreo (undersampling) o sobremuestreo (oversampling). En el submuestreo se reduce la cantidad de ejemplos de la clase más numerosa de forma aleatoria y en el sobremuestreo se generan nuevos ejemplos de la clase menos numerosa a partir de los ejemplos existentes. Una de las librerías más utilizadas que ataca este problema se llama imblearn. . Transformaciones para atributos con distribución sesgada . Para distribuciones fuertemente sesgadas (skewed), aplicar transformaciones a la misma generalmente mejora la performance de los modelos entrenados con dichos datos. Dos transformaciones recomendadas son la función logarítmica y la transformación Box-Cox. . . . Selección de atributos y feature engineering (Utilizado y explicado en casos de estudio y post) . Modelado . La etapa de modelado refiere a la creación de modelos a partir de datos. Estos modelos aprenderán de relaciones entre los datos con los que se entrenen y podrán hacer predicciones sobre datos no vistos. . A continuación se presentarán 2 grandes categorías de algoritmos de Machine Learning: los algoritmos lineales y los no lineales. . Algoritmos Lineales . Los algoritmos lineales asumen una relación lineal entre los predictores y la variable objetivo. Son algoritmos simples que tienen un gran potencial para modelar relaciones lineales. El poseer distribuciones gausianas en los atributos ayuda a estos algoritmos a obtener mejores resultados. . Regresión Lineal . A pesar de la simplicidad del algortitmo de regresión lineal, este algoritmo sigue siendo muy utilizado por su capacidad de modelar relaciones lineales. A diferencia de algoritmos más complejos, este posee numerosas ventajas como lo son: . -Tiempo de entrenamiento bajo . -Posibilidad de explicar por qué se predice un valor para un ejemplo no visto . -Tiempo de predicción/inferencia bajo . Regresión Logística . Para problemas de clasificación binarios, la regresión logística es un algoritmo simple y performante. Este utiliza la función logística (o función sigmoide) que transforma una combinacion lineal de variables a un rango entre 0 y 1. En este caso, el valor que tomará la función coincide con la probabilidad con la que el modelo puede afirmar que un ejemplo pertenece a la clase por defecto. . Utilizando esta probabilidad, podremos generar predicciones escogiendo un umbral (desde un aspecto probabilístico 0,5 sería el umbral correcto) y luego definiendo que un ejemplo pertenece a la clase primaria si su probabilidad es mayor a ese umbral y que pertenece a la clase secundaria en el caso contrario. . Análisis Discriminante Lineal . En el caso de problemas de clasificación multiclase, la opción lineal por defecto es el análisis discriminante lineal. Este algoritmo no posee ciertas desventajas que existen en la regresión logística como la limitación a problemas de 2 clases, su inestabilidad con clases bien separadas y su inestabilidad cuando se tienen pocos ejemplos. . Algoritmos no lineales . Estos algoritmos no asumen una relación de algún tipo específico como lo hacen los algoritmos lineales. Suelen tener menos sesgo (bias) dado que no asumen una relación específica entre los predictores y la variable objetivo, pero una mayor varianza (variance) que los algoritmos lineales. . Árboles de Decisión . Los árboles de decisión son algoritmos que derivan de la rama de informática. Utilizando la estructura de un árbol, cada nodo posee una condición asociada a una única variable de entrada, y dependiendo del valor, el árbol decidirá por qué hijo se deberá proceder para que se pueda aplicar una predicción. Al llegar a un nodo hoja, se aplica la predicción. . Para el entrenamiento de los mismos se utilizan conceptos como la ganancia de información o el índice de gini para determinar cuáles son las mejores particiones de los datos tal que exista una mayor ganancia de información o mayor separación de las clases. . Este tipo de algoritmo requiere que se aplique una optimización de hiperparámetros para reducir el sobreajuste del modelo al set de entrenamiento. Existen múltiples parámetros que ayudan a que el ajuste sea el correcto como por ejemplo limitar la altura del arbol, escoger un mínimo de ejemplos para cada nodo hoja, etc. . Máquinas de Soporte Vectorial . Las máquinas de soporte vectorial son un tipo de algoritmo de machine learning relativamente moderno (las primeras implementeciones se desarrollaron en los años 90). Este es un algoritmo para problemas de clasificación binaria (en su implementación básica, pues existen extensiones que amplian el algoritmo para problemas multiclase). . El propósito de este tipo de algoritmo es utilizar vectores de soporte (puntos de ambas clases que actúen como límites de su clase), un tipo de kernel (lineal, polinómico, radial, etc) y una constante C (definirá la flexibilidad con la que permitiremos violaciones respecto al márgen o frontera que se utilizará para la clasificación) para generar una función n-dimensional que separe a ambas clases de la mejor manera posible. . Artículos publicados y presentaciones realizadas . Despliega tu primer modelo de reconocimiento de imágenes en la nube GitHub . Watson de la A a la Z - Visual Recognition y Natural Language Classifier .",
            "url": "/Portafolio-ML/markdown/crispdm/businessunderstanding/dataunderstanding/datapreparation/normalization/linearregression/logisticregression/lineardiscriminantanalysis/decisiontrees/svm/2021/12/02/Introducci%C3%B3n-al-modelo-CrispDm.html",
            "relUrl": "/markdown/crispdm/businessunderstanding/dataunderstanding/datapreparation/normalization/linearregression/logisticregression/lineardiscriminantanalysis/decisiontrees/svm/2021/12/02/Introducci%C3%B3n-al-modelo-CrispDm.html",
            "date": " • Dec 2, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Caso De Estudio 3   Enfermedad Cardíaca",
            "content": "Caso de estudio 3 . Predicción de enfermedad de corazón . En la actualidad, las enfermedades cardíacas son la principal casusa de muerte a nivel mundial[1] . La enfermedad isquémica del corazón fue responsable del 16% de las muertes globales en el 2019. En los Estados Unidos, la principal causa de muerte es otra enfermedad cardíaca llamada enfermedad de la arteria coronaria[2]. . Además de esto, entre 3 a 5% de las admisiones a hospitales se deben a pacientes con insuficiencia cardíaca. Un 2% de los costos médicos de los países desarrollados son resultado de dicha condición.[3] . Debido a este problema mundial, una de las disciplinas en las que se aborda el tema con la intención de mitigar los riesgos y costos, mejorando así la calidad de vida de las personas es Machine Learning. Utilizando diferentes técnicas de aprendizaje automático, se puede estimar con cierto grado de confianza la presencia, el subtipo, y la severidad de una enfermedad cardíaca. Más aún, se pueden predecir eventos como desestabilizaciones, re-hospitalizaciones y mortalidad. . Descripción de los conjutos de datos utilizados . Los conjuntos de datos utilizados en este caso de estudio son 4 datasets distintos. Cada uno de estos contiene medidas de 76 atributos (los mismos atributos en cada dataset), pero los datasets provienen de diferentes instituciones médicas. Estas son: Cleveland Clinic Foundation, Hungarian Institute of Cardiology, Budapest, V.A. Medical Center, Long Beach, CA y University Hospital, Zurich, Switzerland. . Los datasets contienen entre 123 y 294 ejemplos cada uno. . Algunos de los atributos a destacar, utilizados en estudios previos son . 1) Age: edad en años del paciente. Posee una distribución gaussiana con media 53, y desvío 9. . 2) Sex: 1-Masculino 0-Femenino. Predominan los ejemplos masculinos por un gran margen (711 contra 188) . 3) Cp: Tipo de dolor en el pecho. Valores de 1 a 4. El valor 4 es el predominante (asintomatico, 485 ejemplos), mientras que el 1 es el más escaso con solo 45 ejemplos. Los valores 2 y 3 poseen entre 150 y 200 ejemplos. . 4) Trestbps: Presión sanguínea en reposo (medida en mm Hg al ingresar al hospital). Posee una distribución gaussiana. Tiene valores faltantes. . 5) Chol: colesterol en suero (en mg/dl). Distribución no se asemeja a una conocida. . 6) Fbs: nivel de azucar en ayuno &gt; 120 mg/dl (1 = verdadero; 0 = falso). 0 es mucho más prevalente en el dataset (674 vs 135) . 7) Restecg: electrocardiograma en reposo (0-Normal, 1-having ST-T wave abnormality, 2-showing probable or definite left ventricular hypertrophy by Estes’ criteria). 0 es el valor más prevalente, 1 y 2 tienen cardinalidades parecidas (177 y 182) . 8) Thalach: ritmo cardíaco máximo alcanzado. Se asemeja a una distribución normal con un leve sesgo hacia la izquierda. . 9) Exang: angina inducida por ejercicio (1-Si, 0-No). Existen 514 ejemplos para el valor 0 y 330 para el valor 1. . 10) Oldpeak: ST depression induced by exercise relative to rest. Distribución no se asemeja a una conocida. . 11) Slope: the slope of the peak exercise ST segment (1-upsloping, 2-flat, 3-downsloping). El caso más común es 2, seguido por 1 y el menos prevalente es el 3. . 12) Ca: number of major vessels (0-3) colored by flourosopy. Distribución sesgada hacia la derecha. A medida que se aumenta el número de vasos, los ejemplos disminuyen. . 13) Thal: 3 = normal; 6 = fixed defect; 7 = reversable defect. Posee 477 valores faltantes. Mayor prevalencia en 3 y 7 (alrededor de 180 ejemplos). . 14) Num: diagnosis of heart disease (angiographic disease status) (0- &lt; 50% diameter narrowing, 1- &gt; 50% diameter narrowing). Los posibles valores enteros van del 0 al 4. #0=404, #1=191, #2=130, #3=132, #4=42. 0 significa que el paciente no posee la condición y los valores de 1 a 4 indican la severidad de la misma . Los datos poseen atributos faltantes y están representados con el valor -9. . El dataset de Cleveland posee unas pocas filas corruptas por encoding al final del archivo. Las primeras 282 filas de las 293 no tienen problemas, por lo que esas serán las utilizadas en los siguientes pasos del caso de estudio. . Preparación previa de los datos . Antes de cargar los datasets en rapidminer, es necesario aplicarles una limpieza o transformación para tenerlos estructurados. El script utilizado se puede observar el el repositorio de github del portafolio. . Flujo Rapidminer . . Selección de atributos previa . Debido a la gran cantidad de valores faltantes en algunos atributos, decido descartar los que tengan más de 33% de valores faltantes. También descarto name e id. . Columnas descartadas: . Name, Id, ccf, pncaden, cigs, years, dm, famhist, smoke, thaltime, slope, rldv5, ca, restckm, exerckm, restef, restwm, exeref, exerwm, thal, thalsev, thalpul, earlobe, ramus, om2, cathef, junk. . Estrategias de imputación . Para los predictores con una mayor cantidad de valores nulos dentro de los que no fueron descartados y de tipo categóricos, decido elegir un valor definido que represente que ahí existe un valor nulo. Imputo con el valor -1. Luego, decido imputar los restantes predictores categóricos con su moda (estos tienen menos de 19 valores nulos que equivale aproximadamente a un 2% de los ejemplos). Además, estos poseen una cardinalidad extremadamente alta en su moda, por lo que esta imputación, con un grado de seguridad muy alto, representa el valor exacto que tendrían estos ejemplos. Por último, se decide imputar los últimos valores faltantes con la media, ya que la gran mayoría de estos poseen distribuciones gaussianas o uniformes y valores enteros o reales. Hay 3 predictores en los que hubiera escogido una estrategia de imputación utilizando la mediana por su distribución fuertemente sesgada hacia la derecha (predictores ridv5e, proto y met, véase la imagen a continuación). Se podría aplicar una transformación logarítmica a estos 3 predictores para obtener una distribución más parecida a una gaussiana. . (Distribuciones simil-gaussianas) . . (Distribuciones fuertemente sesgadas hacia la derecha) . . Para el modelado con KNN, se aplicará además una normalización a los datos, ya que este modelo requiere dicha normalización. . Una nota importante sobre la preparación previa de los datos es que la única estrategia que se realizará previo a los bloques anidados de cross validation es la estrategia de imputación con el valor conocido -1 que representa que el valor es uno no conocido. Esto no depende del set de datos elegido por un Split específico de entrenamiento/test, ya que este valor siempre será el mismo. . La historia es diferente cuando hablamos de la media, moda y la transformación específica que se aplica para normalizar. Estas transformaciones sí dependen de los datos que se eligen, por lo que normalizar o imputar con uno de estos valores antes de separar los conjuntos en entrenamiento y testeo contribuye con un fenómeno llamado contaminación accidental. Dicho fenómeno causa que las estimaciones de las métricas que realizamos para validar nuestros modelos no sean realistas, siendo más optimistas que el rendimiento verdadero que tendrá nuestro modelo. . Por los argumentos expuestos en el párrafo anterior, estas transformaciones se realizarán para los conjuntos de entrenamiento (por ejemplo, se toma la media y desviación estándar del conjunto de entrenamiento), y luego se transforma a los datos de testeo con los valores hallados para la transformación de los datos de entrenamiento. . En la imagen a continuación, se pueden observar los operadores dentro del proceso de CV externo. Se puede apreciar que la media y moda (no hay un bloque de normalización debido al algoritmo random forest) se calculan para el conjunto de entrenamiento, y previo a realizar nuestras predicciones con el modelo en el conjunto de testeo, se aplican las imputaciones deseadas en el bloque Apply Model (3). . 4- Algoritmos y modelos . En cuanto al algoritmo de aprendizaje para generar el modelo de predicción de enfermedades del corazón, claramente debemos considerar los algoritmos supervisados de clasificación. . Utilizando los estudios Citados por el paper “Heart Failure: Diagnosis, Severity Estimation and Prediction of Adverse Events Through Machine Learning Techniques”[3] , diversos grupos de investigadores utilizaron diferentes algoritmos de ML, por lo que elegir uno a priori sin probar una gran variedad de ellos sería un enfoque bastante ingenuo y sesgado en mi opinión. Debido a esto, utilizaré una variedad de algoritmos y elegiré el que mejor se adecúe al dataset elegido. . Como primer paso, propongo utilizar Naive bayes como benchmark inicial de performance. Este utilizará la corrección de Laplace para prevenir el error provocado por el caso en el que exista un valor en el set de test que no haya estado en el set de entrenamiento. Me parece pertinente remarcar que se utilizará sampling estratificado en el bloque de cross validation (siempre utilizaré este tipo de sampling, pero este es especialmente importante para Naive Bayes). Luego, consideraré una serie de algoritmos utilizados por los investigadores de los papers consultados. . Dentro de estos, se podría justificar no utilizar knn por un consumo alto de memoria debido a un dataset de entrenamiento grande, pero con los bajos costos de memoria de hoy en día, este no debería ser un impedimento para un conjunto de datos que pese un par de megas. El orden cuadrático para realizar predicciones (n*(n-1)/2) igualmente podría suponer un problema. Como nuestro dataset no es uno muy grande (no supera los 1000 ejemplos), no habría que descartar este algoritmo. Si utilizaramos otra herramienta como Python-scikitlearn, se debería aplicar one-hot encoding para los predictores categóricos, pero en rapidminer esto no es necesario, ya que se puede utilizar una mixed measure llamada mixed euclidean distance. Para los valores nominales, la distancia es 0 si ambos valores son iguales y 1 si son diferentes. Para valores numéricos, se utiliza la distancia euclídea. Por esto, para que todos los predictores tengan el mismo rango, se normaliza con la transformación min-max (de 0 a 1) . Luego de obtener las métricas para el modelo de KNN, utilizaré difentes tipos de ensambles inicialmente Random Forest y alguna variante de Boosted Trees como AdaBoost o GradientBoostedTrees (Rapidminer) debido a su alta performance para casos de la industria estudiada. Además, como otra justificación para utilizar estos métodos, no existe un requerimiento de que las predicciones tengan que ser extremadamente rápidas, por lo que la utilización de un modelo de Random Forest con miles de árboles no sería un problema. Como los boosted trees son muy susceptibles a outliers, se deberá analizar cuidadosamente la presencia de estos. . SVM es un algoritmo que no podrá ser utilizado para este caso en específico pues se poseen atributos categóricos, no soportados por este algoritmo. . Como este es mi último caso de estudio, decido utilizar toda la batería de conceptos aprendidos en el curso para generar modelos robustos con una validación correcta con la menor contaminación accidental posible. También, se tomarán en cuenta las distintas métricas que podemos obtener de cada modelo para hacer las comparaciones de los modelos obtenidos. . Todos los bloques de cross validation utilizados en la solución, usan la misma semilla y k=5 debido al altísimo costo computacional necesario para tanto entrenar los mejores modelos posibles (utilización de estrategias evolutivas para selección de atributos y para tuning de hiperparámetros) como aplicar una correcta validación de los mismos. Esta correcta validación implica bloques de cross validation anidados. . Naive Bayes . . . KNN . Se optimiza k (valores entre 1 y 100) . . . Random Forest . Se optimizan los hiperparámetros: . 1) subset_ratio entre 0.2 y 1. (Según [4] Se recomienda utilizar la raíz de la cantidad de predictores para problemas de clasificación) . 2) number_of_trees entre 500 y 3000. . . GradientBoostedTrees . Se optimizan los hiperparámetros: . 1) Number_of_trees: de 50 a 500 . 2) Learning_rate: de 0.05 a 1 . 3) Maximal_depth: de 1 a 10 . . . Conclusiones sobre los diferentes modelos . Como se puede ver en las matrices de confusión expuestas anteriormente, dos de los modelos quedan claramente descartados. Estos son KNN y Naive bayes. Su performance no está al nivel de los dos últimos modelos generados, y como lo que buscamos para este caso es maximizar la performance al máximo, no seguimos explorando la posibilidad de utilizar estos algoritmos. . En cambio, la performance de los ensembles escogidos es buena, significativamente mejor que la de los modelos anteriores. Pero eso nos lleva a la pregunta: ¿cuál de los 2 modelos escogemos como el mejor para este caso de estudio? . Normalmente, la métrica a la que se le da más importancia es al accuracy o exactitud. Si nos guiáramos por esa métrica, claramente escogeríamos el modelo entrenado utilizando Random Forest por su aumento de 2.56% en el accuracy promedio frente al otro modelo según la validación cruzada que se realizó. . No obstante, en este caso de estudio, no nos guiaremos por esa métrica absoluta, pues ese no es el punto más importante. Como en este caso un falso negativo (con negativo me refiero a no enfermedad) para la clase 0 (predecir que un paciente no posee la enfermedad cuando en realidad la tiene) es extremadamente costoso en cuanto a costos monetarios y la vida de las personas, considero que lo más importante es obtener la mejor precisión posible. Es de suma importancia priorizar esta métrica para la clase 0. . Las 4 personas que el modelo de Random Forest predijo como clase 0 cuando en realidad eran clase 4 (máximo grado de severidad posible de enfermedad cardíaca) es un error muy grave. Sin embargo, el modelo de GradientBoostedTrees solo se equivocó con casos de la clase 1 (menor grado de severidad) y en menor medida que el otro modelo (por lo tanto tiene mayor precision). También, el modelo de boosted tres posee un mayor recall para la clase 0, lo cual es un extra. . Resumiendo, por la mejor precisión para la clase 0 y por el accuracy comparable al modelo con mejor accuracy que posee el modelo de GradientBoostedTrees, este es el mejor modelo para este caso de estudio. . 5- Conclusiones generales sobre el caso abordado y la viabilidad u oportunidad de aplicación de técnicas de machine learning en el mismo. . Comparando los resultados obtenidos por el modelo elegido, este posee una mayor exactitud (no tenemos la precisión) que la que obtuvieron múltiples estudios rigurosos en 1989 citados en las referencias de este caso de estudio[5]. Se utilizó una mayor cantidad de atributos (sin abarcar la totalidad de los atributos elegidos por dichos estudios) y un algoritmo más moderno que los que había en la época de los estudios. Es debido a estas razones que se consiguieron mejores resultados. . En cuanto a la viabilidad de la aplicación de técnicas de machine learning para predicción de enfermedad cardíaca, si las personas están dispuestas a generar los datos (sus datos personales) necesarios para que se pueda predecir si se tiene la enfermedad o no, está muy claro que se brindaría un gran beneficio a ellas y a la sociedad entera. . Bibliografía . [1] https://www.who.int/news-room/fact-sheets/detail/the-top-10-causes-of-death . [2] Application of Machine Learning Algorithms to Predict Coronary Artery Calcification With a Sibship-Based Design Yan V. Sun1,* , Lawrence F. Bielak1, Patricia A. Peyser1, Stephen T. Turner2, Patrick F. Sheedy II3, Eric Boerwinkle4, and Sharon L.R. Kardia (2008) . [3] Heart Failure: Diagnosis, Severity Estimation and Prediction of Adverse Events Through Machine Learning Techniques Evanthia E. Tripoliti, Theofilos G. Papadopoulos , Georgia S. Karanasiou, Katerina K. Naka, Dimitrios I. Fotiadis (2016) . [4] https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/ . [5] Past Usage: . 1. Detrano,~R., Janosi,~A., Steinbrunn,~W., Pfisterer,~M., Schmid,~J., . Sandhu,~S., Guppy,~K., Lee,~S., &amp; Froelicher,~V. (1989). { it . International application of a new probability algorithm for the . diagnosis of coronary artery disease.} { it American Journal of . Cardiology}, { it 64},304–310. . -- International Probability Analysis . -- Address: Robert Detrano, M.D. . Cardiology 111-C . V.A. Medical Center . 5901 E. 7th Street . Long Beach, CA 90028 . -- Results in percent accuracy: (for 0.5 probability threshold) . Data Name: CDF CADENZA . -- Hungarian 77 74 . Long beach 79 77 . Swiss 81 81 . -- Approximately a 77% correct classification accuracy with a . logistic-regression-derived discriminant function . 2. David W. Aha &amp; Dennis Kibler . -- Instance-based prediction of heart-disease presence with the . Cleveland database . -- NTgrowth: 77.0% accuracy . -- C4: 74.8% accuracy . 3. John Gennari . -- Gennari, J.~H., Langley, P, &amp; Fisher, D. (1989). Models of . incremental concept formation. { it Artificial Intelligence, 40}, . 11–61. . -- Results: . -- The CLASSIT conceptual clustering system achieved a 78.9% accuracy . on the Cleveland database. .",
            "url": "/Portafolio-ML/2021/12/01/Caso-de-Estudio-3-Enfermedad-Card%C3%ADaca.html",
            "relUrl": "/2021/12/01/Caso-de-Estudio-3-Enfermedad-Card%C3%ADaca.html",
            "date": " • Dec 1, 2021"
        }
        
    
  
    
  
    
        ,"post3": {
            "title": "Caso De Estudio 2   Parkinsons Disease Document",
            "content": "Parkinsons Disease Data Set Case Study . Case Study introduction . “Parkinson’s disease is a degenerative brain disorder which usually presents symptoms like stiffness, body tremors, and difficulty maintaining balance and coordination. The symptoms get worse with age and usually start developing around 60 years of age. . There is currently no way to diagnose Parkinson’s using blood or laboratory tests for non-genetic cases. Diagnoses are done using patients’ medical history and neurological exams. It is important to diagnose Parkinson’s early, due to very similar symptoms it has to other diseases which require different treatments.”[1] . Brief description of the data set and a summary of its attributes . The dataset was created by Max Little (University of Oxford) in collaboration with the National Centre for Voice and Speech (Denver, Colorado), who recorded speech signals. . It contains biomedical voice measurements from 31 people, 23 with Parkinson’s disease. The attributes are particular voice measures, and the examples correspond with the 195 voice recordings. . Attributes: . name - ASCII subject name and recording number . MDVP:Fo(Hz) - Average vocal fundamental frequency . MDVP:Fhi(Hz) - Maximum vocal fundamental frequency . MDVP:Flo(Hz) - Minimum vocal fundamental frequency . MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several . measures of variation in fundamental frequency . MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude . NHR,HNR - Two measures of ratio of noise to tonal components in the voice . status - Health status of the subject (one) - Parkinson’s, (zero) - healthy . RPDE,D2 - Two nonlinear dynamical complexity measures . DFA - Signal fractal scaling exponent . spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation . (Attribute information from Parkinson.names file in the UCI Repository). . All attributes but name (pacient name) and status (pacient has the disease (1) or not (0)) are real values. The pacient name is not important as it shouldn’t have any descriptive insight to whether a person has Parkinson or not, so we will later remove that attribute. This makes all our predictors real values. . Initial plan for data exploration . The initial plan is to explore attribute distributions, correlations and detect outliers within the distributions. . First, I have decided to plot (sns pairplot) one of each type of attribute so as to have a clearer visualization than if we had called sns pairplot on the entire dataset. By type of attribute im referring to choosing only one predictor that models a certain characteristic. For example, I only plot NHR instead of plotting NHR and HNR in order to reduce the number of graphs created. . . By checking at some pairs of attributes, we can notice some degree of separation between both classes. For example, we can see that when comparing NHR and spread1, patients who have parkinsons disease have higher values of spread 1 than patients who don’t have the disease. The outlier points in the majority of the plots tend to belong to the parkinsons class too. . The highest correlations between attributes and status are 0.56 (spread1) and 0.53 (PPE). . Feature engineering and data cleaning . Regarding Data cleaning, not much is required for the selected dataset. We don’t have highly correlated columns and there are no null values. Thus we don’t need to think about which would be the best strategy (imputation, drop column, drop row, etc.). The only cleaning made was to drop the “pacient name” attribute. . As for feature engineering, I have noticed the dataset has some attributes whose distribution is heavily skewed. . . Due to this fact, I have decided to use a log transformation for attributes with a skew value greater than 1. This makes the distribution of the transformed predictors to resemble a gaussian distribution, which is a prerequisite for many ML algorithms. . Attributes’ distribution after transformation: . . The next step is to normalize the data. I chose to use z transformation, but Min-Max normalization is another option . . Key findings and insights . By comparing the distributions of parkinson patients’ attributes with non- parkinson patients’ attributes, we can identify some insights which were previously mentioned. The range of values in almost all of the attributes is undeniably greater than the one In attributes of non- parkinson patients. This could be a key insight when trying to predict a parkinson patient. For example, if spread1 is greater than -4, we could infer that the patient has parkinson. Also, the higher spread1 is, the more likely a given patient has parkinson. Also, outlier points tend to correspond with patients who have the disease. . Hypothesis Testing and Formal Significance Tests . 1-Null hypothesis: The mean of spread1 for parkinsons patients is the same than the mean of spread1 for non-parkinsons patients. . Alternative hypothesis: The mean is different . . Thus, we reject the null hypothesis by a great margin. The means are different. . 2-Null hypothesis: The mean of spread1 for parkinsons patients is larger than the mean of spread1 for non-parkinsons patients by at least 1.5 standard deviations of the attribute’s distribution. . Alternative-hypothesis: The mean of spread1 for parkinsons patients is not larger than the mean of spread1 for non-parkinsons patients by at least 1.5 standard deviations of the attribute’s distribution. . . The null hypothesis is accepted, so we can confirm with 95%confidence that the null hypothesis is correct. . 3-Null hypothesis: The mean of DFA for parkinsons patients is larger than the mean of DFA for non-parkinsons patients by at least 1 standard deviation of the attributes distribution. . Alternative-hypothesis: The mean of DFA for parkinsons patients is not larger than the mean of DFA for non-parkinsons patients by at least 1 standard deviation of the attributes distribution. . . The null hypothsis is rejected, so we can’t confirm with a 95% confidence that the mean of DFA for parkinsons’ patients is larger than the mean of DFA for non-parkinsons’ patients by at least 1 standard deviation of the attributes distribution. . Suggestions for next steps in analyzing this data. . I would suggest to analyze and compare the distributions of attributes within the same attribute types (attributes that measure the same thing). It is important to separate the distributions by status, so we can check the distribution for parkinsons patients and non-parkinsons patients. . Also, we could implement Principal component analysis to check which attributes have the greatest variance. . Summary of quality of data . I believe the quality of the dataset is decent. There are no missing values and you can visually have an idea of which attributes are going to have a greater effect on predictions (such as spread1). . However, the dataset only has 195 rows, so we could be building this whole EDA on a biased small group of individuals. If more data could be generated with patients from different parts of the world, different age ranges and social status, I would be more confident that the model generated with this data would be a solid one, capable of making correct predictions from samples all round the world. . References: . [1]: https://www.beloit.edu/live/profiles/2530-employing-data-mining-techniques-on-biomedical . https://archive.ics.uci.edu/ml/datasets/parkinsons . Required Citations:  ’Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection’, Little MA, McSharry PE, Roberts SJ, Costello DAE, Moroz IM. BioMedical Engineering OnLine 2007, 6:23 (26 June 2007) .",
            "url": "/Portafolio-ML/2021/11/12/Caso-de-Estudio-2-Parkinsons-Disease-Document.html",
            "relUrl": "/2021/11/12/Caso-de-Estudio-2-Parkinsons-Disease-Document.html",
            "date": " • Nov 12, 2021"
        }
        
    
  
    
  
    
        ,"post5": {
            "title": "Diferentes Estrategias Para Feature Selection   Dataset Sonar",
            "content": "Feature selection para el dataset Sonar . Feature selection es una estrategia muy importante para generar modelos buenos y robustos. Si mantenemos atributos que contengan una gran cantidad de ruido, identificar los patrones que se buscan en un set de datos será una tarea difícil, hasta para los algoritmos de aprendizaje más modernos. . Por este motivo, existen diferentes técnicas para seleccionar los mejores atributos para nuestro modelo. Estas caen en 2 categorías, las del tipo filtro y las del tipo wrapper. . Las técnicas del tipo filtro refieren a una elección de artibutos a remover sin incluir el proceso de aprendizaje/modelado. Dentro de esta categoría, podemos encontrar a la eliminación de atributos por conocimiento del dataset o a la eliminación de atributos conocidos que no aportan a los modelos (id o nombre en algunos casos). También, técnicas como el análisis de la correlación de los atributos con la salida y entre sí, caen dentro de esta categoría. Por último, otra técnica de tipo filtro sería utilizar PCA, tanto para utilizar los primeros componentes principales en el modelado como para identificar los atributos con coeficientes mayores en el cálculo de los primeros componentes principales. . Por otra parte, las técnicas de tipo wrapper seleccionan predictores de forma iterativa incluyendo los pasos de entrenamiento y validación para decidir cuáles predictores son los que generan los mejores modelos. Como una estrategia de fuerza bruta, o sea probar todas las combinaciones posibles es increíblemente costoso de un punto de vista computacional (orden exponencial – 2 a la n para n atributos), surgen heurísticas variadas para atacar esta problemática. Algunos ejemplos de estas son: forward propagation, backward elimination y evolutionary feature selection. Estas heurísticas serán analizadas a continuación. . Forward Selection . Como primer paso, se generan modelos con cada uno de los predictores del dataset (se utiliza únicamente 1 predictor). El atributo con el que fue entrenado el modelo que obtuvo mejores resultados, será seleccionado y se pasa a la siguiente iteración. Luego, se entrenan modelos con 2 atributos, las combinaciones entre el atributo elegido en el paso anterior y el resto de los predictores (n-1 modelos con n predictores). Dentro de estos modelos, se compara la performance entre sí y con el modelo que utilizó únicamente 1 atributo. Si la performance no mejora respecto al modelo con 1 atributo menos, termina la heurística. Si, efectivamente, se logra obtener una mayor performance, se sigue iterando hasta llegar a la condición de corte mencionada en la oración anterior. Se puede ver que el número de entrenamientos necesarios para esta heurística posee una cota superior dada por la suma de los primeros n naturales de 1 hasta la cantidad de predictores. Esta cantidad de modelos entrenados es mucho menor que la cantidad si utilizamos el enfoque de fuerza bruta (para 100 atributos existen como máximo 5050 combinaciones para forward selection y 1,267,650,600,228,229,401,496,703,205,375 para el enfoque de fuerza bruta). . Backward elimination . Esta heurística se puede pensar como la opuesta/inversa de forward selection. En vez de empezar con 0 e ir agregando atributos iterativamente, aquí se comienza con la totalidad de los predictores. Luego de entrenar el primer modelo con todos los atributos, se mide la performance de los modelos que poseen todos los predictores menos 1. Si existe alguno de estos nuevos modelos que posea una mejor performance que la que obtuvo el modelo de la iteración anterior (modelo entrenado con todos los predictores), se descarta el atributo no utilizado y se itera nuevamente. En el caso que la performance no mejore, termina la heurística. . Las 2 heurísticas mencionadas son claramente mucho más eficientes desde el punto de vista computacional que la fuerza bruta, pero tienen sus desventajas. Son muy susceptibles a seleccionar óptimos locales y no el óptimo global. Debido a esto, la performance obtenida con estos enfoques normalmente es inferior a la obtenida con el enfoque de fuerza bruta. Entonces, ¿cuál sería nuestra opción si quisiéramos lo mejor de los dos mundos? . Algoritmos Evolutivos para la selección de atributos. . Los algoritmos evolutivos son una técnica de optimización que se basa en la selección natural. En resumen, esta estrategia funciona inicialmente generando una población inicial (combinaciones de predictores en nuestro caso). Luego, de forma iterativa hasta llegar a un máximo de iteraciones (se les llama generaciones) o a alguna otra condición de corte, se aplican los siguientes pasos. Cruzamiento: Se cruzan los individuos de la población. Mutación: se muta a los individuos (normalmente involucra un cambio pequeño randómico como seleccionar un atributo nuevo o des-seleccionar uno). Evaluación: se evalúa a la población. En nuestro caso esto se hará con la exactitud (por ejemplo) de los modelos generados con los atributos representados por cada individuo de la población. Selección: se seleccionan losindividuos que posean mejores métricas. . Esta estrategia, aunque tiene un costo computacional mayor que las 2 heurísticas antes mencionadas, suele llegar al óptimo global o de los contrario a una solución mejor que la obtenida con dichas heurísticas. El costo igualmente, no se compara con el de la fuerza bruta. . Dataset Sonar . Este dataset contiene medidas realizadas al fondo marino utilizando un sonar. Cada medida (ejemplo) contiene múltiples bandas de frecuencia (predictores) y una clase definida: rock o mine. La idea es poder detectar la presencia de minas en el fondo marino. Al graficar los datos, se puede observar el espectro de frecuencia para cada objeto. Dichos espectros, difieren claramente en algunas zonas específicas. Se definen los siguientes intervalos de atributos como zonas de interés. La idea es utilizar diferentes técnicas de feature selection para observar los atributos que se eligen y la performance que se obtiene. . Intervalos . 1) 9-13 . 2) 19-25 . 3) 33-37 . 4) 42-50 . Ejercicio 1: Benchmarking . Se entrena un modelo con todos los predictores y el algoritmo naive bayes. . Se obtienen los mismos resultados con o sin la corrección de Laplace. . Ejercicio 2: Forward selection . Se entrena un modelo con los predictores seleccionados con la heurística forward selection y el algoritmo naive bayes. . . Atributos seleccionados: 12, 15, 17, 18 . El algoritmo encuentra un máximo local (atributo 12) en 4 iteraciones. . Ejercicio 3: Backward Elimination . Se entrena un modelo con los predictores seleccionados con la heurística backward elimination y el algoritmo naive bayes. . . Atributos descartados: 3, 14, 20, 36, 47, 48, 52, 59 . Ejercicio 4 . Se entrena un modelo con los predictores seleccionados con una estrategia evolutiva y el algoritmo naive bayes. . . . Análisis personal de los resultados . La mejor exactitud fue obtenida por el modelo en el que se utilizó el enfoque genético para la selección de atributos. Igualmente, considero que se eligieron demasiados atributos, muchos de los cuales no hacen más que incluir ruido en el modelo. . Debido a esto, decido indagar más en los parámetros del bloque optimize selection y encuentro una opción con la que se puede limitar la cantidad de atributos seleccionados. Escojo el valor 10. Los resultados son los siguientes. . 10 atributos: . . Se obtuvieron mejores resultados y los atributos seleccionados tienen más sentido cuando se los compara con los intervalos en los que las 2 gráficas (mine y rock) están más separadas. .",
            "url": "/Portafolio-ML/2021/10/10/Diferentes-estrategias-para-Feature-Selection-Dataset-Sonar.html",
            "relUrl": "/2021/10/10/Diferentes-estrategias-para-Feature-Selection-Dataset-Sonar.html",
            "date": " • Oct 10, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Clustering",
            "content": "Clustering . En este post me gustaría hablar sobre diferentes estrategias de clustering de datos para problemas supervisados y no supervisados. . Previo a un análisis de casos especiales, me parece pertinente hacer una breve introducción al concepto de clustering. Supongamos que se tiene un conjunto de datos estructurado o tabular como por ejemplo un archivo .csv. En él habrá una gran cantidad de filas o ejemplos y otra cantidad de columnas o atributos. ¿Cómo podríamos hacer para extraer información relevante de este conjunto de datos? Si nuestro dataset contiene información de clientes de un supermercado y de los artículos que este compra, algo que podríamos querer hacer es agrupar a diferentes tipos de clientes para poder realizar ofertas direccionadas. El proceso de agrupar o armar clusters es lo que se conoce como clustering y tiene el propósito de extraer información mediante diferentes formas de agrupación. . Hierarchical clustering . El clustering jerárquico es un método de análisis de clusters que busca construir una jerarquía de clusters. Este tipo de clustering se basa en la idea que explica que un objeto está más relacionado con un objeto cercano que con uno que está a una distancia mayor. Por lo tanto, el clustering jerárquico conecta objetos o ejemplos basándose en la distancia entre ambos. A distancias diferentes, se formarán distintos clusters, que se pueden representar utilizando un dendograma. . Existen 2 estrategias para clustering jerárquico: . Agglomerative: Cada ejemplo comienza en su cluster propio y a medida que se aumenta la distancia (a medida que nos movemos hacia arriba en la jerarquía), se irán formando las agrupaciones. . Divisive: Los ejemplos comienzan agrupadops en un único cluster y se van separando recursivamente a medida que bajamos en la jerarquía. . Existen diferentes modos de medir las distancias entre clusters. Cada una tiene sus casos de aplicación particular, pero en el siguiente ejercicio, analizaremos el propósito de 2 de estos modos (complete link y single link). . K-Means . K-means es un tipo de clustering en el que el número de clusters a generar es un parámetro del algoritmo (k). Este algoritmo iniciará escogiendo k puntos aleatorios del dataset (centroides) y asignando un cluster a cada punto del dataset dependiendo de qué centroide esté más cerca de él. Luego, de forma iterativa, se vuelven a calcular los centroides tal que estos centroides (no necesariamente pertenecientes al dataset) minimicen la suma de las distancias a todos los puntos del cluster. Luego, se reclusterizan los puntos dependiendo de la proximidad de los centroides recalculados. La heurística termina cuando los centroides se hayan estabilizado o luego de un número máximo de iteraciones. Este algoritmo no siempre devuelve los mismos agrupamientos debido a la inicialización randómica de los centroides, por lo que es una buena práctica correr el algoritmo múltiples veces. . Ejercicio . Se desea agrupar los ejemplos del dataset ripley provisto por rapidminer en clusters. . Nota: Es de crucial importancia normalizar los atributos previo a realizar clustering, pues las formas de clustering discutidas utilizan la distancia euclídea para medir la similaridad entre ejemplos o clusters. . . Agglomerative clustering . Para el agglomerative clustering, se decide hacer una comparación entre la utilización entre complete link y single link. . Complete link y distancia euclídea . . Single link y distancia euclídea . Como se puede observar en las imágenes anteriores, la utilización de comlpete link obtiene clusters más parejos (uniformes) y distancias intraclusters medias menores que el single link. Aquí ya podemos entender el propósito de la utilización de cada uno. . Si se desea obtener clusters uniformes y parejos, la mejor forma de medir distancias entre clusters es complete link. . Si, por el contrario se desea localizar datos extremos o outliers, single link es la forma de distancia a elegir. . Resultados para agglomerative clustering con complete link: . Avg. within cluster distance: -31.972 . Avg. within cluster distance for cluster 0: -37.551 . Avg. within cluster distance for cluster 1: -13.095 . Avg. within cluster distance for cluster 2: -34.942 . Top-Down Clustering . En cuanto a top-down clustering, se decide utilizar k-means con Max depth=5 y . Max leaf size=1. . . Resultados: . kmeans k=2 . Avg. within cluster distance: -36.971 . Avg. within cluster distance for cluster 0: -55.705 . Avg. within cluster distance for cluster 1: -23.788 . Avg. within cluster distance for cluster 2: -13.534 . Conclusiones . Se puede observar que la estrategia de aglomerative clustering obtuvo una menor distancia promedio intercluster que la estrategia utilizando top-down clustering con kmeans. Debido a esto, podemos argumentar que para este dataset, la mejor estrategia es la de agglomerative clustering . Referencias: . https://docs.rapidminer.com/latest/studio/operators/modeling/segmentation/agglomerative_clustering.html .",
            "url": "/Portafolio-ML/2021/10/09/Clustering.html",
            "relUrl": "/2021/10/09/Clustering.html",
            "date": " • Oct 9, 2021"
        }
        
    
  
    
  
    
        ,"post8": {
            "title": "Modelado Con El Dataset Titanic Documento",
            "content": "Preparación de datos y modelado para el dataset Titanic . Importar librerías y paquetes . Además de hablar de las librerías y paquetes, me pareció interesante destacar ciertos conceptos que no fueron presentados en la clase y quizás alguna posible mejora para alguna parte del código en el caso de manejar datasets más pesados y más cómputo-dependientes. . Inicialmente, se importan las librerías necesarias como numpy y pandas para cálculos matemáticos y estructuras de datos además de matplotlib y seaborn para la parte de visualización de datos y gráficas. . Luego, se importan varias funcionalidades/modelos de la librería scikit learn. Los modelos importados y una breve explicación de ellos es la siguiente. . SVC, Linear SVC son algoritmos del tipo Support vector machines, los cuales buscan definir una recta, plano o hiperplano (hablando de un SVM lineal) tal que la distancia entre esta recta y el punto más cercano de cada clase sea máxima. . Dentro de los algoritmos de árboles o ensembles, se utilizan un decisionTreeClassifier, un randomForestClassifier y XGBoost. El decisionTreeClassifier construye un único árbol y lo utiliza para hacer las predicciones. El random forest construye múltiples árboles armados utilizando bagging y luego al realizar una predicción se le da un peso al resultado de cada árbol del bosque y se decide qué predice el bosque (juntando las predicciones de cada uno y sus pesos). Por último, XGBoost utiliza una forma diferente de armar los árboles dado que son dependientes uno de otro, intentando mejorar las métricas con cada árbol nuevo. . También se utiliza la técnica de Gridsearch para tunear los hiperparámetros de los distintos algoritmos. Una mejor alternativa para un caso computacionalmente complejo sería utilizar randomsearch para ubicar los puntos de interés donde es muy probable encontrar óptimos en la cercanía y aplicar gridsearch en espacios pequeños cercanos a dichos puntos de interés. . Cargar el dataset y mostrarlo . Para realizar estas dos tareas, se utilizan las funcionalidades de pandas llamadas read_csv para leer un archivo csv y cargarlo utilizando la estructura de datos llamada dataframe y head como forma de imprimir las primeras x filas (5 por defecto) del dataset de una forma prolija. . ## . Gestionar los valores faltantes . Se utiliza la función pd.isnull para identificar los valores faltantes de cada columna. Se dropean las columnas Cabin y ticket por la gran cantidad de valores faltantes. . Se observa una distribución relativamente parecida a una normal para el atributo age, pero con un leve sesgo hacia la derecha, lo que determinará la estrategia de imputación utilizada. Debido a este sesgo, elegir la mediana es una mejor alternativa que elegir la media. Si no tuviéramos este sesgo, la estrategia de imputación elegida sería la media. . Para el atributo fare, la distribución está fuertemente sesgada hacia la derecha, por lo que se elige la mediana como valor a imputar. Alternativamente, se podría elegir aplicar una transformación a los datos para que su distribución se asemeje más a una normal. 2 estrategias para esto podrían ser aplicar el logaritmo natural a los valores del atributo o alguna de las variantes de la transformación “Box and Cox” (1964) presentada en applied-predictivemodeling, presente en la bibliografía del curso. . Graficar los datos . Se grafican varios atributos en relación al atributo a predecir para formar unas hipótesis iniciales sobre si algún atributo está correlacionado con la variable a predecir. Las librerías para . hacer esto son seaborn y matplotlib.pyplot. Aquí se pueden observar ratios como la proporción de mujeres que sobrevivieron o indicios como qué clase es la que presentó más proporción de personas que sobrevivieron. . ## . Feature Engineering . En la parte de feature engineering, lo primero que se hace es encodear los atributos categóricos a valores enteros. Se podría realizar mediante one hot encoding pero en este notebok, se decide no hacerlo. . Una hipótesis muy interesante provista por Samson Qian (creador del notebook en kaggle) es que el titulo de la persona puede influir en si esta sobrevivió o no. Desde mi punto de vista personal, quizás esto podría estar agregando un atributo fuertemente correlacionado con Sex. Luego de calcular la matriz de correlación, se ve que la correlación es muy baja, lo que no me parece intuitivo, dado que la gran mayoría de los ejemplos de títulos (más del 90%) son Mr, Mrs y Miss los cuales se podrían relacionar directamente con el género. . (Imagen de correlación provista debajo) . . Luego, se normalizan los datos con el standard scaler (estandarización Z). . Pasos Opcionales . Como no se requiere, no voy a comentar todos los aspectos sobre la parte de modelado, predicción y evaluación de rendimiento, pero sí los incluí en el código y me interesaría remarcar algunos puntos. . Se puede ver que los mejores resultados a primera vista, pues la única métrica que estamos mirando es accuracy (no es suficiente para tomar una decisión sobre si el modelo es bueno o no), son los de SVC y random forest, alrededor de 0.82 – 0.83, pero varía cada vez, dado que el random forest tiene un grado de aleatoriedad en su construcción. . Cabe destacar que, aunque los resultados del random forest sean los mejores para este set de testeo, el modelo tardó 60 segundos en entrenar, mientras que SVC tardó 1.2 segundos y la regresión logística 3.4s. Para conjuntos de datos más grandes o muy cambiantes para los que se necesita reentrenar el modelo constantemente, habría que ver si los pocos puntos de mejora en la accuracy justifican el costo inmensamente mayor de entrenamiento del modelo de random forest. Probablemente los tiempos de inferencia también sean mucho mayores en el random forest. . .",
            "url": "/Portafolio-ML/2021/09/20/Modelado-con-el-dataset-Titanic-Documento.html",
            "relUrl": "/2021/09/20/Modelado-con-el-dataset-Titanic-Documento.html",
            "date": " • Sep 20, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Naive Bayes   Dataset Golf",
            "content": "Naive Bayes . El propósito de este post es analizar la performance del algoritmo Naive Bayes al utilizar diferentes transformaciones a los datos. . Las transformaciones en cuestión son: . 1) Utilizar los atributos numéricos sin aplicar transformaciones . 2) Transformar los atributos numéricos a categóricos . Se utiliza el dataset golf provisto por Rapidminer para este ejercicio. . Parte 1 . Conversión de atributos numéricos a categóricos (multinominales). Esto se aplica tanto para el dataset de entrenamiento como para el dataset de test. . Se genera el modelo con Naive Bayes y los resultados fueron los siguientes: . . La exactitud obtenida es de 10/14 . Parte 2 . Los atributos numéricos se tratan como numéricos. Se genera el modelo con Naive Bayes y los resultados fueron los siguientes: . . La exactitud obtenida es de 9/14 . Comparación de resultados . En este caso podríamos concluir que el modelo generado con las variables categóricas tiene mejores resultados que el de variables numéricas.  . Las distribuciones no se asemejan a una distribución gaussiana. De aquí pueden derivar los peores resultados al tratar a los atributos como numéricos. . .",
            "url": "/Portafolio-ML/2021/09/18/Naive-Bayes-Dataset-Golf.html",
            "relUrl": "/2021/09/18/Naive-Bayes-Dataset-Golf.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Knn   Dataset Iris",
            "content": "KNN Iris . Se crea un nuevo proceso con el dataset Iris de UCI. Luego, se grafican petal length y petal width en un gráfico bidimensional, diferenciando las diferentes especies por color. . . Claramente la especie iris-setosa es muy distinta a las otras 2 y por lo tanto un modelo construido con knn debería reconocer perfectamente a todos los ejemplos de dicha clase. Para el caso de iris versicolor e iris virginica, la separación es más difusa, y es muy probable que nuestro modelo de knn cometa errores al clasificar algunos ejemplos de ellas. . En cuanto a las tareas de acondicionamiento necesarias para este dataset y este algoritmo, se deberán normalizar todos los atributos dado que KNN trabaja con distancias. Una diferencia en escala entre los atributos podría perjudicar substancialmente la performance del modelo. . Operador Knn de rapidminer . K: Refiere a la cantidad de vecinos más cercanos que utilizará el algoritmo para hacer una predicción sobre un ejemplo no visto. . Weighted vote: La aplicación de un voto ponderado refiere a que; dentro de los k vecinos escogidos, el más cercano a un nuevo ejemplo será el que tendrá el mayor peso al aplicar la predicción y el más lejano será el que tendrá el menor peso. . Measure types: El tipo de medida que se utilizará para encontrar los vecinos más cercanos. Aquí se podrá mencionar si se tendrán atributos categóricos, numéricos ambos, etc. . Función de medida: Se podrá seleccionar un tipo de distancia tal que sea acorde con el tipo de medida que se seleccionó anteriormente. Por ejemplo para datos numéricos, se puede utilizar la distancia euclídea, mientras que para datos nominales se puede utilizar la Jaccard similarity. . Entrenamiento de modelo y resultados . K=5 . K=3 . K=1 . K=1 Distancia= Chebychev Distance . Análisis de resultados . Contrario a lo que hubiera pensado, los mejores resultados fueron obtenidos con k=1 para este Split de este dataset. Las distancias de manhattan y euclídea presentaron los mismos resultados para k = 1, mientras que la de chebychev presentó una exactitud levemente menor. . Mi hipótesis inicial para la clase iris-setosa resultó ser cierta para la mayoría de los modelos creados (menos para el modelo en el que se utiliza la distancia de chebychev). Esta clase obtuvo una precisión y un recall de 100%. .",
            "url": "/Portafolio-ML/2021/09/16/KNN-Dataset-Iris.html",
            "relUrl": "/2021/09/16/KNN-Dataset-Iris.html",
            "date": " • Sep 16, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Árboles De Decisión   Dataset Iris Rapidminer",
            "content": "Utilización de árboles de decisión para clasificar los ejemplos del dataset Iris utilizando Rapidminer. . . . .",
            "url": "/Portafolio-ML/2021/09/15/%C3%81rboles-de-decisi%C3%B3n-Dataset-Iris-Rapidminer.html",
            "relUrl": "/2021/09/15/%C3%81rboles-de-decisi%C3%B3n-Dataset-Iris-Rapidminer.html",
            "date": " • Sep 15, 2021"
        }
        
    
  
    
  
    
        ,"post13": {
            "title": "Svm",
            "content": "Máquinas de soporte vectorial . Se generó un dataset de ejemplo para mostrar un caso en el que las máquinas de soporte vectorial demuestran su potencial. El dataset utilizado es SVM-Anillos.csv. . Al visualizar los datos, las clases parecerían estar separadas mediante un anillo. Por esta razón podríamos plantearnos las siguientes hipótesis: . 1) Utilizar un kernel lineal es una mala opción, pues no existe una recta que haga una buena separación de los 2 conjuntos. . 2) Probablemente un kernel radial o uno polinómico de grado par obtenga buenos resultados . . Flujo de Rapidminer . . Cross validation . . Resultados: . Kernel lineal . . Kernel radial . . ## . Kernel Polinómico Par e Impar . . Observamos el mismo comportamiento que con el kernel radial para un kernel polinómico de grado 2. Modificando el grado del kernel a valores pares, se puede ver una accuracy de 100%, mientras que utilizando un grado impar, los resultados se acercan más al 70-75% utilizando 10-fold-CV. . Cambio de la constante C . No se observan cambios notorios al modificar la constante c para este dataset, dado que los datos están perfectamente separados. . Conclusiones . Como conclusión, se probaron ambas hipótesis planteadas mediante el entrenamiento de diferentes máquinas de soporte vectorial con la utilización de distintos kernels. .",
            "url": "/Portafolio-ML/2021/09/15/SVM.html",
            "relUrl": "/2021/09/15/SVM.html",
            "date": " • Sep 15, 2021"
        }
        
    
  
    
  
    
  
    
        ,"post16": {
            "title": "Normalización Y Detección De Outliers",
            "content": "Rapid Miner outlier detection and normalization tutorial. . Ejercicio 1 . Se aplica normalización para luego aplicar el cálculo de las distancias para detectar a los outliers. Siempre se debe normalizar previo al cálculo de los outliers por el hecho de que los diferentes atributos suelen no tener la misma unidad o escala. Para la normalización se utiliza la transformación z. . El operador detect outliers detectará los 10 ejemplos que se alejen más del resto. En el flujo, luego colocamos un filter para remover los 10 outliers antes mencionados. . Ejercicio 2 . El dataset posee 13 atributos sin incluir a la variable objetivo. Estos describen distintos elementos de la composición de vinos para luego poder clasificarlos. . La variable objetivo posee tres valores posibles. Estos valores representan a 3 diferentes variantes de viñedos (las plantas de las que se genera el vino son diferentes genéticamente entre sí). . La variable objetivo está relativamente balanceada, no hay una clase que presente un numero muy bajo de ejemplos, por lo que argumentaría que no habría que aplicar ningún tipo de sampling. . Para algunos atributos, la distribución parece ser normal (ash, alcalinity of ash, proanthocyanins), mientras que algunas otras parecen tener un sesgo hacia la derecha (por ejemplo malic acid y proline), pero no es muy grave tampoco. . Creación del modelo con naive bayes. . Al visualizar objetivamente la matriz de confusión, los resultados no variaron al aplicar la normalización. Con ambos tipos de normalización (z score y min-max), los resultados fueron los mismos. . Flujo de rapidminer: . El para los Split data, se utilizó la misma seed y la misma proporción (70/30) para que los conjuntos de entrenamiento y testeo sean los mismos para ambas pruebas, con la diferencia que uno estará normalizado y otro no. . . Matriz sin normalizado . . Matriz con normalizado . . Conclusiones . El normalizado no afecta los resultados para este caso en particular. Esto se debe al algoritmo que utilizamos principalmente. .",
            "url": "/Portafolio-ML/2021/09/10/Normalizaci%C3%B3n-y-detecci%C3%B3n-de-outliers.html",
            "relUrl": "/2021/09/10/Normalizaci%C3%B3n-y-detecci%C3%B3n-de-outliers.html",
            "date": " • Sep 10, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Actualmente soy un estudiante de Ingeniería en Informática y trabajo como científico de datos y desarrollador de aplicaciones desde el año 2020. El propósito de este portafolio es exponer algunos proyectos realizados en relación a Machine Learning. .",
          "url": "/Portafolio-ML/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "/Portafolio-ML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}